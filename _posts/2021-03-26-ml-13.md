---
layout: post
title: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (åä¸‰) - äººè„¸è¯†åˆ«
tag: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨
---

è¿™ç¯‡å°†ä¼šä»‹ç»äººè„¸è¯†åˆ«æ¨¡å‹çš„å®ç°ï¼Œä»¥åŠå¦‚ä½•ç»“åˆå‰å‡ ç¯‡æ–‡ç« çš„æ¨¡å‹æ¥è¯†åˆ«å›¾ç‰‡ä¸Šçš„äººï¼Œæœ€ç»ˆæ•ˆæœå¦‚ä¸‹ï¼š

![img_output4.png](./img_output4.png)

è¿™è¯æˆ‘ä¸¾åŒæ‰‹åŒè„šåŒæ„ğŸ¤¬ï¼Œç¾å›½æ˜¯åœ¨å¤ªéœ¸é“äº†ï¼Œä¸­å›½ç»ä¸èƒ½åœ¨é¢†åœŸå’Œä¸»æƒé—®é¢˜ä¸Šé€€è®©ã€‚

## å®ç°äººè„¸è¯†åˆ«çš„æ–¹æ³•

ä½ å¯èƒ½ä¼šæƒ³èµ·[ç¬¬å…«ç¯‡æ–‡ç« ](https://303248153.github.io/ml-08/)ä»‹ç»å¦‚ä½•è¯†åˆ«å›¾ç‰‡ä¸Šç‰©ä½“ç±»å‹çš„ CNN æ¨¡å‹ï¼Œé‚£ä¹ˆäººè„¸æ˜¯å¦ä¹Ÿèƒ½ç”¨åŒæ ·çš„æ–¹æ³•è¯†åˆ«å‘¢ï¼Ÿä¾‹å¦‚æœ‰ 100 ä¸ªäººï¼ŒæŠŠè¿™ 100 ä¸ªäººå½“ä½œ 100 ä¸ªåˆ†ç±»ï¼Œç„¶åç”¨ä»–ä»¬çš„ç…§ç‰‡æ¥è®­ç»ƒï¼Œä¼¼ä¹å°±å¯ä»¥è®­ç»ƒå‡ºå¯ä»¥æ ¹æ®å›¾ç‰‡è¯†åˆ«å“ªä¸ªäººçš„æ¨¡å‹äº†ï¼ŒçœŸçš„å—ğŸ¤”ã€‚

![cifar-10.png](cifar-10.png)

å¾ˆé—æ†¾ï¼Œç”¨äºè¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹å¹¶ä¸èƒ½ç”¨åœ¨äººè„¸è¯†åˆ«ä¸Šï¼Œä¸»è¦æœ‰ä»¥ä¸‹åŸå› ï¼š

- è¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹é€šå¸¸è¦æ±‚æ¯ä¸ªåˆ†ç±»æœ‰å¤§é‡çš„å›¾ç‰‡ï¼Œè€Œäººè„¸è¯†åˆ«æ¨¡å‹å¾ˆå¤šæ—¶å€™åªèƒ½æ‹¿åˆ°ä¸ªä½æ•°çš„äººè„¸ï¼Œè¿™æ ·è®­ç»ƒå‡ºæ¥çš„ç²¾åº¦å¾ˆä¸ç†æƒ³ã€‚è¿™ä¸ªé—®é¢˜åˆç§° One-shot å­¦ä¹ é—®é¢˜ (æ¯ä¸ªåˆ†ç±»åªæœ‰å¾ˆå°‘çš„æ ·æœ¬æ•°é‡)ã€‚
- è¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹åªèƒ½è¯†åˆ«è®­ç»ƒè¿‡çš„ç±»å‹ï¼Œå¦‚æœæƒ³æ·»åŠ æ–°ç±»å‹åˆ™éœ€è¦é‡æ–°å¼€å§‹è®­ç»ƒ (å¦‚æœä¸€å¼€å§‹é¢„ç•™æœ‰å¤šçš„åˆ†ç±»æ•°é‡å¯ä»¥åŸºäºä¸Šä¸€æ¬¡çš„æ¨¡å‹çŠ¶æ€ç»§ç»­è®­ç»ƒï¼Œè¿™ä¸ªåšæ³•åˆç§°è¿ç§»å­¦ä¹ )
- åŒä¸Šï¼Œè¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹ä¸èƒ½è¯†åˆ«æ²¡æœ‰å­¦ä¹ è¿‡çš„äººç‰©

æˆ‘ä»¬éœ€è¦ç”¨ä¸åŒçš„æ–¹æ³•æ¥å®ç°äººè„¸è¯†åˆ«ğŸ˜¤ï¼Œç›®å‰ä¸»æµçš„æ–¹æ³•æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯åŸºäºæŒ‡æ ‡ï¼Œæ ¹æ®äººè„¸ç”Ÿæˆå¯¹åº”çš„ç¼–ç ï¼Œç„¶åè°ƒæ•´ç¼–ç ä¹‹é—´çš„è·ç¦» (åŒä¸€ä¸ªäººçš„ç¼–ç æ¥è¿‘ï¼Œä¸åŒçš„äººçš„ç¼–ç è¿œç¦») æ¥å®ç°äººè„¸çš„åŒºåˆ†ï¼›å¦ä¸€ç§æ˜¯åŸºäºåˆ†ç±»ï¼Œå¯ä»¥çœ‹ä½œæ˜¯è¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹çš„æ”¹è¿›ç‰ˆï¼ŒåŒæ ·ä¼šæ ¹æ®äººè„¸ç”Ÿæˆå¯¹åº”çš„ç¼–ç ï¼Œä½†æœ€åä¼šæ·»åŠ ä¸€å±‚è¾“å‡ºåˆ†ç±»çš„çº¿æ€§æ¨¡å‹ï¼Œæ¥å®ç°é—´æ¥çš„è°ƒæ•´ç¼–ç ã€‚

### åŸºäºæŒ‡æ ‡çš„æ–¹æ³•

åŸºäºæŒ‡æ ‡çš„æ–¹æ³•ä½¿ç”¨çš„æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š

![01](./01.png)

æˆ‘ä»¬æœ€ç»ˆæƒ³è¦æ¨¡å‹æ ¹æ®äººè„¸è¾“å‡ºç¼–ç ï¼Œå¦‚æœæ˜¯åŒä¸€ä¸ªäººé‚£ä¹ˆç¼–ç å°±ä¼šæ¯”è¾ƒæ¥è¿‘ï¼Œå¦‚æœæ˜¯ä¸åŒçš„äººé‚£ä¹ˆç¼–ç å°±ä¼šæ¯”è¾ƒè¿œç¦»ã€‚å¦‚æœè®­ç»ƒæˆåŠŸï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å·²æœ‰çš„äººè„¸æ„å»ºä¸€ä¸ªç¼–ç æ•°æ®åº“ï¼Œè¯†åˆ«æ–°çš„äººè„¸æ—¶ç”Ÿæˆæ–°çš„äººè„¸çš„ç¼–ç ï¼Œç„¶åå¯¹æ¯”æ•°æ®åº“ä¸­çš„ç¼–ç æ‰¾å‡ºæœ€æ¥è¿‘çš„äººè„¸ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![02](./02.png)

è¾“å‡ºç¼–ç çš„æ¨¡å‹å®šä¹‰å¦‚ä¸‹ï¼Œè¿™é‡Œçš„ç¼–ç é•¿åº¦æ˜¯ 32 (å®Œæ•´ä»£ç ä¼šåœ¨åé¢ç»™å‡º)ï¼š

``` python
# Resnet çš„å®ç°
self.resnet = torchvision.models.resnet18(num_classes=256)
# æ”¯æŒé»‘ç™½å›¾ç‰‡
if USE_GRAYSCALE:
    self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
# æœ€ç»ˆè¾“å‡ºç¼–ç çš„çº¿æ€§æ¨¡å‹
# å› ä¸º torchvision çš„ resnet æœ€ç»ˆä¼šä½¿ç”¨ä¸€ä¸ª Linearï¼Œè¿™é‡Œçœç•¥æ‰ç¬¬ä¸€ä¸ª Linear
self.encode_model = nn.Sequential(
    nn.ReLU(inplace=True),
    nn.Linear(256, 128),
    nn.ReLU(inplace=True),
    nn.Linear(128, 32))
```

è€Œæ¯”è¾ƒç¼–ç æ‰¾å‡ºæœ€æ¥è¿‘çš„äººè„¸å¯ä»¥ä½¿ç”¨ä»¥ä¸‹çš„ä»£ç  (è®¡ç®—ç¼–ç ä¸­å„ä¸ªå€¼çš„ç›¸å·®çš„å¹³æ–¹çš„åˆè®¡)ï¼š

``` python
diff = (new_code - exists_code).pow(2).sum(dim=1).sort()
most_similar = diff.indices[0]
```

å¦‚æœç¼–ç æ•°æ®åº“ä¸­æœ‰å¤§é‡çš„ç¼–ç ï¼Œè®¡ç®—æ‰€æœ‰ç¼–ç çš„è·ç¦»æ¶ˆè€—ä¼šæ¯”è¾ƒå¤§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ”¯æŒæœç´¢å‘é‡çš„æ•°æ®åº“ï¼Œä¾‹å¦‚æŠŠç¼–ç ä¿å­˜åˆ°  Elastic Search æ•°æ®åº“å¹¶ä½¿ç”¨ dense_vector ç±»å‹ï¼ŒElastic Search æ•°æ®åº“ä¼šæ ¹æ®ç¼–ç æ„å»ºç´¢å¼•å¹¶å®ç°æ›´é«˜æ•ˆçš„æŸ¥æ‰¾ğŸ¤’ã€‚

çœ‹åˆ°è¿™é‡Œä½ å¯èƒ½ä¼šè§‰å¾—ï¼Œå°±è¿™ä¹ˆç‚¹å—ï¼Ÿé‚£è¯¥å¦‚ä½•è®­ç»ƒæ¨¡å‹ï¼Œè®©åŒä¸€ä¸ªäººçš„ç¼–ç æ›´æ¥è¿‘å‘¢ï¼Ÿè¿™å°±æ˜¯æœ€éš¾çš„éƒ¨åˆ†äº†ğŸ¤•ï¼Œä¸€å¼€å§‹å¤§å®¶æƒ³åˆ°çš„æ˜¯ä»¥ä¸‹çš„æ–¹å¼ï¼š

``` python
# è®¡ç®—æŸå¤±çš„é€»è¾‘
loss1 = åŒä¸€ä¸ªäººçš„ç¼–ç è·ç¦»
loss2 = ReLU(å¸¸é‡A - ä¸åŒçš„äººçš„ç¼–ç è·ç¦»)
loss = loss1 + loss2
```

è¿™æ ·åšçœ‹ä¸Šå»ç»è¿‡è®­ç»ƒä»¥ååŒä¸€ä¸ªäººçš„ç¼–ç ä¼šå®Œå…¨ç›¸åŒï¼Œè€Œä¸åŒçš„äººçš„ç¼–ç è·ç¦»æœ€å°‘éœ€è¦å¤§äºå¸¸é‡Aï¼Œä½†å®é™…ä¸Šè¿™ä¸ªæ–¹å¼å¾ˆéš¾è®­ç»ƒæˆåŠŸï¼Œå› ä¸ºå³ä½¿æ˜¯åŒä¸€ä¸ªäººï¼Œå›¾ç‰‡ä¸Šçš„äººè„¸è§’åº¦ã€å…‰çº¿ã€è„¸è‰²ã€ä»¥åŠèƒŒæ™¯éƒ½ä¸ä¸€æ ·ï¼Œç”Ÿæˆå®Œå…¨ä¸€æ ·çš„ç¼–ç ä¼šéå¸¸å›°éš¾ã€‚

2015 å¹´çš„ [Facenet è®ºæ–‡](https://arxiv.org/abs/1503.03832) æå‡ºäº†ä½¿ç”¨ Triplet Loss æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹çš„æ‰‹æ³•ï¼Œç®€å•æ¥è¯´å°±æ˜¯åŒæ—¶å‡†å¤‡ä¸¤ä¸ªäººçš„ä¸‰å¼ å›¾ç‰‡ (åˆç§°ä¸‰å…ƒç»„)ï¼Œç„¶åè®©åŒä¸€ä¸ªäººçš„ç¼–ç è·ç¦»å°äºä¸åŒçš„äººçš„ç¼–ç è·ç¦»ï¼Œè®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

``` python
loss = ReLU(åŒä¸€ä¸ªäººçš„ç¼–ç è·ç¦» + å¸¸é‡A - ä¸åŒçš„äººçš„ç¼–ç è·ç¦»)
```

çœ‹ä¸Šå»åªæ˜¯æŠŠå‰é¢çš„ loss1 æ”¾åˆ°äº† loss2 çš„ ReLU å‡½æ•°é‡Œé¢å•Šï¼Œå¯¹ğŸ¤’ï¼Œè¿™ä¹ˆåšäº†ä»¥ååŒä¸€ä¸ªäººçš„ç¼–ç è·ç¦»ä¸éœ€è¦ç­‰äº 0ï¼Œåªéœ€è¦å’Œä¸åŒçš„äººçš„ç¼–ç è·ç¦»ç›¸å·®å¸¸é‡Aå³å¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![03](./03.png)

ç»è¿‡è®­ç»ƒä»¥åçš„ç¼–ç åˆ†å¸ƒå¤§æ¦‚ä¼šåƒä¸‹å›¾ï¼ŒåŒä¸€ä¸ªäººç‰©çš„ç¼–ç ä¸ä¼šå®Œå…¨ä¸€æ ·ä½†ä¼šèšé›†åœ¨ä¸€èµ· ï¼ˆè¿™å°±æ˜¯ä¸€ç§é€šè¿‡æœºå™¨å­¦ä¹ å®ç°èšç±»çš„æ–¹æ³•ğŸ¤’)ï¼š

![04](./04.png)

ç°åœ¨æˆ‘ä»¬çŸ¥é“é€‰å–ä¸¤ä¸ªäººçš„ä¸‰å¼ å›¾ç‰‡ (åˆç§°ä¸‰å…ƒç»„)ï¼Œç„¶åä½¿ç”¨ Triplet Loss è®¡ç®—æŸå¤±å³å¯è®­ç»ƒæ¨¡å‹èšç±»äººè„¸ï¼Œé‚£åº”è¯¥æ€æ ·é€‰å–å›¾ç‰‡å‘¢ï¼Ÿç®€å•çš„åšæ³•æ˜¯éšæœºé€‰å–å›¾ç‰‡ï¼Œä½†éšç€è®­ç»ƒæ¬¡æ•°å¢å¤šï¼ŒåŒä¸€ä¸ªäººç‰©çš„ç¼–ç è·ç¦»å°äºä¸åŒäººç‰©çš„ç¼–ç è·ç¦»çš„é¢‘ç‡å°±è¶Šé«˜ï¼Œä¹Ÿå³æ˜¯è¯´ loss ä¸º 0 çš„é¢‘ç‡è¶Šé«˜ï¼Œå¦‚æœ 90% çš„ loss ä¸º 0ï¼Œé‚£ä¹ˆå°±ä»£è¡¨ 90% çš„è®¡ç®—éƒ½ç™½è´¹äº†ã€‚è€Œä¸”ï¼Œè¿™æ ·è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å¯¹äºçœ‹ä¸Šå»ç›¸ä¼¼ä½†æ˜¯ä¸æ˜¯åŒä¸€ä¸ªäººçš„è¯†åˆ«èƒ½åŠ›ä¼šæ¯”è¾ƒå¼±ã€‚

æ›´å¥½çš„æ–¹æ³•æ˜¯è®°å½•ä¸Šä¸€æ¬¡è®­ç»ƒæ—¶å„ä¸ªå›¾ç‰‡çš„ç¼–ç ï¼Œå…ˆé€‰å–ä¸€å¼ åŸºç¡€å›¾ç‰‡ (Anchor)ï¼Œç„¶åé€‰å–"åŒä¸€ä¸ªäººç‰©ä½†ç¼–ç è·ç¦»æœ€è¿œ"çš„ä¸€å¼ å›¾ç‰‡ (Hard Positive)ï¼Œå’Œ"ä¸åŒçš„äººä½†ç¼–ç è·ç¦»æœ€è¿‘"çš„ä¸€å¼ å›¾ç‰‡ (Hard Negative)ã€‚è¿™æ ·å¯ä»¥ç»™æ¨¡å‹å°½å¯èƒ½å¤§çš„å‹åŠ›æ¥è®­ç»ƒçœ‹ä¸Šå»ä¸ç›¸ä¼¼ä½†æ˜¯åŒä¸€ä¸ªäººï¼Œå’Œçœ‹ä¸Šå»ç›¸ä¼¼ä½†ä¸æ˜¯åŒä¸€ä¸ªäººçš„è¯†åˆ«èƒ½åŠ›ã€‚è¿™ä¸ªæ–¹æ³•å®ç°èµ·æ¥æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œå› ä¸ºï¼š

- å¦‚æœè®­ç»ƒçš„å›¾ç‰‡æ•°é‡å¾ˆå¤šï¼Œä¾‹å¦‚ä¸Šç™¾ä¸‡å¼ ï¼Œé‚£ä¹ˆæ¯æ¬¡é€‰å–å›¾ç‰‡éƒ½éœ€è¦è®¡ç®—åŸºç¡€å›¾ç‰‡çš„ç¼–ç å’Œä¸Šç™¾ä¸‡ä¸ªç¼–ç ä¹‹é—´çš„è·ç¦»ï¼Œè®¡ç®—é‡ä¼šéå¸¸åºå¤§ï¼Œè®­ç»ƒèµ·æ¥åƒä¹Œé¾Ÿä¸€æ ·æ…¢ğŸ˜±
- å¦‚æœä½ ä¸å°å¿ƒæŠŠåŒä¸€ä¸ªäººçš„å›¾ç‰‡æ”¾åˆ°å…¶ä»–äººçš„æ–‡ä»¶å¤¹ï¼Œæˆ–è€…æ··æ‚ä¸€äº›è´¨é‡æ¯”è¾ƒåƒåœ¾çš„å›¾ç‰‡ï¼Œè¿™æ—¶å€™å°±å¥½ç©äº†ï¼Œæ¨¡å‹ä¼šæƒ³æ–¹è®¾æ³•çš„å»é€‚åº”è¿™äº›å›¾ç‰‡ï¼Œå¯¼è‡´è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ä¸èƒ½æ³›åŒ–
- å¦‚æœä½ ä¸€ç›´ç»™æ¨¡å‹çœ‹å¾ˆç›¸ä¼¼çš„äººç‰©ç„¶åå‘Šè¯‰æ¨¡å‹è¿™ä¸æ˜¯åŒä¸€ä¸ªäºº (æœ‰å¯èƒ½å› ä¸ºç¬¬äºŒä¸ªåŸå› ï¼Œä¹Ÿæœ‰å¯èƒ½å› ä¸ºçœŸæ˜¯åŒèƒèƒğŸ¤—)ï¼Œé‚£æ¨¡å‹ä¸‹æ¬¡çœ‹åˆ°åŒä¸€ä¸ªäººä¹Ÿä¼šæ€€ç–‘ä¸æ˜¯åŒä¸€ä¸ªï¼ŒåŒæ ·ä¼šå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

Facenet ä¸­ç¼“è§£è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯æŠŠå›¾ç‰‡åˆ‡åˆ†å°æ‰¹æ¬¡ (Mini batch)ï¼Œç„¶ååœ¨å°æ‰¹æ¬¡ä¸­å±€éƒ¨æŸ¥æ‰¾ç¼–ç è·ç¦»æœ€è¿‘ä½†ä¸åŒçš„äººï¼Œä¹Ÿä¼šåœ¨å°éƒ¨åˆ†æ ·æœ¬ä¸­éšæœºé€‰å–æ‰¹æ¬¡å¤–çš„äººç‰©ã€‚æœ¬æ–‡ç»™å‡ºçš„å®ç°å°†ä¼šä½¿ç”¨å¦ä¸€ç§æ–¹æ³•ï¼Œå…·ä½“çœ‹åé¢çš„ä»‹ç»å§ğŸ¥³ã€‚

é¡ºé“ä¸€æï¼ŒFacenet ä¸­ä½¿ç”¨çš„ç¼–ç é•¿åº¦æ˜¯ 32ï¼Œå¸¸é‡Açš„å€¼æ˜¯ 0.2ï¼Œæœ¬æ–‡çš„å®ç°ä¹Ÿä¼šä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼Œå‚è€ƒåé¢ç»™å‡ºçš„ä»£ç å­ã€‚

### åŸºäºåˆ†ç±»çš„æ–¹æ³•

åŸºäºæŒ‡æ ‡çš„æ–¹æ³•å¯ä»¥ç›´æ¥è°ƒæ•´ç¼–ç ä¹‹é—´çš„è·ç¦»ï¼Œä½†é€‰å–ä¸‰å…ƒç»„æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œå¹¶ä¸”éšç€æ•°æ®é‡å¢å¤šï¼Œé€‰å–æ—¶çš„è®¡ç®—é‡ä¹Ÿä¼šè¶Šå¤šã€‚åŸºäºåˆ†ç±»çš„æ–¹æ³•æ˜¯å¦å¤–ä¸€ç§é€”å¾„ï¼Œå¯ä»¥æ— é¡»é€‰å–ä¸‰å…ƒç»„è€Œé—´æ¥çš„è°ƒæ•´ç¼–ç ä¹‹é—´çš„è·ç¦»ã€‚æ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼š

![05](./05.png)

çœ‹èµ·æ¥åªæ˜¯åœ¨è¾“å‡ºç¼–ç ä»¥ååŠ ä¸€ä¸ªå•å±‚çº¿æ€§æ¨¡å‹ï¼Œç„¶åè¾“å‡ºäººç‰©å¯¹åº”çš„åˆ†ç±»ï¼Œå¦‚æœæ˜¯åŒä¸€ä¸ªåˆ†ç±»é‚£ä¹ˆç¼–ç åº”è¯¥ä¼šæ›´æ¥è¿‘ã€‚å¦‚æœå¿½è§†æ‰ç¼–ç ï¼ŒæŠŠå¤šå±‚çº¿æ€§æ¨¡å‹å’Œå•å±‚çº¿æ€§è¿åœ¨ä¸€èµ·ï¼Œå°±æ˜¯æ™®é€šè¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹ã€‚çœŸçš„èƒ½è¡Œå—ï¼Ÿ

å½“ç„¶ï¼Œæ²¡è¿™ä¹ˆç®€å•ğŸ¤¬ï¼Œå¦‚æ–‡ç« å¼€å§‹æåˆ°è¿‡çš„ï¼Œç›´æ¥åº”ç”¨è¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹åˆ°äººè„¸ä¸Šæ•ˆæœä¼šå¾ˆå·®ï¼Œå› ä¸ºå„ä¸ªäººçš„æ ·æœ¬æ•°é‡éƒ½ä¸å¤šï¼ŒåŠ ä¸Šæœ€åçš„å•å±‚çº¿æ€§æ¨¡å‹åªéœ€è¦åˆ’åˆ†ç¼–ç åˆ°åˆ†ç±»è€Œä¸éœ€è¦èšé›†ç¼–ç ï¼Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹è¯†åˆ«èƒ½åŠ›ä¼šå¾ˆå¼±ã€‚è®­ç»ƒå‡ºæ¥çš„äººè„¸åˆ†å¸ƒå¯èƒ½ä¼šåƒä¸‹å›¾ä¸€æ ·ï¼š

![06](./06.png)

å…³é”®ç‚¹åœ¨äºè®¡ç®—æŸå¤±çš„å‡½æ•°ï¼Œæ™®é€šè¯†åˆ«ç‰©ä½“ç±»å‹çš„æ¨¡å‹ä¼šä½¿ç”¨ Softmax + CrossEntropyLossï¼Œè€Œè¯†åˆ«äººè„¸çš„æ¨¡å‹åˆ™éœ€è¦ä½¿ç”¨å˜ç§å‡½æ•°è®¡ç®—æŸå¤±ï¼Œæœ¬æ–‡ä¸ä¼šè¯¦ç»†ä»‹ç»è¿™äº›å˜ç§å‡½æ•°ï¼Œå¦‚æœä½ æœ‰å…´è¶£å¯ä»¥å‚è€ƒ [CosFace](https://arxiv.org/abs/1801.09414)ï¼Œ[SphereFace](https://arxiv.org/abs/1704.08063v4)ï¼Œå’Œ [ArcFace](https://arxiv.org/abs/1801.07698v2) çš„è®ºæ–‡ã€‚

å› ä¸ºåŸºäºåˆ†ç±»çš„æ–¹æ³•é€Ÿåº¦æ›´å¿«ï¼Œå®ƒæ›´é€‚åˆè®¡ç®—æ•°é‡éå¸¸åºå¤§çš„æ•°æ®é›†ï¼Œè€Œè¿™ç¯‡çš„ä¾‹å­æ”¶é›†åˆ°çš„äººè„¸æ•°æ®æ¯”è¾ƒå°‘ï¼Œæ‰€æœ‰è¿˜æ˜¯ä¼šé‡‡ç”¨åŸºäºæŒ‡æ ‡çš„æ–¹æ³•æ¥å®ç°ï¼Œæ…¢ä¸€ç‚¹å°±æ…¢ä¸€ç‚¹å§ğŸ¤’ã€‚

### å…³äºè®¡ç®—ç¼–ç è·ç¦»çš„è¡¥å……

è®¡ç®—ç¼–ç è·ç¦»ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ï¼Œç¬¬ä¸€ç§æ˜¯è®¡ç®—æ¬§å‡ é‡Œå¾·è·ç¦» (Euclidean Distance)ï¼Œä¹Ÿå°±æ˜¯åœ¨å‰é¢çœ‹åˆ°è¿‡çš„è®¡ç®—æ–¹æ³•ï¼›ç¬¬äºŒç§æ˜¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)ï¼Œå¦‚æœä½ å‚è€ƒåŸºäºåˆ†ç±»çš„æ–¹æ³•çš„è®ºæ–‡ä¼šå‘ç°é‡Œé¢åŸºæœ¬ä¸Šéƒ½ä¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ã€‚

ä½¿ç”¨ pytorch è®¡ç®—æ¬§å‡ é‡Œå¾·è·ç¦»çš„ä¾‹å­å¦‚ä¸‹ï¼ŒTriplet Loss ä½¿ç”¨çš„æ—¶å€™ä¼šé™¤æ‰ sqrt çš„éƒ¨åˆ†ï¼š

``` python
>>> import torch
>>> a = torch.tensor([1, 0, 0, 0.9, 0.1])
>>> b = torch.tensor([1, 0, 0.2, 0.8, 0])
>>> (a - b).pow(2).sum().sqrt()
tensor(0.2449)
// ç»“æœç­‰äº 0 ä»£è¡¨å®Œå…¨ç›¸åŒ
```

ä½¿ç”¨ pytorch è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çš„ä¾‹å­å¦‚ä¸‹ï¼š

``` python
>>> import torch
>>> a = torch.tensor([1, 0, 0, 0.9, 0.1])
>>> b = torch.tensor([1, 0, 0.2, 0.8, 0])
>>> torch.nn.functional.cosine_similarity(a, b, dim=0)
tensor(0.9836)
// ç›¸å½“äº
>>> (a * b).sum() / (a.pow(2).sum().sqrt() * b.pow(2).sum().sqrt())
tensor(0.9836)
// ç»“æœç­‰äº 1 ä»£è¡¨å®Œå…¨ç›¸åŒï¼Œç»“æœç­‰äº 0 ä»£è¡¨å®Œå…¨ç›¸å
```

## å®ç°äººè„¸è®¤è¯çš„æ–¹æ³•

ä½¿ç”¨ä»¥ä¸Šçš„æ–¹æ³•æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°æœ€æ¥è¿‘çš„äººè„¸ï¼Œé‚£è¿™ä¸ªäººè„¸æ˜¯å¦å°±æ˜¯æˆ‘ä»¬ä¼ å…¥çš„äººè„¸æ˜¯åŒä¸€ä¸ªäººå‘¢ï¼Ÿåˆ¤æ–­æ˜¯å¦åŒä¸€ä¸ªäººçš„ä¾æ®å¯ä»¥æ˜¯ç¼–ç è·ç¦»æ˜¯å¦å°äºæŸä¸ªé˜ˆå€¼ï¼Œç„¶è€Œè¿™ä¸ªé˜ˆå€¼æ˜¯å¾ˆéš¾å®šä¹‰çš„ã€‚æœ‰ä¸ªç°è±¡æ˜¯ï¼Œç»è¿‡è®­ç»ƒçš„åŒä¸€ä¸ªäººçš„ç¼–ç è·ç¦»æ˜æ˜¾å°äºæ²¡æœ‰ç»è¿‡è®­ç»ƒçš„åŒä¸€ä¸ªäººçš„ç¼–ç è·ç¦»ï¼Œäººè„¸åˆ†å¸ƒå¯èƒ½ä¼šå¦‚ä¸‹ï¼š

![07](./07.png)

ä¸€ä¸ªæ¯”è¾ƒå¥½çš„æ–¹æ³•æ˜¯è®­ç»ƒå¦å¤–ä¸€ä¸ªä¸“é—¨æ ¹æ®äººè„¸ç¼–ç è·ç¦»åˆ¤æ–­æ˜¯å¦åŒä¸€ä¸ªäººçš„æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹åªæœ‰ä¸€å±‚çº¿æ€§æ¨¡å‹ï¼Œå®ƒä¼šç»™ç¼–ç ä¸­çš„æ¯ä¸ªæŒ‡æ ‡ä¹˜ä»¥ä¸€ä¸ªç³»æ•°ï¼Œç„¶ååŠ ä¸Šåç§»å€¼ï¼Œå†äº¤ç»™ Sigmoid è½¬æ¢åˆ° 0 ï½ 1 ä¹‹é—´çš„å€¼ï¼Œ0 ä»£è¡¨ä¸æ˜¯åŒä¸€ä¸ªäººï¼Œ1 ä»£è¡¨æ˜¯åŒä¸€ä¸ªäººã€‚

æ¨¡å‹çš„å®šä¹‰å¦‚ä¸‹ï¼Œå®Œæ•´ä»£ç å‚è€ƒåé¢ï¼š

``` python
self.verify_model = nn.Sequential(
    nn.Linear(32, 1),
    nn.Sigmoid())
```

å‡è®¾å¦‚æœæœ‰ä»¥ä¸‹çš„ç¼–ç ï¼š

``` python
>>> a = torch.tensor([1, 0, 0, 0.9, 0.1])
>>> b = torch.tensor([1, 0, 0.2, 0.8, 0])
>>> c = torch.tensor([1, 1, 0.9, 0.2, 1])
```

a ä¸ bï¼Œa ä¸ c ä¹‹é—´çš„è·ç¦»å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š

``` python
>>> diff_1 = (a - b).pow(2)
>>> diff_1
tensor([0.0000, 0.0000, 0.0400, 0.0100, 0.0100])
>>> diff_2 = (a - c).pow(2)
>>> diff_2
tensor([0.0000, 1.0000, 0.8100, 0.4900, 0.8100])
```

å†å‡è®¾æ¨¡å‹å‚æ•°å¦‚ä¸‹ï¼š

``` python
>>> w = torch.tensor([[-1.58, -2.96, -0.8, -0.1, -1.28]]).transpose(0, 1)
>>> b = torch.tensor(3.68)
```

å†åº”ç”¨åˆ°ç¼–ç ç›¸å·®å€¼å°±ä¼šå‘ç° a ä¸ b æ˜¯åŒä¸€ä¸ªäººçš„å¯èƒ½æ€§å¾ˆé«˜ (æ¥è¿‘ 1)ï¼Œè€Œ a ä¸ c æ˜¯åŒä¸€ä¸ªäººçš„å¯èƒ½æ€§å¾ˆä½ (æ¥è¿‘ 0)ï¼š

``` python
>>> torch.nn.functional.sigmoid(diff_1.unsqueeze(0).mm(w) + b)
tensor([[0.9743]])
>>> torch.nn.functional.sigmoid(diff_2.unsqueeze(0).mm(w) + b)
tensor([[0.2662]])
```

è®­ç»ƒäººè„¸è®¤è¯æ¨¡å‹çš„ä»£ç ä¼šåœ¨åé¢ç»™å‡ºã€‚

çœ‹åˆ°è¿™é‡Œä½ å¯èƒ½ä¼šé—®ï¼Œä¸ºä»€ä¹ˆéœ€è¦ç»™ç¼–ç ä¸­çš„æŒ‡æ ‡åˆ†åˆ«è®­ç»ƒä¸åŒçš„ç³»æ•°å‘¢ï¼Ÿä¸èƒ½ç›´æ¥ç”¨ sum ç›¸åŠ èµ·æ¥ï¼Œå†æ ¹æ®è¿™ä¸ªç›¸åŠ çš„å€¼æ¥åˆ¤æ–­å—ï¼Ÿæƒ³æƒ³ç¼–ç é‡Œé¢çš„å†…å®¹ä»£è¡¨äº†ä»€ä¹ˆï¼Œæ¨¡å‹ä¸ºäº†åŒºåˆ†äººè„¸ï¼Œéœ€è¦ç»™ä¸åŒçš„äººç‰©åˆ†é…ä¸åŒçš„ç¼–ç ï¼Œè€Œè¿™ä¸ªç¼–ç å®é™…ä¸Šå°±éšå«äº†äººç‰©çš„å±æ€§ï¼Œä¾‹å¦‚æŸä¸ªæŒ‡æ ‡å¯èƒ½ä»£è¡¨äººç‰©çš„æ€§åˆ«ï¼ŒæŸä¸ªæŒ‡æ ‡å¯èƒ½ä»£è¡¨äººç‰©çš„å¹´é¾„ï¼ŒæŸä¸ªæŒ‡æ ‡å¯èƒ½ä»£è¡¨äººç‰©çš„å™¨å®˜å½¢çŠ¶ï¼Œè¿™äº›æŒ‡æ ‡çš„ç›¸å·®å€¼æœ‰çš„ä¼šæ›´é‡è¦ï¼Œä¾‹å¦‚ä»£è¡¨æ€§åˆ«çš„æŒ‡æ ‡ä¸ä¸€è‡´é‚£å°±è‚¯å®šæ˜¯ä¸åŒçš„äººäº†ï¼Œè€Œä»£è¡¨å¹´é¾„çš„æŒ‡æ ‡ä¸ä¸€è‡´åˆ™è¿˜æœ‰ä½™åœ°ã€‚ç»™æ¯ä¸ªæŒ‡æ ‡åˆ†åˆ«è®­ç»ƒä¸åŒçš„ç³»æ•° (ä¹Ÿå¯ä»¥ç§°ä¸ºæƒé‡) å¯ä»¥æ›´ç²¾å‡†çš„åˆ¤æ–­æ˜¯å¦åŒä¸€ä¸ªäººã€‚

## å‡†å¤‡è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†

å¥½äº†ï¼Œåˆåˆ°åŠ¨æ‰‹çš„æ—¶å€™äº†ğŸ¤—ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦å‡†å¤‡æ•°æ®é›†ï¼Œè¿™æ¬¡è¿˜æ˜¯åœ¨ kaggle ä¸Šæ‰’ï¼Œä¸€å…±æœ‰ä¸‰ä¸ªæ•°æ®é›†ç¬¦åˆè¦æ±‚ï¼Œåœ°å€å¦‚ä¸‹ï¼š

- https://www.kaggle.com/atulanandjha/lfwpeople
- https://www.kaggle.com/vasukipatel/face-recognition-dataset
- https://www.kaggle.com/hereisburak/pins-face-recognition

åˆè®¡ä¸€å…±æœ‰ 5855 ä¸ªäººå’Œ 33329 å¼ å›¾ç‰‡ï¼Œå’Œå…¶ä»–å…¬ç”¨æ•°æ®é›†ä¸€æ ·ï¼Œé‡Œé¢å¤§éƒ¨åˆ†æ˜¯ç™½äººï¼Œå¯¹äºšæ´²äººå’Œé»‘äººçš„æ•ˆæœä¼šæ‰“ä¸ªæŠ˜ğŸ¤’ã€‚è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹é€šå¸¸éœ€è¦ä¸Šç™¾ä¸‡å¼ äººè„¸ï¼Œè€Œè¿™é‡Œåªæœ‰ä¸‰ä¸‡å¤šå¼ ï¼Œæ‰€ä»¥é¢„è®¡ç²¾ç¡®åº¦ä¼šç¨å¾®ä½ä¸€äº›ğŸ¤•ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé‡Œé¢æœ‰ç›¸å½“ä¸€éƒ¨åˆ†äººç‰©æ˜¯åªæœ‰ä¸€å¼ å›¾ç‰‡çš„ï¼Œè¿™ç§äººç‰©ä¼šåªæ‹¿æ¥å½“è´Ÿæ ·æœ¬ (ä¸åŒçš„äººç‰©) ä½¿ç”¨ã€‚

æ­¤å¤–ï¼Œè®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹çš„æ—¶å€™äººè„¸çš„ä½ç½®å’Œå æ¯”è¦æ¯”è¾ƒæ ‡å‡†ï¼Œæ•°æ®è´¨é‡ä¼šç›´æ¥å½±å“è®­ç»ƒå‡ºæ¥çš„æ­£ç¡®ç‡ã€‚ä»¥ä¸Šä¸‰ä¸ªæ•°æ®é›†çš„äººè„¸å›¾ç‰‡éƒ½æ˜¯ç»è¿‡é¢„å¤„ç†çš„ï¼Œä¸éœ€è¦ä½¿ç”¨ä¸Šä¸€ç¯‡æ–‡ç« ä»‹ç»çš„æ¨¡å‹æ¥è°ƒæ•´ä¸­å¿ƒç‚¹ï¼Œä½†æ•°æ®é›†çš„äººè„¸å æ¯”ä¸ä¸€æ ·ï¼Œæ‰€ä»¥ä¼šç»è¿‡è£å‰ªå†å‚ä¸è®­ç»ƒã€‚

![08](./08.png)

è£å‰ªæ¯”ä¾‹åˆ†åˆ«æ˜¯ï¼š

- ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼šä¸­å¿ƒ 50%
- ç¬¬äºŒä¸ªæ•°æ®é›†ï¼šä¸è£å‰ª
- ç¬¬ä¸‰ä¸ªæ•°æ®é›†ï¼šä¸­å¿ƒ 70%

è¿è¡Œåé¢çš„ä»£ç ä»¥åå¯ä»¥åˆ° `debug_faces` ç›®å½•ä¸‹æŸ¥çœ‹è£å‰ªåçš„äººè„¸ï¼Œå†…å®¹å¤§è‡´å¦‚ä¸‹ï¼š

![face1](./face1.png)

## å®Œæ•´ä»£ç 

åˆ°ä»‹ç»å®Œæ•´ä»£ç çš„æ—¶å€™äº†ğŸ˜­ï¼Œä»¥ä¸‹ä»£ç åŒ…å«äº†äººè„¸è¯†åˆ«æ¨¡å‹å’Œäººè„¸è®¤è¯æ¨¡å‹ï¼Œéœ€è¦å…ˆè®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ï¼Œå†è®­ç»ƒäººè„¸è®¤è¯æ¨¡å‹ï¼Œä»£ç ä¸‹é¢ä¼šç»™å‡ºä¸€äº›å®ç°ç»†èŠ‚çš„è¯´æ˜ã€‚

``` python
import os
import sys
import torch
import gzip
import itertools
import random
import numpy
import math
import json
import torchvision
from PIL import Image
from torch import nn
from matplotlib import pyplot
from collections import defaultdict
from functools import lru_cache

# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (80, 80)
# è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†è·¯å¾„
DATASET_1_DIR = "./dataset/lfwpeople/lfw_funneled"
DATASET_2_DIR = "./dataset/face-recognition-dataset/Faces/Faces"
DATASET_3_DIR = "./dataset/105_classes_pins_dataset"
# æ¯ä¸€è½®è®­ç»ƒä¸­æ ·æœ¬çš„é‡å¤æ¬¡æ•°
REPEAT_SAMPLES = 2
# ç”¨äºå¯¹æ¯”çš„ä¸åŒäººç‰© (è´Ÿæ ·æœ¬) æ•°é‡
NEGATIVE_SAMPLES = 10
# è´Ÿæ ·æœ¬ä¸­éšæœºæŠ½å–çš„æ•°é‡
NEGATIVE_RANDOM_SAMPLES = 3
# è·³è¿‡æœ€æ¥è¿‘çš„äººè„¸æ•°é‡
# é¿å…åŒèƒèƒé—®é¢˜ï¼š
# å¦‚æœä½ ç»™æ¨¡å‹å¤§é‡å¾ˆç›¸ä¼¼çš„äººè„¸ (æœ‰å¯èƒ½å› ä¸ºè¯¯æ ‡è®°ï¼Œæœ‰å¯èƒ½å› ä¸ºå›¾ç‰‡è´¨é‡å¾ˆä½ï¼Œä¹Ÿæœ‰å¯èƒ½å› ä¸ºçœŸç›¸ä¼¼)
# ç„¶åè·Ÿæ¨¡å‹è¯´ä¸æ˜¯åŒä¸€ä¸ªäººï¼Œä¸‹æ¬¡æ¨¡å‹çœ‹åˆ°æœªç»è¿‡è®­ç»ƒçš„åŒä¸€ä¸ªäººä¹Ÿä¼šè®¤ä¸ºä¸æ˜¯
# Facenet è®ºæ–‡ä¸­é¿å…è¿™ä¸ªé—®é¢˜ä½¿ç”¨çš„æ–¹æ³•æ˜¯è®¡ç®—å±€éƒ¨æœ€æ¥è¿‘çš„ä¸åŒäººç‰©
# è€Œè¿™é‡Œä¼šè®¡ç®—å…¨å±€æœ€æ¥è¿‘ä½†è·³è¿‡æ’åœ¨å‰é¢çš„äººè„¸ï¼Œæ•°æ®é‡ä¸å¤šçš„æ—¶å€™å¯ä»¥è¿™ä¹ˆåš
NEGATIVE_SKIP_NEAREST = 20
# è¯†åˆ«åŒä¸€äººç‰©æœ€å°‘è¦æ±‚çš„å›¾ç‰‡æ•°é‡
MINIMAL_POSITIVE_SAMPLES = 2
# å¤„ç†å›¾ç‰‡å‰æ˜¯å¦å…ˆè½¬æ¢ä¸ºé»‘ç™½å›¾ç‰‡
USE_GRAYSCALE = True

# ç”¨äºå¯ç”¨ GPU æ”¯æŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FaceRecognitionModel(nn.Module):
    """äººè„¸è¯†åˆ«æ¨¡å‹ï¼Œè®¡ç®—ç”¨äºå¯»æ‰¾æœ€æ¥è¿‘äººè„¸çš„ç¼–ç  (åŸºäº ResNet çš„å˜ç§)"""
    # ç¼–ç é•¿åº¦
    EmbeddedSize = 32
    # è¦æ±‚ä¸åŒäººç‰©ç¼–ç ä¹‹é—´çš„è·ç¦» (å¹³æ–¹å€¼åˆè®¡)
    ExclusiveMargin = 0.2

    def __init__(self):
        super().__init__()
        # Resnet çš„å®ç°
        self.resnet = torchvision.models.resnet18(num_classes=256)
        # æ”¯æŒé»‘ç™½å›¾ç‰‡
        if USE_GRAYSCALE:
            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        # æœ€ç»ˆè¾“å‡ºç¼–ç çš„çº¿æ€§æ¨¡å‹
        # å› ä¸º torchvision çš„ resnet æœ€ç»ˆä¼šä½¿ç”¨ä¸€ä¸ª Linearï¼Œè¿™é‡Œçœç•¥æ‰ç¬¬ä¸€ä¸ª Linear
        self.encode_model = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, FaceRecognitionModel.EmbeddedSize))

    def forward(self, x):
        tmp = self.resnet(x)
        y = self.encode_model(tmp)
        return y

    @staticmethod
    def loss_function(predicted):
        """æŸå¤±è®¡ç®—å™¨"""
        losses = []
        verify_positive = torch.ones(1).to(device)
        verify_negative = torch.zeros(NEGATIVE_SAMPLES).to(device)
        for index in range(0, predicted.shape[0], 2 + NEGATIVE_SAMPLES):
            a = predicted[index]   # åŸºç¡€äººç‰©çš„ç¼–ç 
            b = predicted[index+1] # åŸºç¡€äººç‰©çš„ç¼–ç  (å¦ä¸€å¼ å›¾ç‰‡)
            c = predicted[index+2:index+2+NEGATIVE_SAMPLES] # å¯¹æ¯”äººç‰©çš„ç¼–ç 
            # è®¡ç®—ç¼–ç ç›¸å·®å€¼
            diff_positive = (a - b).pow(2).sum()
            diff_negative = (a - c).pow(2).sum(dim=1)
            # è®¡ç®—æŸå¤±
            # ä½¿ç”¨ Triplet Lossï¼Œè¦æ±‚åŒä¸€äººç‰©ç¼–ç è·ç¦»å’Œä¸åŒäººç‰©ç¼–ç è·ç¦»è‡³å°‘ç›¸å·® ExclusiveMargin
            loss = nn.functional.relu(
                diff_positive - diff_negative + FaceRecognitionModel.ExclusiveMargin).sum()
            losses.append(loss)
        loss_total = torch.stack(losses).mean()
        return loss_total

    @staticmethod
    def calc_accuracy(predicted):
        """æ­£ç¡®ç‡è®¡ç®—å™¨"""
        total_count = 0
        correct_count = 0
        for index in range(0, predicted.shape[0], 2 + NEGATIVE_SAMPLES):
            a = predicted[index]   # åŸºç¡€äººç‰©çš„ç¼–ç 
            b = predicted[index+1] # åŸºç¡€äººç‰©çš„ç¼–ç  (å¦ä¸€å¼ å›¾ç‰‡)
            c = predicted[index+2:index+2+NEGATIVE_SAMPLES] # å¯¹æ¯”äººç‰©çš„ç¼–ç 
            # åˆ¤æ–­åŒä¸€äººç‰©çš„ç¼–ç æ˜¯å¦å°äºä¸åŒäººç‰©çš„ç¼–ç 
            diff_positive = (a - b).pow(2).sum()
            diff_negative = (a - c).pow(2).sum(dim=1)
            if (diff_positive < diff_negative).sum() == diff_negative.shape[0]:
                correct_count += 1
            total_count += 1
        return correct_count / total_count

class FaceVerificationModel(nn.Module):
    """äººè„¸è®¤è¯æ¨¡å‹ï¼Œåˆ¤æ–­æ˜¯å¦åŒä¸€ä¸ªäººï¼Œå‚æ•°æ˜¯ç¼–ç ç›¸å·®å€¼çš„å¹³æ–¹"""
    # åˆ¤æ–­æ˜¯å¦åŒä¸€ä¸ªäººçš„é˜ˆå€¼ï¼Œå®é™…ä½¿ç”¨æ¨¡å‹æ—¶å¯ä»¥ç”¨æ›´é«˜çš„å€¼é˜²æ­¢è¯¯åˆ¤
    VerifyThreshold = 0.5

    def __init__(self):
        super().__init__()
        # åˆ¤æ–­æ˜¯å¦åŒä¸€äººç‰©çš„çº¿æ€§æ¨¡å‹
        self.verify_model = nn.Sequential(
            nn.Linear(FaceRecognitionModel.EmbeddedSize, 1),
            nn.Sigmoid())

    def forward(self, x):
        # ç»è¿‡è®­ç»ƒå weight åº”è¯¥æ˜¯è´Ÿæ•°ï¼Œbias åº”è¯¥æ˜¯æ­£æ•°
        y = self.verify_model(x)
        return y.view(-1)

    @staticmethod
    def loss_function(predicted):
        """æŸå¤±è®¡ç®—å™¨"""
        # è¾“å‡ºåº”è¯¥ä¸º [ åŒä¸€äººç‰©, ä¸åŒäººç‰©, ä¸åŒäººç‰©, ..., åŒä¸€äººç‰©, ä¸åŒäººç‰©, ä¸åŒäººç‰©, ... ]
        # è¿™é‡Œéœ€è¦åˆ†åˆ«è®¡ç®—æ­£è´ŸæŸå¤±ï¼Œå¦åˆ™ä¼šå› ä¸ºè´Ÿæ ·æœ¬å å¤šè€Œå¼•èµ· bias è¢«è°ƒæ•´ä¸ºè´Ÿæ•°
        positive_indexes = []
        negative_indexes = []
        for index in list(range(0, predicted.shape[0], 1+NEGATIVE_SAMPLES)):
            positive_indexes.append(index)
            negative_indexes += list(range(index+1, index+1+NEGATIVE_SAMPLES))
        positive_loss = nn.functional.mse_loss(
            predicted[positive_indexes], torch.ones(len(positive_indexes)).to(device))
        negative_loss = nn.functional.mse_loss(
            predicted[negative_indexes], torch.zeros(len(negative_indexes)).to(device))
        return (positive_loss + negative_loss) / 2

    @staticmethod
    def calc_accuracy(predicted):
        """æ­£ç¡®ç‡è®¡ç®—å™¨"""
        positive_correct = 0
        positive_total = 0
        negative_correct = 0
        negative_total = 0
        for index in range(0, predicted.shape[0], 1+NEGATIVE_SAMPLES):
            positive_correct += (predicted[index] >=
                FaceVerificationModel.VerifyThreshold).sum().item()
            negative_correct += (predicted[index+1:index+1+NEGATIVE_SAMPLES] <
                FaceVerificationModel.VerifyThreshold).sum().item()
            positive_total += 1
            negative_total += NEGATIVE_SAMPLES
        # å› ä¸ºè´Ÿæ ·æœ¬å å¤§å¤šæ•°ï¼Œè¿™é‡Œè¿”å›æ­£æ ·æœ¬æ­£ç¡®ç‡å’Œè´Ÿæ ·æœ¬æ­£ç¡®ç‡çš„å¹³å‡å€¼
        return (positive_correct / positive_total + negative_correct / negative_total) / 2

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

# ä¸ºäº†å‡å°‘è¯»å–æ—¶é—´è¿™é‡Œç¼“å­˜äº†è¯»å–çš„ tensor å¯¹è±¡
# å¦‚æœå†…å­˜ä¸å¤Ÿåº”è¯¥é€‚å½“å‡å°‘ maxsize
@lru_cache(maxsize=10000)
def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def calc_resize_parameters(sw, sh):
    """è®¡ç®—ç¼©æ”¾å›¾ç‰‡çš„å‚æ•°"""
    sw_new, sh_new = sw, sh
    dw, dh = IMAGE_SIZE
    pad_w, pad_h = 0, 0
    if sw / sh < dw / dh:
        sw_new = int(dw / dh * sh)
        pad_w = (sw_new - sw) // 2 # å¡«å……å·¦å³
    else:
        sh_new = int(dh / dw * sw)
        pad_h = (sh_new - sh) // 2 # å¡«å……ä¸Šä¸‹
    return sw_new, sh_new, pad_w, pad_h

def resize_image(img):
    """ç¼©æ”¾å›¾ç‰‡ï¼Œæ¯”ä¾‹ä¸ä¸€è‡´æ—¶å¡«å……"""
    sw, sh = img.size
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    img_new = Image.new("RGB", (sw_new, sh_new))
    img_new.paste(img, (pad_w, pad_h))
    img_new = img_new.resize(IMAGE_SIZE)
    return img_new

def image_to_tensor_grayscale(img):
    """ç¼©æ”¾å¹¶è½¬æ¢å›¾ç‰‡å¯¹è±¡åˆ° tensor å¯¹è±¡ (é»‘ç™½)"""
    img = img.convert("L") # è½¬æ¢åˆ°é»‘ç™½å›¾ç‰‡å¹¶ç¼©æ”¾
    arr = numpy.asarray(img)
    t = torch.from_numpy(arr)
    t = t.unsqueeze(0) # æ·»åŠ é€šé“
    t = t / 255.0 # æ­£è§„åŒ–æ•°å€¼ä½¿å¾—èŒƒå›´åœ¨ 0 ~ 1
    return t

def image_to_tensor_rgb(img):
    """ç¼©æ”¾å¹¶è½¬æ¢å›¾ç‰‡å¯¹è±¡åˆ° tensor å¯¹è±¡ (å½©è‰²)"""
    img = img.convert("RGB") # ç¼©æ”¾å›¾ç‰‡
    arr = numpy.asarray(img)
    t = torch.from_numpy(arr)
    t = t.transpose(0, 2) # è½¬æ¢ç»´åº¦ H,W,C åˆ° C,W,H
    t = t / 255.0 # æ­£è§„åŒ–æ•°å€¼ä½¿å¾—èŒƒå›´åœ¨ 0 ~ 1
    return t

if USE_GRAYSCALE:
    image_to_tensor = image_to_tensor_grayscale
else:
    image_to_tensor = image_to_tensor_rgb

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # æˆªå–åçš„äººè„¸å›¾ç‰‡ä¼šä¿å­˜åœ¨ debug_faces æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("debug_faces"):
        os.makedirs("debug_faces")

    # æŸ¥æ‰¾äººç‰©å’Œå¯¹åº”çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
    # { äººç‰©åç§°: [ å›¾ç‰‡è·¯å¾„, å›¾ç‰‡è·¯å¾„, .. ] }
    images_map = defaultdict(lambda: [])
    def add_image(name, path):
        if os.path.splitext(path)[1].lower() not in (".jpg", ".png"):
            return
        name = name.replace(" ", "").replace("-", "").replace(".", "").replace("_", "").lower()
        images_map[name].append(path)
    for dirname in os.listdir(DATASET_1_DIR):
        dirpath = os.path.join(DATASET_1_DIR, dirname)
        if not os.path.isdir(dirpath):
            continue
        for filename in os.listdir(dirpath):
            add_image(dirname, os.path.join(DATASET_1_DIR, dirname, filename))
    for filename in os.listdir(DATASET_2_DIR):
        add_image(filename.split("_")[0], os.path.join(DATASET_2_DIR, filename))
    for dirname in os.listdir(DATASET_3_DIR):
        dirpath = os.path.join(DATASET_3_DIR, dirname)
        name = dirname.replace("pins_", "")
        if not os.path.isdir(dirpath):
            continue
        for filename in os.listdir(dirpath):
            add_image(name, os.path.join(DATASET_3_DIR, dirname, filename))
    images_count = sum(map(len, images_map.values()))
    print(f"found {len(images_map)} peoples and {images_count} images")

    # ä¿å­˜å„ä¸ªäººç‰©çš„å›¾ç‰‡æ•°æ®
    # è¿™é‡Œä¸ç¿»è½¬å›¾ç‰‡ï¼Œå› ä¸ºäººè„¸ç…§ç‰‡é€šå¸¸ä¸ä¼šå·¦å³ç¿»è½¬ï¼Œè€Œä¸”éƒ¨åˆ†å™¨å®˜çš„ç‰¹å¾ä¼šå› å·¦å³æœ‰å·®å¼‚
    img_index = 0
    for index, (name, paths) in enumerate(images_map.items()):
        images = []
        for path in paths:
            img = Image.open(path)
            # è£å‰ªå›¾ç‰‡è®©å„ä¸ªæ•°æ®é›†çš„äººè„¸å æ¯”æ›´æ¥è¿‘
            if path.startswith(DATASET_1_DIR):
                w, h = img.size
                img = img.crop((int(w*0.25), int(h*0.25), int(w*0.75), int(h*0.75)))
            elif path.startswith(DATASET_3_DIR):
                w, h = img.size
                img = img.crop((int(w*0.15), int(h*0.15), int(w*0.85), int(h*0.85)))
            # ä¿å­˜æˆªå–åçš„äººè„¸å›¾ç‰‡ä»¥è°ƒè¯•èŒƒå›´
            img.save(f"debug_faces/{img_index}.png")
            img_index += 1
            images.append(img)
        tensors = [ image_to_tensor(resize_image(img)) for img in images ]
        tensor = torch.stack(tensors) # ç»´åº¦: (å›¾ç‰‡æ•°é‡, 3, å®½åº¦, é«˜åº¦)
        save_tensor(tensor, os.path.join("data", f"{name}.{len(images)}.pt"))
        print(f"saved {index+1}/{len(images_map)} peoples")

    print("done")

def train():
    """å¼€å§‹è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = FaceRecognitionModel().to(device)

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = model.loss_function

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = -1
    validating_accuracy_highest_epoch = 0

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    calc_accuracy = model.calc_accuracy

    # è¯»å–äººç‰©åˆ—è¡¨ï¼ŒåŒºåˆ†å›¾ç‰‡æ•°é‡è¶³å¤Ÿçš„äººç‰©å’Œå›¾ç‰‡æ•°é‡ä¸è¶³çš„äººç‰©
    # å›¾ç‰‡æ•°é‡ä¸è¶³çš„äººç‰©ä¼šä½œä¸ºè´Ÿæ ·æœ¬ä½¿ç”¨
    filenames = os.listdir("data")
    multiple_samples = []
    single_samples = []
    for filename in filenames:
        if int(filename.split('.')[-2]) >= MINIMAL_POSITIVE_SAMPLES:
            multiple_samples.append(filename)
        else:
            single_samples.append(filename)
    random.shuffle(multiple_samples)
    random.shuffle(single_samples)
    total_multiple_samples = len(multiple_samples)
    total_single_samples = len(single_samples)

    # åˆ†å‰²è®­ç»ƒé›† (80%)ï¼ŒéªŒè¯é›† (10%) å’Œæµ‹è¯•é›† (10%)
    training_set = multiple_samples[:int(total_multiple_samples*0.8)]
    training_set_single = single_samples[:int(total_single_samples*0.8)]
    validating_set = multiple_samples[int(total_multiple_samples*0.8):int(total_multiple_samples*0.9)]
    validating_set_single = single_samples[int(total_single_samples*0.8):int(total_single_samples*0.9)]
    testing_set = multiple_samples[int(total_multiple_samples*0.9):]
    testing_set_single = single_samples[int(total_single_samples*0.9):]

    # è®­ç»ƒé›†çš„å„ä¸ªäººç‰©å¯¹åº”çš„ç¼–ç  (åŸºäºæœ€åä»¥åä¸€æ¬¡è®­ç»ƒä½¿ç”¨çš„å›¾ç‰‡)
    training_image_to_vector_index = {}
    training_vector_index_to_image = {}
    for filename in training_set + training_set_single:
        for image_index in range(int(filename.split('.')[1])):
            vector_index = len(training_image_to_vector_index)
            training_image_to_vector_index[(filename, image_index)] = vector_index
            training_vector_index_to_image[vector_index] = (filename, image_index)
    training_vectors = torch.zeros(len(training_image_to_vector_index), FaceRecognitionModel.EmbeddedSize)
    training_vectors_calculated_indices = set()

    # ç”Ÿæˆç”¨äºè®­ç»ƒçš„è¾“å…¥
    # è¿”å› [ åŸºç¡€å›¾ç‰‡, åŒä¸€äººç‰©å›¾ç‰‡ (æ­£æ ·æœ¬), ä¸åŒäººç‰©å›¾ç‰‡ (è´Ÿæ ·æœ¬), ... ]
    def generate_inputs(dataset_multiple, dataset_single, batch_size):
        # è·å–å·²è®¡ç®—è¿‡çš„ç¼–ç 
        is_training = dataset_multiple == training_set
        if is_training:
            calculated_index_list = list(training_vectors_calculated_indices)
            calculated_index_set = set(calculated_index_list)
            calculated_index_to_image = {
                ci: training_vector_index_to_image[vi]
                for ci, vi in enumerate(calculated_index_list)
            }
            training_vectors_calculated = training_vectors[calculated_index_list]
        # æšä¸¾æ•°æ®é›†ï¼Œä¼šé‡å¤ REPEAT_SAMPLES æ¬¡ä»¥å‡å°‘éšæœºé€‰æ‹©å¯¼è‡´çš„æ­£ç¡®ç‡æµ®åŠ¨
        image_tensors = []
        vector_indices = []
        for base_filename in dataset_multiple * REPEAT_SAMPLES:
            # è¯»å–åŸºç¡€äººç‰©çš„å›¾ç‰‡
            base_tensor = load_tensor(os.path.join("data", base_filename))
            base_tensors = list(enumerate(base_tensor))
            # æ‰“ä¹±é¡ºåºï¼Œç„¶åä¸¤å¼ ä¸¤å¼ å›¾ç‰‡çš„é€‰å–åŸºç¡€å›¾ç‰‡å’Œæ­£æ ·æœ¬
            random.shuffle(base_tensors)
            for index in range(0, len(base_tensors)-1, 2):
                # æ·»åŠ åŸºç¡€å›¾ç‰‡å’Œæ­£æ ·æœ¬åˆ°åˆ—è¡¨
                anchor_image_index, anchor_tensor = base_tensors[index]
                positive_image_index, positive_tensor = base_tensors[index+1]
                image_tensors.append(anchor_tensor)
                image_tensors.append(positive_tensor)
                if is_training:
                    vector_indices.append(training_image_to_vector_index[(base_filename, anchor_image_index)])
                    vector_indices.append(training_image_to_vector_index[(base_filename, positive_image_index)])
                # å¦‚æœæ˜¯è®­ç»ƒé›†ï¼Œåˆ™è®¡ç®—åŸºç¡€å›¾ç‰‡çš„ç¼–ç ä¸å…¶ä»–ç¼–ç çš„è·ç¦»
                nearest_indices = []
                if is_training:
                    vector_index = training_image_to_vector_index[(base_filename, anchor_image_index)]
                    if vector_index in calculated_index_set:
                        nearest_indices = ((training_vectors_calculated -
                            training_vectors[vector_index]).abs().sum(dim=1).sort().indices).tolist()
                # é€‰å–è´Ÿæ ·æœ¬
                # å¦‚æœæ˜¯è®­ç»ƒé›†åˆ™é€‰å–ç¼–ç æœ€æ¥è¿‘çš„æ ·æœ¬+éšæœºæ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
                # å¦‚æœæ˜¯éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ™éšæœºé€‰å–æ ·æœ¬
                if is_training and nearest_indices:
                    negative_samples = NEGATIVE_SAMPLES - NEGATIVE_RANDOM_SAMPLES
                    negative_random_samples = NEGATIVE_RANDOM_SAMPLES
                else:
                    negative_samples = 0
                    negative_random_samples = NEGATIVE_SAMPLES
                negative_skip_nearest = NEGATIVE_SKIP_NEAREST
                for calculated_index in nearest_indices:
                    if negative_samples <= 0:
                        break
                    filename, image_index = calculated_index_to_image[calculated_index]
                    if filename == base_filename:
                        continue # è·³è¿‡åŒä¸€äººç‰©
                    if negative_skip_nearest > 0:
                        negative_skip_nearest -= 1
                        continue # è·³è¿‡éå¸¸ç›¸ä¼¼çš„äººç‰©
                    target_tensor = load_tensor(os.path.join("data", filename))
                    # æ·»åŠ è´Ÿæ ·æœ¬åˆ°åˆ—è¡¨
                    image_tensors.append(target_tensor[image_index])
                    if is_training:
                        vector_indices.append(training_image_to_vector_index[(filename, image_index)])
                    negative_samples -= 1
                while negative_random_samples > 0:
                    file_index = random.randint(0, len(dataset_multiple) + len(dataset_single) - 1)
                    if file_index < len(dataset_multiple):
                        filename = dataset_multiple[file_index]
                    else:
                        filename = dataset_single[file_index - len(dataset_multiple)]
                    if filename == base_filename:
                        continue # è·³è¿‡åŒä¸€äººç‰©
                    target_tensor = load_tensor(os.path.join("data", filename))
                    image_index = random.randint(0, target_tensor.shape[0] - 1)
                    # æ·»åŠ è´Ÿæ ·æœ¬åˆ°åˆ—è¡¨
                    image_tensors.append(target_tensor[image_index])
                    if is_training:
                        vector_indices.append(training_image_to_vector_index[(filename, image_index)])
                    negative_random_samples -= 1
                assert negative_samples == 0
                assert negative_random_samples == 0
                # å¦‚æœå›¾ç‰‡æ•°é‡å¤§äºæ‰¹æ¬¡å¤§å°ï¼Œåˆ™è¿”å›æ‰¹æ¬¡
                if len(image_tensors) >= batch_size:
                    yield torch.stack(image_tensors).to(device), vector_indices
                    image_tensors.clear()
                    vector_indices.clear()
        if image_tensors:
            yield torch.stack(image_tensors).to(device), vector_indices

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(0, 200):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼
        model.train()
        training_accuracy_list = []
        for index, (batch_x, vector_indices) in enumerate(
            generate_inputs(training_set, training_set_single, 400)):
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•å„ä¸ªäººç‰©çš„ç¼–ç 
            for vector_index, vector in zip(vector_indices, predicted):
                # å¤åˆ¶å› cpu å¹¶å»æ‰ç”¨äºè‡ªåŠ¨å¾®åˆ†çš„è®¡ç®—è·¯å¾„ä¿¡æ¯
                training_vectors[vector_index] = vector.to("cpu").detach()
                training_vectors_calculated_indices.add(vector_index)
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_batch_accuracy = calc_accuracy(predicted)
            # è¾“å‡ºæ‰¹æ¬¡æ­£ç¡®ç‡
            training_accuracy_list.append(training_batch_accuracy)
            print(f"epoch: {epoch}, batch: {index}, accuracy: {training_batch_accuracy}")
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼
        model.eval()
        validating_accuracy_list = []
        for batch_x, _ in generate_inputs(validating_set, validating_set_single, 100):
            predicted = model(batch_x)
            validating_batch_accuracy = calc_accuracy(predicted)
            validating_accuracy_list.append(validating_batch_accuracy)
            # é‡Šæ”¾ predicted å ç”¨çš„æ˜¾å­˜é¿å…æ˜¾å­˜ä¸è¶³çš„é”™è¯¯
            predicted = None
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨å¤šæ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        # å› ä¸ºéªŒè¯é›†çš„è´Ÿæ ·æœ¬æ˜¯éšæœºé€‰æ‹©çš„ï¼Œå…è®¸ 1% çš„æ³¢åŠ¨ä½¿å¾—æ¨¡å‹å¯ä»¥è®­ç»ƒæ›´å¤šæ¬¡
        if (validating_accuracy + 0.01) > validating_accuracy_highest:
            if validating_accuracy > validating_accuracy_highest:
                validating_accuracy_highest = validating_accuracy
                print("highest validating accuracy updated")
            else:
                print("highest validating accuracy not dropped")
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.recognition.pt")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨å¤šæ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because validating accuracy dropped from highest in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.recognition.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch_x, _ in generate_inputs(testing_set, testing_set_single, 100):
        predicted = model(batch_x)
        testing_batch_accuracy = calc_accuracy(predicted)
        testing_accuracy_list.append(testing_batch_accuracy)
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training_accuracy")
    pyplot.plot(validating_accuracy_history, label="validating_accuracy")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def train_verify():
    """å¼€å§‹è®­ç»ƒäººè„¸è®¤è¯æ¨¡å‹"""
    # åˆ›å»ºäººè„¸è¯†åˆ«æ¨¡å‹å®ä¾‹å¹¶åŠ è½½è®­ç»ƒå¥½çš„å‚æ•°
    recognize_model = FaceRecognitionModel().to(device)
    recognize_model.load_state_dict(load_tensor("model.recognition.pt"))
    recognize_model.eval()

    # åˆ›å»ºäººè„¸è®¤è¯æ¨¡å‹å®ä¾‹
    model = FaceVerificationModel().to(device)

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = model.loss_function

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = -1
    validating_accuracy_highest_epoch = 0

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    calc_accuracy = model.calc_accuracy

    # è¯»å–äººç‰©åˆ—è¡¨ï¼ŒåŒºåˆ†å›¾ç‰‡æ•°é‡è¶³å¤Ÿçš„äººç‰©å’Œå›¾ç‰‡æ•°é‡ä¸è¶³çš„äººç‰©
    # å›¾ç‰‡æ•°é‡ä¸è¶³çš„äººç‰©ä¼šä½œä¸ºè´Ÿæ ·æœ¬ä½¿ç”¨
    filenames = os.listdir("data")
    multiple_samples = []
    single_samples = []
    for filename in filenames:
        if int(filename.split('.')[-2]) >= MINIMAL_POSITIVE_SAMPLES:
            multiple_samples.append(filename)
        else:
            single_samples.append(filename)
    random.seed(123) # è®©è¿™é‡Œçš„é¡ºåºè·Ÿè®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹æ—¶çš„é¡ºåºä¸ä¸€æ ·
    random.shuffle(multiple_samples)
    random.shuffle(single_samples)
    total_multiple_samples = len(multiple_samples)
    total_single_samples = len(single_samples)

    # åˆ†å‰²è®­ç»ƒé›† (80%)ï¼ŒéªŒè¯é›† (10%) å’Œæµ‹è¯•é›† (10%)
    training_set = multiple_samples[:int(total_multiple_samples*0.8)]
    training_set_single = single_samples[:int(total_single_samples*0.8)]
    validating_set = multiple_samples[int(total_multiple_samples*0.8):int(total_multiple_samples*0.9)]
    validating_set_single = single_samples[int(total_single_samples*0.8):int(total_single_samples*0.9)]
    testing_set = multiple_samples[int(total_multiple_samples*0.9):]
    testing_set_single = single_samples[int(total_single_samples*0.9):]

    # ç¼–ç çš„ç¼“å­˜ { (æ–‡ä»¶å,ç´¢å¼•å€¼): ç¼–ç  }
    vector_cache = {}

    # æ ¹æ®å›¾ç‰‡è·å–ç¼–ç 
    def get_vector(filename, image_index, image_tensor):
        key = (filename, image_index)
        vector = vector_cache.get(key)
        if vector is None:
            with torch.no_grad():
                vector = recognize_model(image_tensor.unsqueeze(0).to(device))[0].to("cpu")
            vector_cache[key] = vector
        return vector

    # ç”Ÿæˆç”¨äºè®­ç»ƒçš„è¾“å…¥
    # è¿”å› [ åŒä¸€äººç‰©ç¼–ç çš„å·®å¼‚, ä¸åŒäººç‰©ç¼–ç çš„å·®å¼‚, ... ]
    def generate_inputs(dataset_multiple, dataset_single, batch_size):
        # æšä¸¾æ•°æ®é›†ï¼Œä¼šé‡å¤ REPEAT_SAMPLES æ¬¡ä»¥å‡å°‘éšæœºé€‰æ‹©å¯¼è‡´çš„æ­£ç¡®ç‡æµ®åŠ¨
        diff_tensors = []
        for base_filename in dataset_multiple * REPEAT_SAMPLES:
            # è¯»å–åŸºç¡€äººç‰©çš„å›¾ç‰‡
            base_tensor = load_tensor(os.path.join("data", base_filename))
            base_tensors = list(enumerate(base_tensor))
            # æ‰“ä¹±é¡ºåºï¼Œç„¶åä¸¤å¼ ä¸¤å¼ å›¾ç‰‡çš„é€‰å–åŸºç¡€å›¾ç‰‡å’Œæ­£æ ·æœ¬
            random.shuffle(base_tensors)
            for index in range(0, len(base_tensors)-1, 2):
                # è®¡ç®—åŸºç¡€å›¾ç‰‡å’Œæ­£æ ·æœ¬çš„ç¼–ç å·®å¼‚å¹¶æ·»åŠ åˆ°åˆ—è¡¨
                anchor_image_index, anchor_tensor = base_tensors[index]
                positive_image_index, positive_tensor = base_tensors[index+1]
                anchor_vector = get_vector(base_filename, anchor_image_index, anchor_tensor)
                positive_vector = get_vector(base_filename, positive_image_index, positive_tensor)
                diff_tensors.append((anchor_vector - positive_vector).pow(2))
                # éšæœºé€‰å–è´Ÿæ ·æœ¬ï¼Œè®¡ç®—å·®å¼‚å¹¶æ·»åŠ åˆ°åˆ—è¡¨
                negative_random_samples = NEGATIVE_SAMPLES
                while negative_random_samples > 0:
                    file_index = random.randint(0, len(dataset_multiple) + len(dataset_single) - 1)
                    if file_index < len(dataset_multiple):
                        filename = dataset_multiple[file_index]
                    else:
                        filename = dataset_single[file_index - len(dataset_multiple)]
                    if filename == base_filename:
                        continue # è·³è¿‡åŒä¸€äººç‰©
                    target_tensor = load_tensor(os.path.join("data", filename))
                    image_index = random.randint(0, target_tensor.shape[0] - 1)
                    negative_vector = get_vector(filename, image_index, target_tensor[image_index])
                    diff_tensors.append((anchor_vector - negative_vector).pow(2))
                    negative_random_samples -= 1
                # å¦‚æœå·®å¼‚æ•°é‡å¤§äºæ‰¹æ¬¡å¤§å°ï¼Œåˆ™è¿”å›æ‰¹æ¬¡
                if len(diff_tensors) >= batch_size:
                    yield torch.stack(diff_tensors).to(device)
                    diff_tensors.clear()
        if diff_tensors:
            yield torch.stack(diff_tensors).to(device)

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 20):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼
        model.train()
        training_accuracy_list = []
        for index, batch_x in enumerate(
            generate_inputs(training_set, training_set_single, 400)):
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_batch_accuracy = calc_accuracy(predicted)
            # è¾“å‡ºæ‰¹æ¬¡æ­£ç¡®ç‡
            training_accuracy_list.append(training_batch_accuracy)
            print(f"epoch: {epoch}, batch: {index}, accuracy: {training_batch_accuracy}")
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼
        model.eval()
        validating_accuracy_list = []
        for batch_x in generate_inputs(validating_set, validating_set_single, 100):
            predicted = model(batch_x)
            validating_batch_accuracy = calc_accuracy(predicted)
            validating_accuracy_list.append(validating_batch_accuracy)
            # é‡Šæ”¾ predicted å ç”¨çš„æ˜¾å­˜é¿å…æ˜¾å­˜ä¸è¶³çš„é”™è¯¯
            predicted = None
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨å¤šæ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        # å› ä¸ºéªŒè¯é›†çš„è´Ÿæ ·æœ¬æ˜¯éšæœºé€‰æ‹©çš„ï¼Œå…è®¸ 1% çš„æ³¢åŠ¨ä½¿å¾—æ¨¡å‹å¯ä»¥è®­ç»ƒæ›´å¤šæ¬¡
        if (validating_accuracy + 0.01) > validating_accuracy_highest:
            if validating_accuracy > validating_accuracy_highest:
                validating_accuracy_highest = validating_accuracy
                print("highest validating accuracy updated")
            else:
                print("highest validating accuracy not dropped")
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.verification.pt")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨å¤šæ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because validating accuracy dropped from highest in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.verification.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch_x in generate_inputs(testing_set, testing_set_single, 100):
        predicted = model(batch_x)
        testing_batch_accuracy = calc_accuracy(predicted)
        testing_accuracy_list.append(testing_batch_accuracy)
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training_accuracy")
    pyplot.plot(validating_accuracy_history, label="validating_accuracy")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "train-verify":
        train_verify()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

ä»¥ä¸‹æ˜¯ä¸€äº›å®ç°ä¸Šçš„ç»†èŠ‚ã€‚

- å› ä¸ºå›¾ç‰‡æ•°é‡ä¸å¤šï¼Œåœ¨å¤„ç†å›¾ç‰‡ä¹‹å‰ä¼šå…ˆè½¬æ¢åˆ°é»‘ç™½å›¾ç‰‡å’Œç¼©æ”¾åˆ° 80x80 ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Œå¦‚æœå›¾ç‰‡æ•°é‡è¶³å¤Ÿå¯ä»¥ä¸éœ€è¦è½¬æ¢åˆ°é»‘ç™½å’Œä½¿ç”¨æ›´é«˜çš„åƒç´ å€¼
- è·Ÿ Facenet è®ºæ–‡ä¸ä¸€æ ·ï¼Œè¿™ä¸ªå®ç°ä¼šå…¨å±€æŸ¥æ‰¾ä¸åŒäººç‰©ä½†ç¼–ç è·ç¦»æœ€æ¥è¿‘çš„äººè„¸ï¼ŒåŒæ—¶è·³è¿‡å‰ 20 ä¸ªäººè„¸
- è·Ÿ Facenet è®ºæ–‡ä¸€æ ·ï¼Œè¿™ä¸ªå®ç°çš„ç¼–ç é•¿åº¦æ˜¯ 32ï¼Œè¦æ±‚ä¸åŒäººç‰©çš„è·ç¦»æ˜¯ 0.2
- è¿™ä¸ªå®ç°é€‰æ‹©äººç‰©çš„æ—¶å€™ä¸ä¼šåªé€‰æ‹© 3 å¼ å›¾ç‰‡ï¼Œè€Œæ˜¯é€‰æ‹© 1 å¼ åŸºç¡€å›¾ç‰‡ï¼Œ1 å¼ æ­£æ ·æœ¬å›¾ç‰‡ (åŒä¸€ä¸ªäºº) å’Œ 10 å¼ è´Ÿæ ·æœ¬å›¾ç‰‡ (ä¸åŒçš„äºº)ï¼Œè¿™æ ·å¯ä»¥è®©è®¡ç®—é€Ÿåº¦æ›´å¿«
- äººè„¸è¯†åˆ«æ¨¡å‹çš„éª¨å¹²ç½‘ç»œè¿˜æ˜¯ç”¨äº† ResNetï¼Œå¦‚æœæœ‰å…´è¶£å¯ä»¥è¯•è¯• Facenet ä½¿ç”¨çš„ Inception æ¨¡å‹ï¼Œtorchvision åŒæ ·æœ‰æä¾›
- äººè„¸è¯†åˆ«æ¨¡å‹çš„æ­£ç¡®ç‡ä¼šä¾æ®é€‰å–çš„"åŸºç¡€å›¾ç‰‡å’Œæ­£æ ·æœ¬å›¾ç‰‡çš„ç¼–ç è·ç¦»"æ˜¯å¦å°äº"åŸºç¡€å›¾ç‰‡å’Œæ‰€æœ‰è´Ÿæ ·æœ¬å›¾ç‰‡çš„ç¼–ç è·ç¦»"è®¡ç®—
- å› ä¸ºéšæœºé€‰æ‹©ä¼šå¯¼è‡´éªŒè¯é›†æ­£ç¡®ç‡æµ®åŠ¨ï¼Œè¿™é‡Œä¼šå…è®¸æœ€é«˜æ­£ç¡®ç‡æµ®åŠ¨ 1%ï¼Œå¹¶ä¸”æœ€å¤šè®­ç»ƒæ¬¡æ•°ä¼šæœ‰æ¯”è¾ƒä½çš„é™åˆ¶

æŠŠä»£ç ä¿å­˜åˆ° face_recognition.pyï¼Œç„¶åæŒ‰ä»¥ä¸‹æ–‡ä»¶å¤¹ç»“æ„å­˜æ”¾ï¼š

- face-recognition
    - dataset
        - 105_classes_pins_dataset
        - face-recognition-dataset
            - Faces
        - lfwpeople
            - lfw_funneled
    - face_recognition.py

å†æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ï¼š

``` text
python3 face_recognition.py prepare
python3 face_recognition.py train
```

æœ€ç»ˆè¾“å‡ºç»“æœå¦‚ä¸‹ï¼Œå¤§æ¦‚ 83%~85% çš„æœ€æ¥è¿‘çš„äººè„¸æ˜¯åŒä¸€ä¸ªäººğŸ¤’ï¼š

``` text
epoch: 199, batch: 646, accuracy: 1.0
epoch: 199, batch: 647, accuracy: 1.0
epoch: 199, batch: 648, accuracy: 1.0
training accuracy: 0.9798332275899607
validating accuracy: 0.8293447293447292
highest validating accuracy updated
highest validating accuracy: 0.8293447293447292 from epoch 199
testing accuracy: 0.8504521963824275
```

æ¥ä¸‹æ¥å†æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯è®­ç»ƒäººè„¸è®¤è¯æ¨¡å‹ï¼š

``` text
python3 face_recognition.py train-verify
```

æœ€ç»ˆè¾“å‡ºç»“æœå¦‚ä¸‹ï¼Œä»¤äººåƒæƒŠçš„æ˜¯è¯†åˆ«æ˜¯å¦åŒä¸€ä¸ªäººçš„ç²¾åº¦è¾¾åˆ°äº† 96%~97% (è¿™è¿˜æ˜¯æ­£è´Ÿæ ·æœ¬æ­£ç¡®ç‡åˆ†åˆ«è®¡ç®—å†å¹³å‡åçš„å€¼)ï¼Œè¿™è¯´æ˜äº†å³ä½¿æ¬§å‡ é‡Œå¾·è·ç¦»æœ€æ¥è¿‘ä¹Ÿä¸ä¸€å®šæ˜¯åŒä¸€ä¸ªäººï¼Œäººè„¸è®¤è¯æ¨¡å‹ç»™å„é¡¹æŒ‡æ ‡åˆ†é…ä¸åŒçš„ç³»æ•°æ¥è®¡ç®—æ˜¯æœ‰ä¸€å®šæ•ˆæœçš„ã€‚åé¢æˆ‘ä»¬åº”ç”¨æ¨¡å‹çš„æ—¶å€™ï¼Œä¼šå…ˆé€‰å–æœ€æ¥è¿‘çš„ N å¼ äººè„¸ï¼Œå†ä½¿ç”¨äººè„¸è®¤è¯æ¨¡å‹æ¥ä¸€ä¸ªä¸ªåˆ¤æ–­æ˜¯å¦åŒä¸€ä¸ªäººã€‚

``` text
epoch: 19, batch: 569, accuracy: 0.9283783783783783
epoch: 19, batch: 570, accuracy: 0.9567567567567568
epoch: 19, batch: 571, accuracy: 0.9304347826086956
training accuracy: 0.9617334193421163
validating accuracy: 0.9590857605178013
highest validating accuracy not dropped
highest validating accuracy: 0.9623381877022668 from epoch 19
testing accuracy: 0.9714264705882371
```

å’Œä¹‹å‰å‡ ç¯‡æ–‡ç« ä¸ä¸€æ ·çš„æ˜¯ï¼Œè¿™ä»½ä»£ç æ²¡æœ‰ç»™å‡º `eval_model` å‡½æ•°ï¼Œè¿™æ˜¯å› ä¸ºäººè„¸å›¾ç‰‡é€šå¸¸éƒ½éœ€è¦ç»è¿‡é¢„å¤„ç†ï¼Œä¸ä¼šç›´æ¥äº¤ç»™æ¨¡å‹ä½¿ç”¨ã€‚ä¸‹é¢å°†ä¼šä»‹ç»å¦‚ä½•ç»“åˆ faster-rcnn æ¨¡å‹æ£€æµ‹å›¾ç‰‡ä¸­çš„äººè„¸ï¼Œå†æ ¹æ®ä¸Šä¸€ç¯‡æ–‡ç« ä»‹ç»çš„è„¸éƒ¨å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹è°ƒæ•´äººè„¸èŒƒå›´ï¼Œæœ€åæˆªå–äººè„¸äº¤ç»™äººè„¸è¯†åˆ«æ¨¡å‹å’Œäººè„¸è®¤è¯æ¨¡å‹ã€‚

## ç»“åˆå‰å‡ ç¯‡çš„æ¨¡å‹å®ç°äººè„¸æ£€æµ‹ + è¯†åˆ«

å‡è®¾æˆ‘ä»¬æƒ³è¯†åˆ«æŸå¼ å›¾ç‰‡ä¸Šçš„äººæ˜¯è°ï¼Œç»„åˆæœ¬æ–‡ä»‹ç»çš„æ¨¡å‹å’Œå‰å‡ ç¯‡æ–‡ç« ä»‹ç»çš„æ¨¡å‹å³å¯å¾—å‡ºä»¥ä¸‹çš„æµç¨‹ï¼š

![09](./09.png)

é¦–å…ˆæˆ‘ä»¬æŠŠä¸‰ä¸ªæ¨¡å‹çš„ä»£ç å’Œè®­ç»ƒå¥½çš„å‚æ•°æ–‡ä»¶ (model.pt) æŒ‰ä»¥ä¸‹ç»“æ„å­˜æ”¾

- faster-rcnn
    - fasterrcnn.py: æ¥æºåœ¨[è¿™é‡Œ](https://303248153.github.io/ml-10_5/)
    - model.pt: å¯ä»¥åˆ°[è¿™é‡Œ](https://github.com/303248153/303248153.github.io/releases/tag/202001)ä¸‹è½½ï¼Œä¸‹è½½åéœ€è¦é‡å‘½å (`faster-rcnn-10.5-model.pt`
 => `model.pt`)
- face-landmark
    - face_landmark.py: æ¥æºåœ¨[è¿™é‡Œ](https://303248153.github.io/ml-12/)
    - model.pt: å¯ä»¥åˆ°[è¿™é‡Œ](https://github.com/303248153/303248153.github.io/releases/tag/202103)ä¸‹è½½ï¼Œä¸‹è½½åéœ€è¦é‡å‘½å (`face-landmark-model.pt` => `model.pt`)
- face-recognition
    - face_recognition.py: æ¥æºåœ¨è¿™ç¯‡æ–‡ç« çš„ä¸Šé¢
    - model.recognition.pt: å¯ä»¥åˆ°[è¿™é‡Œ](https://github.com/303248153/303248153.github.io/releases/tag/202103)ä¸‹è½½ï¼Œä¸‹è½½åéœ€è¦é‡å‘½å (`face-recognition-model.recognition.pt` => `model.recognition.pt`)
    - model.verification.pt: å¯ä»¥åˆ°[è¿™é‡Œ](https://github.com/303248153/303248153.github.io/releases/tag/202103)ä¸‹è½½ï¼Œä¸‹è½½åéœ€è¦é‡å‘½å (`face-recognition-model.verification.pt` => `model.verification.pt`)

ä¹‹åæˆ‘ä»¬éœ€è¦å‡†å¤‡æ„å»ºäººè„¸ç¼–ç æ•°æ®åº“çš„äººè„¸ï¼Œä¹Ÿå°±æ˜¯è¯†åˆ«çš„å¯¹è±¡ï¼Œè¿™é‡Œæˆ‘ä¸ºäº†æ–¹ä¾¿æµ‹è¯•æ”¶é›†äº†ä¸€äº›ä¸­å›½é¢†å¯¼äººçš„ç…§ç‰‡ (è¿™æ˜¯é¦–å…ˆè®©æ¨¡å‹è®¤é¢†å¯¼ğŸ˜¤)ï¼Œå¯ä»¥åˆ°[è¿™é‡Œ](https://github.com/303248153/303248153.github.io/tree/master/ml-13/face_recognition_samples)ä¸‹è½½ã€‚

ç„¶åå°±æ˜¯ç»“åˆè¿™äº›æ¨¡å‹çš„ä»£ç äº†ï¼š

``` python
import sys
import os
import torch
from collections import defaultdict
from PIL import Image, ImageDraw, ImageFont

# åŠ è½½æ¨¡å‹æ‰€åœ¨çš„æ¨¡å—
sys.path.append("../face-landmark")
sys.path.append("../face-recognition")
import fasterrcnn
import face_landmark
import face_recognition

# æ„å»ºç¼–ç æ•°æ®åº“ä½¿ç”¨çš„æ–‡ä»¶å¤¹
# è¦æ±‚å›¾ç‰‡è·¯å¾„æ˜¯ {SAMPLES_DIR}/{äººç‰©åç§°}/å›¾ç‰‡å
SAMPLES_DIR = "./face_recognition_samples"

# è¾“å‡ºäººè„¸å›¾ç‰‡ä½¿ç”¨çš„æ–‡ä»¶å¤¹
FACES_OUTPUT_DIR = "./detected_faces"

# ç¼–ç æ•°æ®åº“çš„ä¿å­˜è·¯å¾„
# å¦‚æœéœ€è¦é‡æ–°æ„å»ºè¯·åˆ é™¤æ­¤æ–‡ä»¶
VECTORS_PATH = "./face_recognition_vectors.pt"

# ä½¿ç”¨å…³é”®ç‚¹è°ƒæ•´è„¸éƒ¨èŒƒå›´çš„æ¬¡æ•°
ADJUST_FACE_REGION_BY_LANDMARK_TIMES = 3

# è„¸éƒ¨èŒƒå›´ç›¸å¯¹å…³é”®ç‚¹èŒƒå›´çš„æ¯”ä¾‹
FACE_REGION_RATIO_BY_LANDMARK = 3.0

# åˆ¤æ–­æ˜¯åŒä¸€äººç‰©éœ€è¦çš„åˆ†æ•°é˜ˆå€¼
FACE_VERIFICATION_THRESHOLD = 0.92

# ç”¨äºå¯ç”¨ GPU æ”¯æŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def adjust_face_region(image, region, face_landmark_model):
    """ä½¿ç”¨å…³é”®ç‚¹è°ƒæ•´è„¸éƒ¨èŒƒå›´"""
    x, y, w, h = region
    child_img = image.crop((x, y, x+w, y+h))
    points = []
    for _ in range(ADJUST_FACE_REGION_BY_LANDMARK_TIMES):
        points = face_landmark_model.detect_landmarks([child_img])[0]
        min_x = min(p[0] for p in points)
        min_y = min(p[1] for p in points)
        max_x = max(p[0] for p in points)
        max_y = max(p[1] for p in points)
        landmark_w = max_x - min_x
        landmark_h = max_y - min_y
        center_x = min_x + landmark_w * 0.5 + x
        center_y = min_y + landmark_h * 0.5 + y
        radius = max(landmark_w, landmark_h) * FACE_REGION_RATIO_BY_LANDMARK / 2
        x0 = int(min(max(center_x - radius, 0), image.size[0]-1))
        x1 = int(min(max(center_x + radius, 0), image.size[0]-1))
        y0 = int(min(max(center_y - radius, 0), image.size[1]-1))
        y1 = int(min(max(center_y + radius, 0), image.size[1]-1))
        x_diff = x0 - x
        y_diff = y0 - y
        x, y, w, h = x0, y0, x1-x0, y1-y0
        points = [ (px - x_diff, py - y_diff) for px, py in points ]
        if w == 0 or h == 0:
            # æ£€æµ‹å…³é”®ç‚¹å¤±è´¥ï¼Œè¿”å›åŸå§‹èŒƒå›´
            return child_img, region, []
        child_img = image.crop((x, y, x+w, y+h))
    return child_img, (x, y, w, h), points

def build_vectors(faster_rcnn_model, face_landmark_model, face_recognition_model):
    """æ ¹æ®äººç‰©å’Œå›¾ç‰‡æ„å»ºç¼–ç æ•°æ®åº“"""
    print("building face recognition vectors from samples")
    names = []
    images = []
    result_names = []
    result_vectors = []
    batch_size = 10
    def process_images():
        # æŸ¥æ‰¾äººè„¸åŒºåŸŸ
        tensor_in = torch.stack([
            fasterrcnn.image_to_tensor(fasterrcnn.resize_image(img))
            for img in images
        ]).to(device)
        cls_result = faster_rcnn_model(tensor_in)[-1]
        # æˆªå–å„å›¾ç‰‡ä¸­çš„äººè„¸åŒºåŸŸ
        face_names = []
        face_images = []
        for name, image, result in zip(names, images, cls_result):
            if not result:
                print("no face found for", name)
                continue
            result = fasterrcnn.MyModel.merge_predicted_result(result)
            # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰å–åˆ†æ•°æœ€é«˜çš„åŒºåŸŸ
            result.sort(key=lambda r: -r[2])
            label, box, rpn_score, cls_score = result[0]
            x, y, w, h = fasterrcnn.map_box_to_original_image(box, *image.size)
            if w == 0 or h == 0:
                print("no face found for", name, "due to incorrect size")
                continue
            # ä½¿ç”¨è„¸éƒ¨å…³é”®ç‚¹è°ƒæ•´è„¸éƒ¨èŒƒå›´
            child_img, _, points = adjust_face_region(image, (x, y, w, h), face_landmark_model)
            # ä¿å­˜æ£€æµ‹ç»“æœç”¨äºè°ƒè¯•
            if not os.path.isdir(FACES_OUTPUT_DIR):
                os.makedirs(FACES_OUTPUT_DIR)
            face_landmark.draw_points(child_img, points)
            child_img.convert("RGB").save(os.path.join(
                FACES_OUTPUT_DIR, f"{name[0]}_{os.path.splitext(name[1])[0]}.png"))
            face_names.append(name)
            face_images.append(child_img)
        # è½¬æ¢äººè„¸å›¾ç‰‡åˆ°ç¼–ç 
        tensor_in = torch.stack([
            face_recognition.image_to_tensor(face_recognition.resize_image(img))
            for img in face_images
        ]).to(device)
        face_vectors = face_recognition_model(tensor_in)
        # æ·»åŠ åç§°å’Œç¼–ç åˆ°ç»“æœåˆ—è¡¨
        for name in face_names:
            result_names.append(name)
        result_vectors.append(face_vectors)
        names.clear()
        images.clear()
    for dirname in os.listdir(SAMPLES_DIR):
        dirpath = os.path.join(SAMPLES_DIR, dirname)
        if not os.path.isdir(dirpath):
            continue
        for filename in os.listdir(dirpath):
            if os.path.splitext(filename)[1].lower() not in (".jpg", ".png"):
                continue
            names.append((dirname, filename))
            images.append(Image.open(os.path.join(dirpath, filename)))
            if len(images) >= batch_size:
                process_images()
    if images:
        process_images()
    # æ•´åˆç¼–ç åˆ—è¡¨
    result_vector = torch.cat(result_vectors, dim=0)
    # ä¿å­˜åç§°å’Œç¼–ç 
    fasterrcnn.save_tensor((result_names, result_vector), VECTORS_PATH)
    peoples_count = len(set(n[0] for n in result_names))
    print(f"built vectors for {peoples_count} peoples and {result_vector.shape[0]} images")

def recognize_face(
    faster_rcnn_model, face_landmark_model,
    face_recognition_model, face_verification_model,
    image, vectors, top_range=10):
    """è¯†åˆ«äººè„¸ä½ç½®ä¸èº«ä»½"""
    # æŸ¥æ‰¾äººè„¸åŒºåŸŸ
    tensor_in = fasterrcnn.image_to_tensor(
        fasterrcnn.resize_image(image)).unsqueeze(0).to(device)
    cls_result = faster_rcnn_model(tensor_in)[-1]
    # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰å–åˆ†æ•°æœ€é«˜çš„åŒºåŸŸ
    result = cls_result[0]
    if not result:
        return None
    result = fasterrcnn.MyModel.merge_predicted_result(result)
    result.sort(key=lambda r: -r[2])
    label, box, rpn_score, cls_score = result[0]
    x, y, w, h = fasterrcnn.map_box_to_original_image(box, *image.size)
    if w == 0 or h == 0:
        return None
    # ä½¿ç”¨è„¸éƒ¨å…³é”®ç‚¹è°ƒæ•´è„¸éƒ¨èŒƒå›´
    child_img, (x, y, w, h), points = adjust_face_region(
        image, (x, y, w, h), face_landmark_model)
    # ç”Ÿæˆäººè„¸ç¼–ç 
    tensor_in = face_recognition.image_to_tensor(
        face_recognition.resize_image(child_img)).unsqueeze(0).to(device)
    vector = face_recognition_model(tensor_in)[0]
    # æ¯”è¾ƒäººè„¸ç¼–ç ï¼Œå¾—å‡ºæœ€æ¥è¿‘çš„ N å¼ äººè„¸
    diff = (vector - vectors).pow(2)
    diff_sorted = diff.sum(dim=1).sort()
    indices = diff_sorted.indices[:top_range].tolist()
    # ä½¿ç”¨äººè„¸è®¤è¯æ¨¡å‹åŒ¹é…äººè„¸ï¼Œèº«ä»½ä¸æ˜æ—¶è¿”å› None
    highest_score = None
    highest_index = None
    scores = face_verification_model(diff[indices]).tolist()
    for index, score in zip(indices, scores):
        if score < FACE_VERIFICATION_THRESHOLD:
            continue
        if highest_score and score <= highest_score:
            continue
        highest_index = index
        highest_score = score
    # è¿”å›äººè„¸å›¾ç‰‡ï¼Œäººè„¸èŒƒå›´ï¼Œäººè„¸å…³é”®ç‚¹ï¼ŒåŒ¹é…å‡ºæ¥çš„èº«ä»½ï¼Œåˆ†æ•°
    # æ²¡æœ‰åŒ¹é…åˆ°èº«ä»½æ—¶ï¼Œèº«ä»½å’Œåˆ†æ•°ä¼šä¸º None
    return child_img, (x, y, w, h), points, highest_index, highest_score

def main():
    """ç»„åˆæ¨¡å‹å®ç°äººè„¸æ£€æµ‹ + è¯†åˆ« + è®¤è¯"""
    # è®© fasterrcnn åˆå¹¶åŒºåŸŸæ—¶ä¸ä½¿ç”¨ NMS ç®—æ³•ï¼Œä½¿å¾—ç»“æœåŒºåŸŸæ›´å¤§
    fasterrcnn.USE_NMS_ALGORITHM = False

    # åŠ è½½ fasterrcnn æ¨¡å‹
    faster_rcnn_model = fasterrcnn.MyModel().to(device)
    faster_rcnn_model.load_state_dict(fasterrcnn.load_tensor("model.pt"))
    faster_rcnn_model.eval()

    # åŠ è½½äººè„¸å…³é”®ç‚¹è¯†åˆ«æ¨¡å‹
    face_landmark_model = face_landmark.FaceLandmarkModel().to(device)
    face_landmark_model.load_state_dict(fasterrcnn.load_tensor("../face-landmark/model.pt"))
    face_landmark_model.eval()

    # åŠ è½½äººè„¸è¯†åˆ«æ¨¡å‹
    face_recognition_model = face_recognition.FaceRecognitionModel().to(device)
    face_recognition_model.load_state_dict(face_recognition.load_tensor("../face-recognition/model.recognition.pt"))
    face_recognition_model.eval()

    # åŠ è½½äººè„¸éªŒè¯æ¨¡å‹
    face_verification_model = face_recognition.FaceVerificationModel().to(device)
    face_verification_model.load_state_dict(face_recognition.load_tensor("../face-recognition/model.verification.pt"))
    face_verification_model.eval()

    # æ ¹æ®äººç‰©å’Œå›¾ç‰‡æ„å»ºç¼–ç æ•°æ®åº“
    if not os.path.isfile(VECTORS_PATH):
        build_vectors(faster_rcnn_model, face_landmark_model, face_recognition_model)

    # è¯»å–ç¼–ç æ•°æ®åº“
    names, vectors = fasterrcnn.load_tensor(VECTORS_PATH)

    # è¯¢é—®å›¾ç‰‡è·¯å¾„ï¼Œå¹¶è¯†åˆ«äººè„¸
    font = ImageFont.truetype("wqy-microhei.ttc", 20)
    while True:
        try:
            # æ‰“å¼€å›¾ç‰‡
            image_path = input("Image path: ")
            if not image_path:
                continue
            image = Image.open(image_path)
            # è¯†åˆ«äººè„¸
            result = recognize_face(
                faster_rcnn_model, face_landmark_model,
                face_recognition_model, face_verification_model, image, vectors)
            if not result:
                print("no face found\n")
                continue
            # æ ¹æ®ç»“æœæ ‡è®°åˆ°å›¾ç‰‡ä¸Š
            child_img, (x, y, w, h), points, index, score = result
            points = [ (px + x, py + y) for px, py in points ]
            label = f"{names[index][0]}: {score:.2f}" if index else "Unknown"
            print("result:", label)
            color = "#FF0000"
            draw = ImageDraw.Draw(image)
            draw.rectangle((x, y, x+w, y+h), outline=color, width=3)
            draw.text((x, y-20), label, fill=color, font=font)
            face_landmark.draw_points(image, points)
            image.save("img_output.png")
            print("saved to img_output.png")
            print()
        except Exception as e:
            print("error:", e)

if __name__ == "__main__":
    main()
```

ä½¿ç”¨è„¸éƒ¨å…³é”®ç‚¹è°ƒæ•´è„¸éƒ¨èŒƒå›´çš„ä»£ç åœ¨ `adjust_face_region` å‡½æ•°ä¸­ï¼Œæ„å»ºäººç‰©ç¼–ç æ•°æ®åº“çš„ä»£ç åœ¨ `build_vectors` å‡½æ•°ä¸­ï¼Œè€Œè¯†åˆ«äººè„¸çš„ä»£ç åœ¨ `recognize_face` å‡½æ•°ä¸­ã€‚

æŠŠä»£ç å’Œäººç‰©å›¾ç‰‡æŒ‰ä»¥ä¸‹çš„æ–‡ä»¶å¤¹ç»“æ„ä¿å­˜ï¼š

- faster-rcnn
    - face_recognition_samples
        - äººç‰©åç§°
            - å›¾ç‰‡1.jpg
            - å›¾ç‰‡2.jpg
        - äººç‰©åç§°
            - å›¾ç‰‡1.jpg
            - å›¾ç‰‡2.jpg
            - å›¾ç‰‡3.jpg
    - fasterrcnn.py
    - fasterrcnn_face_recognition.py (ä¸Šé¢ç»™å‡ºçš„ä»£ç )
    - model.pt
- face-landmark
    - face_landmark.py
    - model.pt
- face-recognition
    - face_recognition.py
    - model.recognition.pt
    - model.verification.pt

ç„¶åå†æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å°è¯•äººè„¸è¯†åˆ«åŠŸèƒ½ï¼š

``` text
cd faster-rcnn
python3 fasterrcnn_face_recognition.py
```

é¦–æ¬¡è¿è¡Œçš„æ—¶å€™ä¼šæ ¹æ® face_recognition_samples æ–‡ä»¶å¤¹ä¸‹çš„äººç‰©æ„å»ºç¼–ç æ•°æ®åº“ï¼Œå¦‚æœæƒ³é‡æ–°æ„å»ºå¯ä»¥åˆ æ‰ `face_recognition_vectors.pt` å†è¿è¡Œã€‚

æ„å»ºç¼–ç æ•°æ®åº“çš„æ—¶å€™ä¼šåŒæ—¶è¾“å‡ºæˆªå–çš„äººè„¸å›¾ç‰‡åˆ° `detected_faces` æ–‡ä»¶å¤¹ä¸‹æ–¹ä¾¿é™¤é”™ï¼Œä»¥ä¸‹æ˜¯æ–‡ä»¶å¤¹çš„å†…å®¹ï¼š

![faces2.png](./faces2.png)

ä»¥ä¸‹æ˜¯éƒ¨åˆ†è¯†åˆ«ç»“æœï¼Œè®©æ¨¡å‹å…ˆè®¤è¯†é¢†å¯¼çš„ä»»åŠ¡å®Œæˆäº†ğŸ¥ºï¼š

![img_output1.png](./img_output1.png)
![img_output2.png](./img_output2.png)
![img_output3.png](./img_output3.png)
![img_output4.png](./img_output4.png)
![img_output5.png](./img_output5.png)
![img_output6.png](./img_output6.png)

ä¸åœ¨æ•°æ®åº“çš„äººç‰©çš„è¯†åˆ«ç»“æœï¼š

![img_output_unknown1.png](./img_output_unknown1.png)
![img_output_unknown2.png](./img_output_unknown2.png)

ç»è¿‡è¯•éªŒç²¾åº¦è¿˜æ˜¯ä¸é”™çš„ğŸ˜¤ã€‚

æœ¬ç¯‡æ–‡ç« ä»‹ç»çš„æ¨¡å‹å¯ä»¥ç”¨åœ¨è€ƒå‹¤ï¼Œè®¿å®¢ç®¡ç†ç­‰ç³»ç»Ÿä¸­ï¼Œä½†ä¸åº”è¯¥ç”¨äºæ”¯ä»˜å’Œè§£é”ï¼Œæ”¯ä»˜å’Œè§£é”ä½¿ç”¨çš„äººè„¸è¯†åˆ«æ¨¡å‹è¦æ±‚ç²¾åº¦æ›´é«˜ï¼Œå¹¶ä¸”å¾ˆå¤§å¯èƒ½è¦æ±‚æ„å»ºä¸‰ç»´æ¨¡å‹è€Œä¸æ˜¯ç®€å•çš„è¯†åˆ«å›¾ç‰‡ğŸ¤§ã€‚æ­¤å¤–ï¼Œè¯†åˆ«é€ƒçŠ¯åè€Œä¸è¦æ±‚å¤ªé«˜çš„å‡†ç¡®åº¦ï¼Œå› ä¸ºé€ƒçŠ¯é€šå¸¸ä¼šæ‰“æ‰®çš„å’Œæ•°æ®åº“ä¸­çš„ç…§ç‰‡ä¸ä¸€æ ·ï¼Œåªè¦è¯†åˆ«å‡ºå¯ç–‘äººç‰©å³å¯é€šçŸ¥è­¦å¯Ÿäººå·¥åˆ¤æ–­ï¼Œå¦‚æœçœŸçš„çœ‹ä¸Šå»æ˜¯åŒä¸€ä¸ªäººï¼Œå°±å¯ä»¥è·Ÿè¸ªæŠ“æ•äº†ğŸ˜¡ã€‚

## å†™åœ¨æœ€å

è¿™ä¸ªç³»åˆ—é¢„è®¡å†å†™ä¸€ä¸¤ç¯‡å°±ä¼šå‘ç›®å½•ç»™å‡ºç‰ˆç¤¾ï¼Œåˆ°æ—¶å€™ä¹Ÿä¼šé‡æ–°ç¼–æ’å†…å®¹è®©å­¦ä¹ æ›´åŠ ç®€å•ï¼Œç»§ç»­ç­‰ç­‰å§ğŸ¤•ã€‚

æ­¤å¤–ï¼Œæˆ‘å‡†å¤‡å¼€æ‹“ä¸€ä¸ªæ–°çš„ç³»åˆ—ä»‹ç» SciSharp çš„ [TensorFlow.NET](https://github.com/SciSharp/TensorFlow.NET)ï¼Œå®šä½åŒæ ·æ˜¯å…¥é—¨ï¼Œä¸»è¦é¢å‘æ²¡æœ‰ Python åŸºç¡€çš„ .NET æ•™å¾’ï¼Œæ•¬è¯·æœŸå¾…å­ğŸ˜ã€‚
