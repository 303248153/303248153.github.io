---
layout: post
title: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (å…­) - åº”ç”¨é€’å½’æ¨¡å‹çš„ä¾‹å­
tag: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨
---

è¿™ä¸€ç¯‡å°†ä¼šä¸¾ä¸¤ä¸ªä¾‹å­è¯´æ˜æ€ä¹ˆåº”ç”¨é€’å½’æ¨¡å‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»å’Œé¢„æµ‹è‚¡ä»·èµ°åŠ¿ã€‚ä¸å‰å‡ ç¯‡ä¸åŒï¼Œè¿™ä¸€ç¯‡ä½¿ç”¨çš„æ•°æ®æ˜¯ç°å®å­˜åœ¨çš„æ•°æ®ï¼Œæˆ‘ä»¬å°†å¯ä»¥çœ‹åˆ°æ›´é«˜çº§çš„æ¨¡å‹å’Œæ‰‹æ³•ğŸ¤ ã€‚

## ä¾‹å­â‘   - æ–‡æœ¬æ„Ÿæƒ…åˆ†ç±»

æ–‡æœ¬æ„Ÿæƒ…åˆ†ç±»æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­ï¼Œç®€å•çš„æ¥è¯´å°±æ˜¯ç»™å‡ºä¸€æ®µè¯ï¼Œåˆ¤æ–­è¿™æ®µè¯æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„ï¼Œä¾‹å¦‚æ·˜å®æˆ–è€…äº¬ä¸œä¸Šå¯¹å•†å“çš„è¯„ä»·ï¼Œè±†ç“£ä¸Šå¯¹ç”µå½±çš„è¯„ä»·ï¼Œæ›´é«˜çº§çš„æƒ…æ„Ÿåˆ†ç±»è¿˜èƒ½å¯¹æ–‡æœ¬ä¸­çš„æ„Ÿæƒ…è¿›è¡Œç»†åˆ†ã€‚å› ä¸ºæ¶‰åŠåˆ°è‡ªç„¶è¯­è¨€ï¼Œæ–‡æœ¬æ„Ÿæƒ…åˆ†ç±»ä¹Ÿå±äºè‡ªç„¶è¯­è¨€å¤„ç† (NLP, Nature Langure Processing)ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥å°†ä¼šä½¿ç”¨ ami66 åœ¨ github ä¸Š[å…¬å¼€çš„æ•°æ®](https://github.com/303248153/ChineseTextClassifier)ï¼Œæ¥å®ç°æ ¹æ®å•†å“è¯„è®ºå†…å®¹è¯†åˆ«æ˜¯æ­£é¢è¯„è®ºè¿˜æ˜¯è´Ÿé¢è¯„è®ºã€‚

![01](./01.png)

åœ¨å¤„ç†æ–‡æœ¬ä¹‹å‰æˆ‘ä»¬éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†ï¼Œåˆ‡åˆ†æ–¹æ³•å¯ä»¥åˆ†ä¸ºæŒ‰å­—åˆ‡åˆ†å’ŒæŒ‰å•è¯åˆ‡åˆ†ï¼ŒæŒ‰å•è¯åˆ‡åˆ†çš„ç²¾åº¦æ›´é«˜ä½†è¦æ±‚ä½¿ç”¨åˆ†è¯ç±»åº“ã€‚å¤„ç†ä¸­æ–‡æ—¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¼€æºçš„ [jieba](https://github.com/fxsjy/jieba) ç±»åº“æ¥æŒ‰å•è¯åˆ‡åˆ†ï¼Œæ‰§è¡Œ `pip3 install jieba --user` å³å¯å®‰è£…ï¼Œä½¿ç”¨ä¾‹å­å¦‚ä¸‹ï¼š

``` python
# æŒ‰å­—åˆ‡åˆ†
>>> words = [c for c in "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦"]
>>> words
['æˆ‘', 'æ¥', 'åˆ°', 'åŒ—', 'äº¬', 'æ¸…', 'å', 'å¤§', 'å­¦']

# æŒ‰å•è¯åˆ‡åˆ†
>>> import jieba
>>> words = list(jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦"))
>>> words
['æˆ‘', 'æ¥åˆ°', 'åŒ—äº¬', 'æ¸…åå¤§å­¦']
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨æ•°å€¼æ¥è¡¨ç¤ºå­—æˆ–è€…å•è¯ï¼Œè¿™ä¹Ÿæœ‰å‡ ç§æ–¹æ³•ï¼Œç¬¬ä¸€ç§æ–¹æ³•æ˜¯ onehotï¼Œå³å‡†å¤‡ä¸€ä¸ªå’Œå­—æ•°é‡ä¸€æ ·çš„åºåˆ—ï¼Œç„¶åç”¨æ¯ä¸ªå…ƒç´ ä»£è¡¨æ¯ä¸ªå­—ï¼Œè¿™ç§æ–¹æ³•å¹¶ä¸å®ç”¨ï¼Œå› ä¸ºå¦‚æœè¦å¤„ç†çš„æ–‡æœ¬é‡Œé¢æœ‰å‡ ä¸‡ç§ä¸åŒçš„å­—ï¼Œé‚£ä¹ˆå°±éœ€è¦å‡ ä¸‡é•¿åº¦çš„åºåˆ—ï¼Œå¤„ç†èµ·æ¥å°†ä¼šéå¸¸éå¸¸æ…¢ï¼›ç¬¬äºŒç§æ–¹æ³•æ˜¯ binaryï¼Œå³ä½¿ç”¨äºŒè¿›åˆ¶æ¥è¡¨ç¤ºæ¯ä¸ªå­—çš„ç´¢å¼•å€¼ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å‡å°‘åºåˆ—é•¿åº¦ä½†æ˜¯ä¼šå½±å“è®­ç»ƒæ•ˆæœï¼›ç¬¬ä¸‰ç§æ–¹æ³•æ˜¯ embeddingï¼Œä½¿ç”¨å‘é‡ (æµ®ç‚¹æ•°ç»„æˆçš„åºåˆ—) æ¥è¡¨ç¤ºæ¯ä¸ªå­—æˆ–è€…å•è¯ï¼Œè¿™ä¸ªå‘é‡å¯ä»¥é¢„å…ˆæ ¹æ®æŸç§è§„å¾‹ç”Ÿæˆï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´ï¼Œè¿™ç§æ–¹æ³•æ˜¯ç›®å‰æœ€æµè¡Œçš„æ–¹æ³•ã€‚é¢„å…ˆæ ¹æ®æŸç§è§„å¾‹ç”Ÿæˆçš„ embedding ç¼–ç è¿˜ä¼šè®©è¯­ä¹‰è¿‘ä¼¼çš„å•è¯çš„å€¼æ›´æ¥è¿‘ï¼Œä¾‹å¦‚ `è‹¹æœ` å’Œ `æ©™å­` çš„å‘é‡å°†ä¼šæ¯”è¾ƒæ¥è¿‘ã€‚æ¥ä¸‹æ¥çš„ä¾‹å­å°†ä¼šä½¿ç”¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´çš„ embedding ç¼–ç ï¼Œç„¶åå†ä»‹ç»å‡ ç§é¢„å…ˆç”Ÿæˆçš„ embedding ç¼–ç åº“ã€‚

![02](./02.png)

å¤„ç†æ–‡æœ¬å¹¶ä¼ ç»™æ¨¡å‹çš„æµç¨‹å¦‚ä¸‹ï¼Œè¿™é‡Œä»…è´Ÿè´£æŠŠå•è¯è½¬æ¢ä¸ºæ•°å€¼ï¼Œembedding å¤„ç†åœ¨æ¨¡å‹ä¸­ (åé¢ä»‹ç»çš„é¢„ç”Ÿæˆ embedding ç¼–ç ä¼šåœ¨ä¼ ç»™æ¨¡å‹å‰å¤„ç†)ï¼š

![03](./03.png)

æ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼Œé¦–å…ˆç”¨ Embedding è´Ÿè´£è½¬æ¢å„ä¸ªå•è¯çš„æ•°å€¼åˆ°å‘é‡ï¼Œç„¶åç”¨ LSTM å¤„ç†å„ä¸ªå•è¯å¯¹åº”çš„å‘é‡ï¼Œä¹‹åç”¨ä¸¤å±‚çº¿æ€§æ¨¡å‹æ¥è¯†åˆ« LSTM è¿”å›çš„æœ€ç»ˆéšè—å€¼ï¼Œæœ€åç”¨ sigmoid å‡½æ•°æŠŠå€¼è½¬æ¢åˆ° 0 ~ 1 ä¹‹é—´ï¼š

![04](./04.png)

pytorch ä¸­çš„ `torch.nn.Embedding` ä¼šéšæœºç»™æ¯ä¸ªæ•°å€¼åˆ†é…ä¸€ä¸ªå‘é‡ï¼Œåºåˆ—ä¸­çš„å€¼ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ï¼Œæœ€ç»ˆè¿™ä¸ªå‘é‡ä¼šä»£è¡¨å•è¯çš„æŸäº›å±æ€§ï¼Œå«ä¹‰æ¥è¿‘çš„å•è¯å‘é‡çš„å€¼ä¹Ÿä¼šæ¥è¿‘ã€‚

æœ€ç»ˆçš„ sigmoid ä¸ä»…ç”¨äºæ§åˆ¶å€¼èŒƒå›´ï¼Œè¿˜å¯ä»¥è®©è°ƒæ•´å‚æ•°æ›´å®¹æ˜“ï¼Œè¯•æƒ³ä¸¤ä¸ªå¥å­éƒ½æ˜¯å¥½è¯„ï¼Œå¦‚æœæ²¡æœ‰ sigmoidï¼Œé‚£ä¹ˆåˆ™éœ€è¦è°ƒæ•´çº¿æ€§æ¨¡å‹çš„è¾“å‡ºå€¼æ¥è¿‘ 1ï¼Œå¤§äºæˆ–å°äºéƒ½å¾—è°ƒæ•´ï¼Œå¦‚æœæœ‰ sigmoidï¼Œé‚£ä¹ˆåªéœ€è¦è°ƒæ•´çº¿æ€§æ¨¡å‹çš„è¾“å‡ºå€¼å¤§äº 6 å³å¯ï¼Œä¾‹å¦‚ç¬¬ä¸€ä¸ªå¥å­è¾“å‡º 8ï¼Œç¬¬äºŒä¸ªå¥å­è¾“å‡º 16ï¼Œä¸¤ä¸ªç»è¿‡ sigmoid ä»¥åéƒ½æ˜¯ 1ã€‚

è®­ç»ƒå’Œä½¿ç”¨æ¨¡å‹çš„ä»£ç å¦‚ä¸‹ï¼Œä¸ä¹‹å‰çš„ä»£ç ç›¸æ¯”éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š

- é¢„æµ‹è¾“å‡ºè¶…è¿‡ 0.5 çš„æ—¶å€™ä¼šåˆ¤æ–­æ˜¯å¥½è¯„ï¼Œæœªè¶…è¿‡ 0.5 çš„æ—¶å€™ä¼šåˆ¤æ–­æ˜¯å·®è¯„
- è®¡ç®—æ­£ç¡®ç‡çš„æ—¶å€™ä¼šä½¿ç”¨é¢„æµ‹è¾“å‡ºå’Œå®é™…è¾“å‡ºçš„åŒ¹é…æ•°é‡å’Œæ€»æ•°é‡ä¹‹é—´çš„æ¯”ä¾‹
- éœ€è¦ä¿å­˜å•è¯åˆ°æ•°å€¼çš„ç´¢å¼•ï¼Œç”¨äºè®¡ç®—æ€»å•è¯æ•°é‡å’Œå®é™…ä½¿ç”¨æ¨¡å‹æ—¶è½¬æ¢å•è¯åˆ°æ•°å€¼
- ä¸ºäº†åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œå‚æ•°è°ƒæ•´å™¨ä» SGD æ¢æˆäº† Adadelta
    - Adadelta æ˜¯ SGD çš„ä¸€ä¸ªæ‰©å±•ï¼Œæ”¯æŒè‡ªåŠ¨è°ƒæ•´å­¦ä¹ æ¯”ä¾‹ (SGD åªèƒ½å›ºå®šå­¦ä¹ æ¯”ä¾‹)ï¼Œä½ å¯ä»¥å‚è€ƒ[è¿™ç¯‡æ–‡ç« ](https://ruder.io/optimizing-gradient-descent/)äº†è§£å·¥ä½œåŸç†

``` python
import os
import sys
import torch
import gzip
import itertools
import jieba
import json
import random
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®è¯„è®ºåˆ†ææ˜¯å¥½è¯„è¿˜æ˜¯å·®è¯„"""
    def __init__(self, total_words):
        super().__init__()
        self.embedding = nn.Embedding(
            num_embeddings=total_words,
            embedding_dim=16,
            padding_idx=0
        )
        self.rnn = nn.LSTM(
            input_size = 16,
            hidden_size = 32,
            num_layers = 1,
            batch_first = True
        )
        self.linear = nn.Sequential(
            nn.Linear(in_features=32, out_features=16),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(in_features=16, out_features=1),
            nn.Sigmoid())

    def forward(self, x, lengths):
        # è½¬æ¢å•è¯å¯¹åº”çš„æ•°å€¼åˆ°å‘é‡
        embedded = self.embedding(x)
        # é™„åŠ é•¿åº¦ä¿¡æ¯ï¼Œé¿å… RNN è®¡ç®—å¡«å……çš„æ•°æ®
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths, batch_first=True, enforce_sorted=False)
        # ä½¿ç”¨é€’å½’æ¨¡å‹è®¡ç®—ï¼Œå› ä¸ºå½“å‰åœºæ™¯åªéœ€è¦æœ€åä¸€ä¸ªè¾“å‡ºï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨ hidden
        # æ³¨æ„ LSTM çš„ç¬¬äºŒä¸ªè¿”å›å€¼åŒæ—¶åŒ…å«æœ€æ–°çš„éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        output, (hidden, cell) = self.rnn(packed)
        # è½¬æ¢éšè—çŠ¶æ€çš„ç»´åº¦åˆ° æ‰¹æ¬¡å¤§å°, éšè—å€¼æ•°é‡
        hidden = hidden.reshape(hidden.shape[1], hidden.shape[2])
        # ä½¿ç”¨å¤šå±‚çº¿æ€§æ¨¡å‹è¯†åˆ«é€’å½’æ¨¡å‹è¿”å›çš„éšè—å€¼
        # æœ€åä½¿ç”¨ sigmoid æŠŠå€¼èŒƒå›´æ§åˆ¶åœ¨ 0 ~ 1
        y = self.linear(hidden)
        return y

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def save_word_to_index(word_to_index):
    """ä¿å­˜å•è¯åˆ°æ•°å€¼çš„ç´¢å¼•"""
    json.dump(word_to_index, open('data/word_to_index.json', 'w'), indent=2, ensure_ascii=False)

def load_word_to_index():
    """è¯»å–å•è¯åˆ°æ•°å€¼çš„ç´¢å¼•"""
    return json.load(open('data/word_to_index.json', 'r'))

def prepare_save_batch(batch, pending_tensors):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æ‰“ä¹±å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    random.shuffle(pending_tensors)

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º tensorï¼Œå¦å¤–ä¿å­˜å„ä¸ªè¾“å…¥ tensor çš„é•¿åº¦
    in_tensor_unpadded = [p[0] for p in pending_tensors]
    in_tensor_lengths = torch.tensor([t.shape[0] for t in in_tensor_unpadded])
    out_tensor = torch.tensor([p[1] for p in pending_tensors])

    # æ•´åˆé•¿åº¦ä¸ç­‰çš„ in_tensor_unpadded åˆ°å•ä¸ª tensorï¼Œä¸è¶³çš„é•¿åº¦ä¼šå¡«å…… 0
    in_tensor = nn.utils.rnn.pad_sequence(in_tensor_unpadded, batch_first=True)

    # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
    random_indices = torch.randperm(in_tensor.shape[0])
    training_indices = random_indices[:int(len(random_indices)*0.6)]
    validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
    testing_indices = random_indices[int(len(random_indices)*0.8):]
    training_set = (in_tensor[training_indices], in_tensor_lengths[training_indices], out_tensor[training_indices])
    validating_set = (in_tensor[validating_indices], in_tensor_lengths[validating_indices], out_tensor[validating_indices])
    testing_set = (in_tensor[testing_indices], in_tensor_lengths[testing_indices], out_tensor[testing_indices])

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # å‡†å¤‡è¯è¯­åˆ°æ•°å€¼çš„ç´¢å¼•
    # é¢„ç•™ PAD ä¸ºå¡«å……ï¼ŒEOF ä¸ºè¯­å¥ç»“æŸ
    word_to_index = { '<PAD>': 0, '<EOF>': 1 }

    # ä» txt è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡å¤„ç† 2000 è¡Œ
    # è¿™é‡Œä½¿ç”¨åŸå§‹æ–¹æ³•è¯»å–ï¼Œæœ€åä¸€ä¸ªæ ‡æ³¨ä¸º 1 ä»£è¡¨å¥½è¯„ï¼Œä¸º 0 ä»£è¡¨å·®è¯„
    batch = 0
    pending_tensors = []
    for line in open('goods_zh.txt', 'r'):
        parts = line.split(',')
        phase = ",".join(parts[:-2])
        positive = int(parts[-1])
        # ä½¿ç”¨ jieba åˆ†è¯ï¼Œç„¶åè½¬æ¢å•è¯åˆ°ç´¢å¼•
        words = jieba.cut(phase)
        word_indices = []
        for word in words:
            if word.isascii() or word in ('ï¼Œ', 'ã€‚', 'ï¼'):
                continue # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
            if word in word_to_index:
                word_indices.append(word_to_index[word])
            else:
                new_index = len(word_to_index)
                word_to_index[word] = new_index
                word_indices.append(new_index)
        word_indices.append(1) # ä»£è¡¨è¯­å¥ç»“æŸ
        # è¾“å…¥æ˜¯å„ä¸ªå•è¯å¯¹åº”çš„ç´¢å¼•ï¼Œè¾“å‡ºæ˜¯æ˜¯å¦æ­£é¢è¯„ä»·
        pending_tensors.append((torch.tensor(word_indices), torch.tensor([positive])))
        if len(pending_tensors) >= 2000:
            prepare_save_batch(batch, pending_tensors)
            batch += 1
            pending_tensors.clear()
    if pending_tensors:
        prepare_save_batch(batch, pending_tensors)
        batch += 1
        pending_tensors.clear()

    # ä¿å­˜è¯è¯­åˆ°å•è¯çš„ç´¢å¼•
    save_word_to_index(word_to_index)

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    total_words = len(load_word_to_index())
    model = MyModel(total_words)

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adadelta(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return ((actual >= 0.5) == (predicted >= 0.5)).sum().item() / actual.shape[0]

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡ºçš„å·¥å…·å‡½æ•°
    def split_batch_xy(batch, begin=None, end=None):
        # shape = batch_size, input_size
        batch_x = batch[0][begin:end]
        # shape = batch_size, 1
        batch_x_lengths = batch[1][begin:end]
        # shape = batch_size, 1
        batch_y = batch[2][begin:end].reshape(-1, 1).float()
        return batch_x, batch_x_lengths, batch_y

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        for batch in read_batches("data/training_set"):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
            for index in range(0, batch[0].shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
                batch_x, batch_x_lengths, batch_y = split_batch_xy(batch, index, index+100)
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x, batch_x_lengths)
                # è®¡ç®—æŸå¤±
                loss = loss_function(predicted, batch_y)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    training_accuracy_list.append(calc_accuracy(batch_y, predicted))
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
            predicted = model(batch_x, batch_x_lengths)
            validating_accuracy_list.append(calc_accuracy(batch_y, predicted))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
        predicted = model(batch_x, batch_x_lengths)
        testing_accuracy_list.append(calc_accuracy(batch_y, predicted))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # è¯»å–è¯è¯­åˆ°å•è¯çš„ç´¢å¼•
    word_to_index = load_word_to_index()

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel(len(word_to_index))
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    while True:
        try:
            phase = input("Review: ")
            # åˆ†è¯
            words = list(jieba.cut(phase))
            # è½¬æ¢åˆ°æ•°å€¼åˆ—è¡¨
            word_indices = [word_to_index[w] for w in words if w in word_to_index]
            word_indices.append(word_to_index['EOF'])
            # æ„å»ºè¾“å…¥
            x = torch.tensor(word_indices).reshape(1, -1)
            lengths = torch.tensor([len(word_indices)])
            # é¢„æµ‹è¾“å‡º
            y = model(x, lengths)
            print("Positive Score:", y[0, 0].item(), "\n")
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å‡†å¤‡æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹ï¼š

``` text
python3 example.py prepare
python3 example.py train
```

è®­ç»ƒæˆåŠŸä»¥åçš„è¾“å‡ºå¦‚ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°éªŒè¯é›†å’Œæµ‹è¯•é›†æ­£ç¡®ç‡éƒ½è¾¾åˆ°äº† 93%ï¼š

``` text
epoch: 70
training accuracy: 0.9745314468456309
validating accuracy: 0.9339881613022567
stop training because highest validating accuracy not updated in 20 epoches
highest validating accuracy: 0.9348816130225674 from epoch 49
testing accuracy: 0.933661672216056
```

æ­£ç¡®ç‡çš„å˜åŒ–å¦‚ä¸‹ï¼š

![05](./05.png)

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š

``` text
python3 example.py eval
```

ä½¿ç”¨ä¾‹å­å¦‚ä¸‹ï¼š

``` text
Review: è¿™æ‰‹æœºåƒåå°ç‰¹åˆ«ä¸¥é‡ï¼Œä¸å»ºè®®è´­ä¹°
Positive Score: 0.010371988639235497

Review: è¿™æ ·å¾ˆå¥½ï¼Œç©¿ç€ç‰¹åˆ«èˆ’é€‚ï¼Œå¾ˆå–œæ¬¢çš„ä¸€åŒé‹å­ï¼Œå¤å¤©ä¹Ÿæ¯”è¾ƒé€æ°”
Positive Score: 1.0

Review: æ€§ä»·æ¯”è¿˜æ˜¯ä¸é”™çš„ï¼Œä½¿ç”¨åˆ°ç°åœ¨è¿˜æ²¡æœ‰å‡ºç°é—®é¢˜
Positive Score: 1.0

Review: æœåŠ¡æ€åº¦å·®ï¼Œç‰©æµæ…¢
Positive Score: 0.009614041075110435

Review: è¿™æ‰‹æœºæœ‰é—®é¢˜ï¼Œååº”åˆ°å®¢æœæ²¡äººç†
Positive Score: 0.00456244358792901

Review: å¼ºçƒˆå»ºè®®è´­ä¹°
Positive Score: 0.9984269142150879

Review: å¼ºçƒˆä¸å»ºè®®è´­ä¹°
Positive Score: 0.03579584136605263
```

æ³¨æ„å¦‚æœä½¿ç”¨çš„å•è¯ä¸åœ¨ç´¢å¼•ä¸­é‚£ä¹ˆè¿™ä¸ªå•è¯ä¼šè¢«å¿½ç•¥ï¼Œè¦è§£å†³è¿™ä¸ªé—®é¢˜å¯ä»¥å¢åŠ æ•°æ®é‡æ¶µç›–å°½é‡å¤šçš„å•è¯ï¼Œæˆ–è€…ä½¿ç”¨æ¥ä¸‹æ¥ä»‹ç»çš„é¢„ç”Ÿæˆ embedding ç¼–ç åº“ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªç¨‹åºå¯ä»¥æ™ºèƒ½åˆ¤æ–­å¯¹æ–¹è¯´çš„æ˜¯å¥½è¯è¿˜æ˜¯åè¯äº†ğŸ˜¤ï¼Œå› ä¸ºç°å®ä¸­çš„å•†åŸæˆ–è€…ç”µå½±è¯„ä»·ç½‘ç«™ä¸€èˆ¬éƒ½ä¼šåŒæ—¶è¦æ±‚ç”¨æˆ·æ‰“åˆ†æ‰€ä»¥è¿™ä¸ªä¾‹å­çš„å®ç”¨ä»·å€¼ä¸å¤§ï¼Œä½†å®ƒä»ç„¶æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­å¸®åŠ©æˆ‘ä»¬ç†è§£æ€æ ·ä½¿ç”¨é€’å½’æ¨¡å‹å¤„ç†è‡ªç„¶è¯­è¨€ã€‚

## ä½¿ç”¨é¢„ç”Ÿæˆ embedding ç¼–ç åº“

ä»¥ä¸Šçš„ä¾‹å­ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´ embedding ç¼–ç ï¼Œè¿™ç§åšæ³•å¾ˆæ–¹ä¾¿ï¼Œä½†åªèƒ½è¯†åˆ«åœ¨ç´¢å¼•ä¸­çš„å•è¯ (æ•°æ®é›†ä¸­åŒ…å«çš„å•è¯)ï¼Œå¦‚æœä½¿ç”¨äº†æœªçŸ¥çš„å•è¯é‚£ä¹ˆæ¨¡å‹æœ‰å¯èƒ½æ— æ³•æ­£ç¡®é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„ç”Ÿæˆçš„ embedding ç¼–ç åº“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™äº›åº“æ˜¯æ ¹æ®æµ·é‡æ•°æ®ç”Ÿæˆçš„ï¼ˆé€šå¸¸ä½¿ç”¨ç™¾ç§‘é—®ç­”æˆ–è€…æ–°é—»ç­‰å…¬å¼€æ•°æ®ï¼‰ï¼ŒåŒ…å«äº†éå¸¸éå¸¸å¤šçš„å•è¯ï¼Œå¹¶ä¸”è¯­ä¹‰æ¥è¿‘çš„å•è¯çš„å‘é‡ä¹Ÿä¼šå¾ˆæ¥è¿‘ï¼Œè®­ç»ƒçš„æ—¶å€™åªè¦ä½¿ç”¨éƒ¨åˆ†å•è¯å°±å¯ä»¥åŒæ—¶é€‚ç”¨äºè¯­ä¹‰æ¥è¿‘çš„æ›´å¤šå•è¯ã€‚

æ³¨æ„ä½¿ç”¨è¿™äº›åº“ä¸éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å‘é‡ï¼Œ`torch.nn.Embedding.from_pretrained` æ”¯æŒå¯¼å…¥é¢„å…ˆè®­ç»ƒå¥½çš„ç¼–ç åº“ï¼Œå¹¶ä¸”ä¸ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å®ƒä»¬ã€‚

### word2vec

ä½¿ç”¨ word2vec é¦–å…ˆæˆ‘ä»¬éœ€è¦å®‰è£… gensim åº“ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å³å¯å®‰è£…ï¼š

``` text
pip3 install gensim --user
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢„å…ˆç”Ÿæˆå¥½çš„ç¼–ç åº“ï¼Œä½ å¯ä»¥åœ¨ github ä¸Šæœç´¢ `word2vec chinese` æˆ–è€… `word2vec ä¸­æ–‡`ï¼Œä¹Ÿå¯ä»¥ç”¨è‡ªå·±çš„è¯­æ–™åº“ç”Ÿæˆã€‚è¿™é‡Œæˆ‘ç®€å•ä»‹ç»æ€æ ·ä½¿ç”¨è‡ªå·±çš„è¯­æ–™åº“ç”Ÿæˆï¼Œæ¥æºæ˜¯ä¸Šé¢çš„è¯„è®ºæ•°æ®ï¼Œä½ ä¹Ÿå¯ä»¥è¯•ç€ä»[è¿™é‡Œ](https://github.com/brightmart/nlp_chinese_corpus)ä¸‹è½½æ›´å¤§çš„æ–‡æœ¬æ•°æ®ã€‚

ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨ jieba åˆ†è¯ï¼Œç„¶åå…¨éƒ¨å†™åˆ°ä¸€ä¸ªæ–‡ä»¶ï¼Œå•è¯ä¹‹é—´ç”¨ç©ºæ ¼éš”å¼€ï¼š

``` python
import jieba
f = open('chinese.text8', 'w')
for line in open('goods_zh.txt', 'r'):
    line = "".join(line.split(',')[:-2])
    words = jieba.cut(line)
    words = [w for w in words if not (w.isascii() or w in ("ï¼Œ", "ã€‚", "ï¼"))]
    f.write(" ".join(words))
    f.write(" ")
```

ç¬¬äºŒæ­¥æ˜¯ä½¿ç”¨ word2vec ç”Ÿæˆå¹¶ä¿å­˜ç¼–ç åº“ï¼š

``` python
from gensim.models import word2vec
sentences = word2vec.Text8Corpus('chinese.text8')
model = word2vec.Word2Vec(sentences, size=200)
model.save("chinese.model")
```

è¯•ç€ä½¿ç”¨ç”Ÿæˆå¥½çš„ç¼–ç åº“ï¼š

``` python
# å¯»æ‰¾è¯­ä¹‰æ¥è¿‘çš„å•è¯ï¼ŒæŒºå‡†ç¡®çš„å§ğŸ™€
>>> from gensim.models import word2vec
>>> w = word2vec.Word2Vec.load("chinese.model")
>>> w.wv.most_similar(["æ‰‹æœº"])
[('æœºå­', 0.6180450916290283), ('æ–°æ‰‹æœº', 0.5946457386016846), ('æ–°æœº', 0.4700007736682892), ('æœºå™¨', 0.4531888961791992), ('è£è€€', 0.4304167628288269), ('çº¢ç±³', 0.42995956540107727), ('ç”µè„‘', 0.4163869023323059), ('ç¬”è®°æœ¬', 0.4093247652053833), ('åšæœ', 0.4016817808151245), ('äº§å“', 0.3963530957698822)]
>>> w.wv.most_similar(["ç‰©æµ"])
[('é€è´§', 0.8435776233673096), ('å¿«é€’', 0.7946128249168396), ('å‘è´§', 0.7307696342468262), ('é€’ç»™', 0.7279399037361145), ('é…é€', 0.6557953357696533), ('å¤„ç†é€Ÿåº¦', 0.6505168676376343), ('ç”¨ç”µ', 0.6292495131492615), ('é€Ÿé€’', 0.6150853633880615), ('è´§å‘', 0.6149879693984985), ('ååº”é€Ÿåº¦', 0.5916593074798584)]

# å®šä½å•è¯å¯¹åº”çš„æ•°å€¼
>>> w.wv.vocab.get("æ‰‹æœº").index
5

# å®šä½å•è¯å¯¹åº”çš„æ•°å€¼å¯¹åº”çš„å‘é‡
>>> w.wv.vectors[5]
array([-1.4774184e-01,  5.9569430e-01,  9.1274220e-01,  8.2012570e-01,
       çœç•¥é€”ä¸­è¾“å‡º
       -7.7284634e-01, -8.3093870e-01,  9.6443129e-01, -1.6938221e+00],
      dtype=float32)
```

åœ¨å‰é¢çš„ä¾‹å­ä¸­ä½¿ç”¨è¿™ä¸ªç¼–ç åº“çš„ä»£ç å¦‚ä¸‹ï¼Œæ”¹åŠ¨äº† `prepare` å’Œæ¨¡å‹éƒ¨åˆ†ï¼Œè™½ç„¶æ¨¡å‹ä½¿ç”¨äº† `torch.nn.Embedding` ä½†é¢„ç”Ÿæˆçš„ç¼–ç åº“ä¸ä¼šéšç€è®­ç»ƒè€Œå˜åŒ–ï¼Œæ­¤å¤–è¿™ä»½ä»£ç ä¸ä¼šåœ¨è¯­å¥ç»“å°¾æ·»åŠ  EOF å¯¹åº”çš„å‘é‡ (åœ¨è¿™ä¸ªä¾‹å­ä¸­ä¸å½±å“æ•ˆæœ)ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import jieba
import json
import random
from gensim.models import word2vec
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®è¯„è®ºåˆ†ææ˜¯å¥½è¯„è¿˜æ˜¯å·®è¯„"""
    def __init__(self, w2v):
        super().__init__()
        self.embedding = nn.Embedding.from_pretrained(
            torch.FloatTensor(w2v.wv.vectors))
        self.rnn = nn.LSTM(
            input_size = 200,
            hidden_size = 32,
            num_layers = 1,
            batch_first = True
        )
        self.linear = nn.Sequential(
            nn.Linear(in_features=32, out_features=16),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(in_features=16, out_features=1),
            nn.Sigmoid())

    def forward(self, x, lengths):
        # è½¬æ¢å•è¯å¯¹åº”çš„æ•°å€¼åˆ°å‘é‡
        embedded = self.embedding(x)
        # é™„åŠ é•¿åº¦ä¿¡æ¯ï¼Œé¿å… RNN è®¡ç®—å¡«å……çš„æ•°æ®
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths, batch_first=True, enforce_sorted=False)
        # ä½¿ç”¨é€’å½’æ¨¡å‹è®¡ç®—ï¼Œå› ä¸ºå½“å‰åœºæ™¯åªéœ€è¦æœ€åä¸€ä¸ªè¾“å‡ºï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨ hidden
        # æ³¨æ„ LSTM çš„ç¬¬äºŒä¸ªè¿”å›å€¼åŒæ—¶åŒ…å«æœ€æ–°çš„éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        output, (hidden, cell) = self.rnn(packed)
        # è½¬æ¢éšè—çŠ¶æ€çš„ç»´åº¦åˆ° æ‰¹æ¬¡å¤§å°, éšè—å€¼æ•°é‡
        hidden = hidden.reshape(hidden.shape[1], hidden.shape[2])
        # ä½¿ç”¨å¤šå±‚çº¿æ€§æ¨¡å‹è¯†åˆ«é€’å½’æ¨¡å‹è¿”å›çš„éšè—å€¼
        # æœ€åä½¿ç”¨ sigmoid æŠŠå€¼èŒƒå›´æ§åˆ¶åœ¨ 0 ~ 1
        y = self.linear(hidden)
        return y

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def load_word2vec_model():
    """è¯»å– word2vec ç¼–ç åº“"""
    return word2vec.Word2Vec.load("chinese.model")

def prepare_save_batch(batch, pending_tensors):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æ‰“ä¹±å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    random.shuffle(pending_tensors)

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º tensorï¼Œå¦å¤–ä¿å­˜å„ä¸ªè¾“å…¥ tensor çš„é•¿åº¦
    in_tensor_unpadded = [p[0] for p in pending_tensors]
    in_tensor_lengths = torch.tensor([t.shape[0] for t in in_tensor_unpadded])
    out_tensor = torch.tensor([p[1] for p in pending_tensors])

    # æ•´åˆé•¿åº¦ä¸ç­‰çš„ in_tensor_unpadded åˆ°å•ä¸ª tensorï¼Œä¸è¶³çš„é•¿åº¦ä¼šå¡«å…… 0
    in_tensor = nn.utils.rnn.pad_sequence(in_tensor_unpadded, batch_first=True)

    # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
    random_indices = torch.randperm(in_tensor.shape[0])
    training_indices = random_indices[:int(len(random_indices)*0.6)]
    validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
    testing_indices = random_indices[int(len(random_indices)*0.8):]
    training_set = (in_tensor[training_indices], in_tensor_lengths[training_indices], out_tensor[training_indices])
    validating_set = (in_tensor[validating_indices], in_tensor_lengths[validating_indices], out_tensor[validating_indices])
    testing_set = (in_tensor[testing_indices], in_tensor_lengths[testing_indices], out_tensor[testing_indices])

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # å‡†å¤‡è¯è¯­åˆ°æ•°å€¼çš„ç´¢å¼•
    w2v = load_word2vec_model()

    # ä» txt è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡å¤„ç† 2000 è¡Œ
    # è¿™é‡Œä½¿ç”¨åŸå§‹æ–¹æ³•è¯»å–ï¼Œæœ€åä¸€ä¸ªæ ‡æ³¨ä¸º 1 ä»£è¡¨å¥½è¯„ï¼Œä¸º 0 ä»£è¡¨å·®è¯„
    batch = 0
    pending_tensors = []
    for line in open('goods_zh.txt', 'r'):
        parts = line.split(',')
        phase = ",".join(parts[:-2])
        positive = int(parts[-1])
        # ä½¿ç”¨ jieba åˆ†è¯ï¼Œç„¶åè½¬æ¢å•è¯åˆ°ç´¢å¼•
        words = jieba.cut(phase)
        word_indices = []
        for word in words:
            if word.isascii() or word in ('ï¼Œ', 'ã€‚', 'ï¼'):
                continue # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
            vocab = w2v.wv.vocab.get(word)
            if vocab:
                word_indices.append(vocab.index)
        if not word_indices:
            continue # æ²¡æœ‰å•è¯åœ¨ç¼–ç åº“ä¸­
        # è¾“å…¥æ˜¯å„ä¸ªå•è¯å¯¹åº”çš„ç´¢å¼•ï¼Œè¾“å‡ºæ˜¯æ˜¯å¦æ­£é¢è¯„ä»·
        pending_tensors.append((torch.tensor(word_indices), torch.tensor([positive])))
        if len(pending_tensors) >= 2000:
            prepare_save_batch(batch, pending_tensors)
            batch += 1
            pending_tensors.clear()
    if pending_tensors:
        prepare_save_batch(batch, pending_tensors)
        batch += 1
        pending_tensors.clear()

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    w2v = load_word2vec_model()
    model = MyModel(w2v)

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adadelta(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return ((actual >= 0.5) == (predicted >= 0.5)).sum().item() / actual.shape[0]

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡ºçš„å·¥å…·å‡½æ•°
    def split_batch_xy(batch, begin=None, end=None):
        # shape = batch_size, input_size
        batch_x = batch[0][begin:end]
        # shape = batch_size, 1
        batch_x_lengths = batch[1][begin:end]
        # shape = batch_size, 1
        batch_y = batch[2][begin:end].reshape(-1, 1).float()
        return batch_x, batch_x_lengths, batch_y

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        for batch in read_batches("data/training_set"):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
            for index in range(0, batch[0].shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
                batch_x, batch_x_lengths, batch_y = split_batch_xy(batch, index, index+100)
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x, batch_x_lengths)
                # è®¡ç®—æŸå¤±
                loss = loss_function(predicted, batch_y)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    training_accuracy_list.append(calc_accuracy(batch_y, predicted))
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
            predicted = model(batch_x, batch_x_lengths)
            validating_accuracy_list.append(calc_accuracy(batch_y, predicted))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
        predicted = model(batch_x, batch_x_lengths)
        testing_accuracy_list.append(calc_accuracy(batch_y, predicted))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # è¯»å– word2vec ç¼–ç åº“
    w2v = load_word2vec_model()

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel(w2v)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    while True:
        try:
            phase = input("Review: ")
            # åˆ†è¯
            words = list(jieba.cut(phase))
            # è½¬æ¢åˆ°æ•°å€¼åˆ—è¡¨
            word_indices = []
            for word in words:
                if word.isascii() or word in ('ï¼Œ', 'ã€‚', 'ï¼'):
                    continue # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
                vocab = w2v.wv.vocab.get(word)
                if vocab:
                    word_indices.append(vocab.index)
            if not word_indices:
                raise ValueError("No known words")
            # æ„å»ºè¾“å…¥
            x = torch.tensor(word_indices).reshape(1, -1)
            lengths = torch.tensor([len(word_indices)])
            # é¢„æµ‹è¾“å‡º
            y = model(x, lengths)
            print("Positive Score:", y[0, 0].item(), "\n")
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

å¦‚æœä½ è¯•ç€ç”¨è¿™ä»½ä»£ç æ¥è®­ç»ƒä¼šå‘ç°ç¬¬ä¸€ä¸ª epoch å°±å·²ç»è¾¾åˆ° 90% ä»¥ä¸Šçš„æ­£ç¡®ç‡ï¼Œå¹¶ä¸”ç»§ç»­è®­ç»ƒä¸‹å»å¯ä»¥è¾¾åˆ°æ¯”ç›´æ¥ä½¿ç”¨ `torch.nn.Embedding` æ›´é«˜çš„æ­£ç¡®ç‡ï¼Œä½¿ç”¨é¢„ç”Ÿæˆç¼–ç åº“çš„æ•ˆæœæƒŠäººå‘€ğŸ¥³ã€‚

å¦‚æœä½ å¯¹ word2vec çš„åŸç†æ„Ÿå…´è¶£å¯ä»¥å‚è€ƒè¿™ç¯‡[æ–‡ç« ](https://www.cnblogs.com/peghoty/p/3857839.html)ï¼ŒåŒæ ·åœ¨åšå®¢å›­ä¸Šã€‚

### transfomers (BERT)

transfomers æ˜¯ä¸€ä¸ªç”¨äºå¤„ç†è‡ªç„¶è¯­è¨€çš„ç±»åº“ï¼ŒåŒ…å«äº†ç›®å‰ä¸–ç•Œä¸Šæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä¼šçœ‹åˆ°å¦‚ä½•ä½¿ç”¨å…¶ä¸­çš„ BERT æ¨¡å‹æ¥å¤„ç†ä¸­æ–‡ã€‚

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

``` text
pip3 install transformers
```

transfomers æ”¯æŒè‡ªåŠ¨ä¸‹è½½å’Œä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨ BERT ä¸­æ–‡æ¨¡å‹çš„ä»£ç  (ç¬¬ä¸€æ¬¡ä½¿ç”¨æ™‚ä¼šè‡ªåŠ¨ä¸‹è½½)ï¼Œæœ‰åˆ†è¯å™¨å’Œæ¨¡å‹ä¸¤éƒ¨åˆ†ï¼š

``` python
>>> from transformers import AutoTokenizer, AutoModel
>>> tt = AutoTokenizer.from_pretrained("bert-base-chinese")
>>> tm = AutoModel.from_pretrained("bert-base-chinese")

# è½¬æ¢ä¸­æ–‡è¯­å¥åˆ°æ•°å€¼åˆ—è¡¨
>>> tt.encode("äº”æ˜Ÿå¥½è¯„èµ")
[101, 758, 3215, 1962, 6397, 6614, 102]

# ç”Ÿæˆå„ä¸ªå•è¯å¯¹åº”çš„å‘é‡
>>> codes, hidden = tm(torch.tensor([[101, 758, 3215, 1962, 6397, 6614, 102]]))
>>> codes.shape
torch.Size([1, 7, 768])
>>> hidden.shape
torch.Size([1, 768])
```

å¦‚æœä½ ç»†å¿ƒè§‚å¯Ÿå¯èƒ½ä¼šå‘ç°ä¸Šé¢å¹¶æ²¡æœ‰å®é™…åˆ†è¯ï¼Œè€Œæ˜¯æ ¹æ®æ¯ä¸ªå­—å•ç‹¬ç”Ÿæˆäº†ç´¢å¼•ï¼Œè¿™æ˜¯å› ä¸º `bert-base-chinese` æ˜¯æŒ‰å­—æ¥åˆ’åˆ†çš„ï¼Œä½ å¯ä»¥è¯•è¯•å…¶ä»–æ¨¡å‹ (æˆ‘ä¸ç¡®å®šæ˜¯å¦æœ‰è¿™æ ·çš„ç°æˆæ¨¡å‹ğŸ˜«)ã€‚å¦å¤–è½¬æ¢ä¸ºå‘é‡æ—¶ï¼Œç¬¬äºŒä¸ªè¿”å›å€¼ä»£è¡¨äº†æœ€ç»ˆçš„å†…éƒ¨çŠ¶æ€ï¼Œè¿™ç‚¹è·Ÿé€’å½’æ¨¡å‹æ¯”è¾ƒåƒï¼Œç¬¬äºŒä¸ªè¿”å›å€¼è¿˜å¯ä»¥ç”¨æ¥ä»£è¡¨æ•´ä¸ªå¥å­çš„ç¼–ç ï¼Œå°½ç®¡ç²¾åº¦ä¼šæœ‰æ‰€é™ä½ã€‚

åœ¨å‰é¢çš„ä¾‹å­ä¸­ä½¿ç”¨ transfomers çš„ä»£ç å¦‚ä¸‹ï¼Œæ³¨æ„å‡†å¤‡æ•°æ®é›†å’Œè®­ç»ƒéƒ½éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼Œè¿™å¯ä»¥è¯´æ˜¯ç”¨ç‰›åˆ€æ€é¸¡ğŸ‘¿ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import json
import random
from transformers import AutoTokenizer, AutoModel
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®è¯„è®ºåˆ†ææ˜¯å¥½è¯„è¿˜æ˜¯å·®è¯„"""
    def __init__(self):
        super().__init__()
        self.rnn = nn.LSTM(
            input_size = 768,
            hidden_size = 32,
            num_layers = 1,
            batch_first = True
        )
        self.linear = nn.Sequential(
            nn.Linear(in_features=32, out_features=16),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(in_features=16, out_features=1),
            nn.Sigmoid())

    def forward(self, x, lengths):
        # transformers å·²ç»å¸®æˆ‘ä»¬è½¬æ¢ä¸ºå‘é‡
        embedded = x
        # é™„åŠ é•¿åº¦ä¿¡æ¯ï¼Œé¿å… RNN è®¡ç®—å¡«å……çš„æ•°æ®
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths, batch_first=True, enforce_sorted=False)
        # ä½¿ç”¨é€’å½’æ¨¡å‹è®¡ç®—ï¼Œå› ä¸ºå½“å‰åœºæ™¯åªéœ€è¦æœ€åä¸€ä¸ªè¾“å‡ºï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨ hidden
        # æ³¨æ„ LSTM çš„ç¬¬äºŒä¸ªè¿”å›å€¼åŒæ—¶åŒ…å«æœ€æ–°çš„éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        output, (hidden, cell) = self.rnn(packed)
        # è½¬æ¢éšè—çŠ¶æ€çš„ç»´åº¦åˆ° æ‰¹æ¬¡å¤§å°, éšè—å€¼æ•°é‡
        hidden = hidden.reshape(hidden.shape[1], hidden.shape[2])
        # ä½¿ç”¨å¤šå±‚çº¿æ€§æ¨¡å‹è¯†åˆ«é€’å½’æ¨¡å‹è¿”å›çš„éšè—å€¼
        # æœ€åä½¿ç”¨ sigmoid æŠŠå€¼èŒƒå›´æ§åˆ¶åœ¨ 0 ~ 1
        y = self.linear(hidden)
        return y

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def load_transfomer_tokenizer():
    """è·å– transformers çš„åˆ†è¯å™¨"""
    return AutoTokenizer.from_pretrained("bert-base-chinese")

def load_transfomer_model():
    """è·å– transofrmers çš„æ¨¡å‹"""
    return AutoModel.from_pretrained("bert-base-chinese")

def prepare_save_batch(batch, pending_tensors):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æ‰“ä¹±å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    random.shuffle(pending_tensors)

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º tensorï¼Œå¦å¤–ä¿å­˜å„ä¸ªè¾“å…¥ tensor çš„é•¿åº¦
    in_tensor_unpadded = [p[0] for p in pending_tensors]
    in_tensor_lengths = torch.tensor([t.shape[0] for t in in_tensor_unpadded])
    out_tensor = torch.tensor([p[1] for p in pending_tensors])

    # æ•´åˆé•¿åº¦ä¸ç­‰çš„ in_tensor_unpadded åˆ°å•ä¸ª tensorï¼Œä¸è¶³çš„é•¿åº¦ä¼šå¡«å…… 0
    in_tensor = nn.utils.rnn.pad_sequence(in_tensor_unpadded, batch_first=True)

    # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
    random_indices = torch.randperm(in_tensor.shape[0])
    training_indices = random_indices[:int(len(random_indices)*0.6)]
    validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
    testing_indices = random_indices[int(len(random_indices)*0.8):]
    training_set = (in_tensor[training_indices], in_tensor_lengths[training_indices], out_tensor[training_indices])
    validating_set = (in_tensor[validating_indices], in_tensor_lengths[validating_indices], out_tensor[validating_indices])
    testing_set = (in_tensor[testing_indices], in_tensor_lengths[testing_indices], out_tensor[testing_indices])

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # åŠ è½½ transformer åˆ†è¯å™¨å’Œæ¨¡å‹
    tt = load_transfomer_tokenizer()
    tm = load_transfomer_model()

    # ä» txt è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡å¤„ç† 2000 è¡Œ
    # è¿™é‡Œä½¿ç”¨åŸå§‹æ–¹æ³•è¯»å–ï¼Œæœ€åä¸€ä¸ªæ ‡æ³¨ä¸º 1 ä»£è¡¨å¥½è¯„ï¼Œä¸º 0 ä»£è¡¨å·®è¯„
    batch = 0
    pending_tensors = []
    for line in open('goods_zh.txt', 'r'):
        parts = line.split(',')
        phase = ",".join(parts[:-2])
        positive = int(parts[-1])

        # ä½¿ç”¨ transformer åˆ†è¯ï¼Œç„¶åè½¬æ¢å„ä¸ªæ•°å€¼åˆ°å‘é‡
        word_indices = tt.encode(phase)
        word_indices = word_indices[:510] # bert-base-chinese ä¸æ”¯æŒè¿‡é•¿çš„åºåˆ—
        words_tensor, hidden = tm(torch.tensor([word_indices]))
        words_tensor = words_tensor.reshape(words_tensor.shape[1], words_tensor.shape[2])
        # è¾“å…¥æ˜¯å„ä¸ªå•è¯å¯¹åº”çš„å‘é‡ï¼Œè¾“å‡ºæ˜¯æ˜¯å¦æ­£é¢è¯„ä»·
        pending_tensors.append((words_tensor, torch.tensor([positive])))
        if len(pending_tensors) >= 500:
            prepare_save_batch(batch, pending_tensors)
            batch += 1
            pending_tensors.clear()
    if pending_tensors:
        prepare_save_batch(batch, pending_tensors)
        batch += 1
        pending_tensors.clear()

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel()

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adadelta(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return ((actual >= 0.5) == (predicted >= 0.5)).sum().item() / actual.shape[0]

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡ºçš„å·¥å…·å‡½æ•°
    def split_batch_xy(batch, begin=None, end=None):
        # shape = batch_size, input_size
        batch_x = batch[0][begin:end]
        # shape = batch_size, 1
        batch_x_lengths = batch[1][begin:end]
        # shape = batch_size, 1
        batch_y = batch[2][begin:end].reshape(-1, 1).float()
        return batch_x, batch_x_lengths, batch_y

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        for batch in read_batches("data/training_set"):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
            for index in range(0, batch[0].shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
                batch_x, batch_x_lengths, batch_y = split_batch_xy(batch, index, index+100)
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x, batch_x_lengths)
                # è®¡ç®—æŸå¤±
                loss = loss_function(predicted, batch_y)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    training_accuracy_list.append(calc_accuracy(batch_y, predicted))
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
            predicted = model(batch_x, batch_x_lengths)
            validating_accuracy_list.append(calc_accuracy(batch_y, predicted))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
        predicted = model(batch_x, batch_x_lengths)
        testing_accuracy_list.append(calc_accuracy(batch_y, predicted))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åŠ è½½ transformer åˆ†è¯å™¨å’Œæ¨¡å‹
    tt = load_transfomer_tokenizer()
    tm = load_transfomer_model()

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel()
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    while True:
        try:
            phase = input("Review: ")
            # ä½¿ç”¨ transformer åˆ†è¯ï¼Œç„¶åè½¬æ¢å„ä¸ªæ•°å€¼åˆ°å‘é‡
            word_indices = tt.encode(phase)
            word_indices = word_indices[:510] # bert-base-chinese ä¸æ”¯æŒè¿‡é•¿çš„åºåˆ—
            words_tensor, hidden = tm(torch.tensor([word_indices]))
            # æ„å»ºè¾“å…¥
            x = words_tensor
            lengths = torch.tensor([len(word_indices)])
            # é¢„æµ‹è¾“å‡º
            y = model(x, lengths)
            print("Positive Score:", y[0, 0].item(), "\n")
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

å¦‚æœä½ å®é™…è®­ç»ƒä½¿ç”¨ä¼šå‘ç°å®ƒä¸ä»…èƒ½åˆ¤æ–­å•†å“è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„ï¼Œä¹Ÿèƒ½åˆ¤æ–­æ™®é€šè¯­å¥æ˜¯å¥½è¯è¿˜æ˜¯åè¯ï¼Œå¯ä»¥è¯´ç›¸å½“çš„ç¥å¥‡ã€‚

transfomers è¿˜å¯ä»¥ç”¨æ¥åšç¿»è¯‘å’Œæ–‡æœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œå› ä¸ºé‡Œé¢çš„æ¨¡å‹å¤ªé«˜çº§äº†æ‰€ä»¥æˆ‘ç›®å‰æ²¡æœ‰ç†è§£å®ƒä»¬æ˜¯æ€ä¹ˆå·¥ä½œçš„ğŸ™‰ï¼Œå¸Œæœ›ä»¥åæœ‰æœºä¼šå¯ä»¥è¯¦ç»†ä»‹ç»ã€‚

## ä¾‹å­â‘¡  - é¢„æµ‹è‚¡ä»·èµ°åŠ¿

å¦‚æœä½ æ˜¯ä¸€ä¸ªè‚¡æ°‘ï¼Œä½ å¯èƒ½ä¼šè¯•å›¾æ‰¾å‡ºé‚£äº›æ¶¨æ¶¨è·Œè·Œä¹‹é—´çš„è§„å¾‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ MACD, KDJ ç­‰æŒ‡æ ‡ï¼Œè¿™é‡Œæˆ‘ä»¬è¯•è¯•åº”ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹è‚¡ä»·èµ°åŠ¿ï¼Œçœ‹çœ‹ç»“æœå¦‚ä½•ã€‚

è®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨çš„æ•°æ®æ˜¯ä¸­å›½é“¶è¡Œ (601988) çš„æ¯æ—¥æ”¶ç›˜ä»·å’Œäº¤æ˜“é‡ï¼Œå¯ä»¥ä»ä»¥ä¸‹åœ°å€ä¸‹è½½ï¼š

- (æœ€æ–°æ•°æ®) https://finance.yahoo.com/quote/601988.SS/history?period1=1152057600&period2=1589500800&interval=1d&filter=history&frequency=1d
- (è¿™ç¯‡æ–‡ç« ä½¿ç”¨çš„æ•°æ®) https://github.com/303248153/303248153.github.io/tree/master/ml-06/601988.SS.csv

csv ä¸­åŒ…å«äº† `æ—¥æœŸ,å¼€ç›˜ä»·,æœ€é«˜ä»·,æœ€ä½ä»·,æ”¶ç›˜ä»·,è°ƒæ•´åæ”¶ç›˜ä»·,äº¤æ˜“é‡`ï¼Œè¾“å…¥å’Œè¾“å‡ºè§„å®šå¦‚ä¸‹

- è¾“å…¥: æ”¶ç›˜ä»· (æ ‡å‡†åŒ–é™¤ä»¥ 100), äº¤æ˜“é‡ (æ ‡å‡†åŒ–é™¤ä»¥ 1 äº¿)
- è¾“å‡º: T+2 çš„æ¶¨è·Œ (æ¶¨ 1 è·Œ 0, T+2 æŒ‡ä¸‹ä¸‹ä¸ªäº¤æ˜“æ—¥)

æ¨¡å‹æ˜¯ GRU + 2 å±‚çº¿æ€§æ¨¡å‹ï¼Œæœ€ç»ˆä½¿ç”¨ sigmoid è½¬æ¢è¾“å‡ºåˆ° 0 ~ 1 ä¹‹é—´çš„å€¼ï¼Œæ•°æ®åˆ’åˆ†è®­ç»ƒé›†åŒ…å« 1500 æ¡æ•°æ®ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†åŒ…å« 100 æ¡æ•°æ®ï¼Œæ—¶åºæŒ‰ `è®­ç»ƒé›† => éªŒè¯é›† => æµ‹è¯•é›†` æ’åˆ—ã€‚

æ³¨æ„ä¼ é€’æ•°æ®ç»™æ¨¡å‹çš„æ—¶å€™ä¼šæŒ‰ 32 æ¡æ•°æ®åˆ†æ‰¹ä¼ é€’ï¼Œæ¨¡å‹éœ€è¦ä¿ç•™éšè—çŠ¶æ€ä½¿å¾—åˆ†æ‰¹ä¼ é€’ä¸å®Œæ•´ä¼ é€’å¯ä»¥å¾—å‡ºç›¸åŒçš„ç»“æœã€‚

è®­ç»ƒå’Œä½¿ç”¨æ¨¡å‹çš„ä»£ç å¦‚ä¸‹ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import random
import pandas
import math
from torch import nn
from matplotlib import pyplot

CSV_FILENAME = "601988.SS.csv"
TRAINING_RECORDS = 1500
VALIDATING_RECORDS = 100
TESTING_RECORDS = 100

class MyModel(nn.Module):
    """æ ¹æ®å†å²æ”¶ç›˜ä»·å’Œæˆäº¤é‡é¢„æµ‹è‚¡ä»·èµ°åŠ¿"""
    def __init__(self):
        super().__init__()
        self.rnn = nn.GRU(
            input_size = 2,
            hidden_size = 50,
            num_layers = 1,
            batch_first = True
        )
        self.linear = nn.Sequential(
            nn.Linear(in_features=50, out_features=20),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(in_features=20, out_features=1),
            nn.Sigmoid())
        self.reset_hidden()

    def forward(self, x):
        # è°ƒæ•´ç»´åº¦
        x = x.reshape(1, x.shape[0], x.shape[1])
        # ä½¿ç”¨é€’å½’æ¨¡å‹è®¡ç®—ï¼Œéœ€è¦æ‰€æœ‰è¾“å‡ºï¼Œå¹¶ä¸”è¿˜éœ€è¦ä¿å­˜éšè—çŠ¶æ€
        # ä¿å­˜éšè—çŠ¶æ€æ—¶éœ€è¦ä½¿ç”¨ detach åˆ‡æ–­å†…éƒ¨çš„è®¡ç®—è·¯å¾„
        output, hidden = self.rnn(x, self.rnn_hidden)
        self.rnn_hidden = hidden.detach()
        # è½¬æ¢è¾“å‡ºçš„ç»´åº¦åˆ° æ‰¹æ¬¡å¤§å°, éšè—å€¼æ•°é‡
        output = output.reshape(output.shape[1], output.shape[2])
        # ä½¿ç”¨å¤šå±‚çº¿æ€§æ¨¡å‹è®¡ç®—é€’å½’æ¨¡å‹è¿”å›çš„è¾“å‡º
        y = self.linear(output)
        return y

    def reset_hidden(self):
        """é‡ç½®éšè—çŠ¶æ€"""
        self.rnn_hidden = torch.zeros(1, 1, 50)

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # ä» csv è¯»å–åŸå§‹æ•°æ®é›†
    df = pandas.read_csv(CSV_FILENAME)
    in_list = [] # æ”¶ç›˜ä»·å’Œæˆäº¤é‡ä½œä¸ºè¾“å…¥
    out_list = [] # T+2 çš„æ¶¨è·Œä½œä¸ºè¾“å‡º
    for value in df.values:
        volume = value[-1] / 100000000 # æˆäº¤é‡é™¤ä»¥ä¸€äº¿
        price = value[-3] / 100 # æ”¶ç›˜ä»·é™¤ä»¥ 100
        if math.isnan(volume) or math.isnan(price):
            continue # åŸå§‹æ•°æ®ä¸­æ˜¯ null
        in_list.append((price, volume))
    for index in range(len(in_list)-2):
        price_t0 = in_list[index][0]
        price_t2 = in_list[index+2][0]
        out_list.append(1. if price_t2 > price_t0 else 0.)
    in_list = in_list[:len(out_list)]

    # ç”Ÿæˆè¾“å…¥å’Œè¾“å‡º
    in_tensor = torch.tensor(in_list)
    out_tensor = torch.tensor(out_list).reshape(-1, 1)

    # åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
    testing_start = -TESTING_RECORDS
    validating_start = testing_start - VALIDATING_RECORDS
    training_start = validating_start - TRAINING_RECORDS
    training_in = in_tensor[training_start:validating_start]
    training_out = out_tensor[training_start:validating_start]
    validating_in = in_tensor[validating_start:testing_start]
    validating_out = out_tensor[validating_start:testing_start]
    testing_in = in_tensor[testing_start:]
    testing_out = out_tensor[testing_start:]

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor((training_in, training_out), f"data/training_set.pt")
    save_tensor((validating_in, validating_out), f"data/validating_set.pt")
    save_tensor((testing_in, testing_out), f"data/testing_set.pt")
    print("saved dataset")

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel()

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adadelta(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return ((actual >= 0.5) == (predicted >= 0.5)).sum().item() / actual.shape[0]
 
    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # é‡ç½®æ¨¡å‹çš„éšè—çŠ¶æ€
        model.reset_hidden()

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        training_in, training_out = load_tensor("data/training_set.pt")
        for index in range(0, training_in.shape[0], 32):
            # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
            batch_x = training_in[index:index+32]
            batch_y = training_out[index:index+32]
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted, batch_y)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_accuracy_list.append(calc_accuracy(batch_y, predicted))
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_in, validating_out = load_tensor("data/validating_set.pt")
        predicted = model(validating_in)
        validating_accuracy = calc_accuracy(validating_out, predicted)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 200 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        # å› ä¸ºæ•°æ®é‡å¾ˆå°‘ï¼Œä»…åœ¨è®­ç»ƒé›†æ­£ç¡®ç‡è¶…è¿‡ 70% æ—¶æ‰§è¡Œè¿™é‡Œçš„é€»è¾‘
        if training_accuracy > 0.7:
            if validating_accuracy > validating_accuracy_highest:
                validating_accuracy_highest = validating_accuracy
                validating_accuracy_highest_epoch = epoch
                save_tensor(model.state_dict(), "model.pt")
                print("highest validating accuracy updated")
            elif epoch - validating_accuracy_highest_epoch > 200:
                # åœ¨ 200 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
                print("stop training because highest validating accuracy not updated in 200 epoches")
                break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_in, testing_out = load_tensor("data/testing_set.pt")
    predicted = model(testing_in)
    testing_accuracy = calc_accuracy(testing_out, predicted)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†çš„è¯¯å·®å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validating")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel()
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # åŠ è½½å†å²æ•°æ®
    training_in, _ = load_tensor("data/training_set.pt")
    model(training_in)

    # é¢„æµ‹æœªæ¥æ•°æ®
    price_list = []
    trend_list = []
    df = pandas.read_csv(CSV_FILENAME)
    for value in df.values[-TESTING_RECORDS-VALIDATING_RECORDS:]:
        volume = float(value[-1])
        price = float(value[-3])
        if math.isnan(volume) or math.isnan(price):
            continue # åŸå§‹æ•°æ®ä¸­æ˜¯ null
        in_tensor = torch.tensor([[price / 100, volume / 100000000]])
        trend = model(in_tensor)[0].item()
        price_list.append(price)
        trend_list.append(trend)

    # æ ¹æ®é¢„æµ‹æ•°æ®æ¨¡æ‹Ÿä¹°å– 100 ä¸‡
    # è§„åˆ™ä¸ºé¢„æµ‹ T+2 æ¶¨åˆ™ä¹°å…¥ï¼Œé¢„æµ‹ T+2 è·Œåˆ™å–å‡ºï¼Œä¸è®¡ç®—å°èŠ±ç¨å’Œåˆ†çº¢
    money = 1000000
    stock = 0
    matched = 0
    total = 0
    buy_list = []
    sell_list = []
    for index in range(len(price_list)):
        price = price_list[index]
        trend = trend_list[index]
        will_rise = trend > 0.5
        will_drop = trend < 0.5
        if stock == 0 and will_rise:
            unit = int(money / price / 100) # 1 æ‰‹ 100 è‚¡
            money -= price * unit * 100
            stock += unit
            buy_list.append(price)
            sell_list.append(0)
            print(f"buy {unit}, money {money}, stock {stock}")
        elif stock != 0 and will_drop:
            unit = stock
            money += price * unit * 100
            stock -= unit
            buy_list.append(0)
            sell_list.append(price)
            print(f"sell {unit}, money {money}, stock {stock}")
        else:
            buy_list.append(0)
            sell_list.append(0)
    money_final = money + price_list[-1] * stock * 100
    print(f"final money {money_final}")
    print(f"stock price goes from {price_list[0]} to {price_list[-1]} in this range")

    # æ˜¾ç¤ºä¸ºå›¾è¡¨
    pyplot.plot(price_list, label="price")
    pyplot.plot(buy_list, label="buy", marker="$b$", linestyle = "None")
    pyplot.plot(sell_list, label="sell", marker="$s$", linestyle = "None")
    pyplot.ylim(min(price_list) - 0.05, max(price_list) + 0.05)
    pyplot.legend()
    pyplot.show()

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

è®­ç»ƒç»“æŸä»¥åçš„è¾“å‡ºå¦‚ä¸‹ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªç†æƒ³çš„ç»“æœğŸ™„:

``` text
epoch: 1004
training accuracy: 0.8902925531914894
validating accuracy: 0.53
stop training because highest validating accuracy not updated in 200 epoches
highest validating accuracy: 0.67 from epoch 803
testing accuracy: 0.5
```

è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–å¦‚ä¸‹ï¼š

![06](./06.png)

éªŒè¯æ¨¡å‹çš„éƒ¨åˆ†ä¼šåŸºäºæ²¡æœ‰è®­ç»ƒè¿‡çš„æœªçŸ¥æ•°æ® (åˆè®¡ 200 æ¡) æ¨¡æ‹Ÿäº¤æ˜“ï¼Œé¦–å…ˆå‡†å¤‡ 100 ä¸‡ï¼Œé¢„æµ‹ T+2 æ¶¨å°±ä¹°ï¼Œé¢„æµ‹ T+2 è·Œå°±å–ï¼Œä¸€å¤©åªæ“ä½œä¸€æ¬¡ï¼Œæ¯æ¬¡ä¹°å–éƒ½æ˜¯æœ€å¤§æ•°é‡ï¼Œä¸è€ƒè™‘å°èŠ±ç¨å’Œåˆ†çº¢ï¼Œæ¨¡æ‹Ÿç»“æœå¦‚ä¸‹ï¼š

``` text
final money 1089605.9999999998
stock price goes from 3.67 to 3.45 in this range
```

æ¨¡æ‹Ÿäº¤æ˜“çš„å›¾è¡¨è¡¨ç°å¦‚ä¸‹ï¼š

![07](./07.png)

åªçœ‹æ¨¡æ‹Ÿç»“æœå¯èƒ½ä¼šè§‰å¾—æ¨¡å‹å¾ˆå‰å®³ï¼Œä½†å®é™…ä¸Šè¿™åªæ˜¯ä¸ªå¶ç„¶ï¼Œè¿™æ¬¡è®­ç»ƒä¸èƒ½ç®—æ˜¯æˆåŠŸï¼Œå› ä¸ºæ­£ç¡®ç‡ä¸é«˜ï¼Œå’ŒççŒœå·®ä¸å¤šğŸ¤¢ã€‚è®­ç»ƒæ²¡æœ‰æˆåŠŸçš„åŸå› æœ‰ä¸‹ï¼š

- è‚¡ä»·çš„ä¸ç¡®å®šå› ç´ å¤ªå¤šäº†ï¼Œåªé æ¯å¤©çš„æ”¶ç›˜ä»·å’Œäº¤æ˜“é‡æ˜¯æ²¡æœ‰åŠæ³•æ­£ç¡®é¢„æµ‹å‡ºè¶‹åŠ¿çš„
- ä¸€èˆ¬æ¥è¯´è‚¡ä»·è¶‹åŠ¿çŸ­æœŸé¢„æµ‹æ¯”é•¿æœŸé¢„æµ‹çš„å‡†ç¡®ç‡è¦é«˜å¾ˆå¤šï¼Œå› ä¸ºçŸ­æœŸé¢„æµ‹çš„ä¸ç¡®å®šå› ç´ æ¯”è¾ƒå°‘ï¼Œä½†æˆ‘æ²¡æœ‰æ‰¾åˆ°å…¬å¼€çš„é«˜é¢‘è‚¡ä»·æ•°æ®
- å•åªè‚¡ç¥¨çš„æ•°æ®é‡å¾ˆå°‘ï¼Œè€Œä¸”æ¯åªè‚¡ç¥¨çš„è‚¡æ€§éƒ½ä¸ä¸€æ · (ä¾èµ–äºæ“ç›˜æ‰‹)ï¼Œå¾ˆéš¾è®­ç»ƒå‡ºé€šç”¨çš„æ¨¡å‹

é™¤äº†ä¸Šé¢çš„æ¨¡å‹ä»¥å¤–æˆ‘è¿˜è¯•äº†å¾ˆå¤šæ–¹å¼ï¼Œä¾‹å¦‚æŠŠæ¶¨è·Œå¹…ä½œä¸ºè¾“å…¥æˆ–è€…è¾“å‡ºä¸åŠ å¤§å‡å°‘æ¨¡å‹çš„ç»“æ„ï¼Œä½†éƒ½æ²¡æœ‰æ‰¾åˆ°å¯ä»¥ç¡®åˆ‡é¢„æµ‹å‡ºèµ°åŠ¿çš„æ¨¡å‹ã€‚

ä½ å¯èƒ½ä¼šå¿ä¸ä½å»è¯•è¯•æ›´å¤šæ–¹å¼ï¼Œç”šè‡³æ‰¾åˆ°æ•ˆæœæ¯”è¾ƒå¥½çš„æ¨¡å‹ï¼Œä½†æˆ‘ä½œä¸ºä¸€ä¸ªè€è‚¡æ°‘åŠä½ ä¸€å¥ï¼Œè‚¡æµ·æ— è¾¹ï¼Œå›å¤´æ˜¯å²¸å‘€ğŸ¤•ã€‚

![08](./08.png)

## å†™åœ¨æœ€å

è¿™ç¯‡æœ¬æ¥è¿˜å‡†å¤‡ä»‹ç»åŒå‘é€’å½’æ¨¡å‹çš„ä¾‹å­ï¼Œä½†é‡åˆ°ä¸€äº›æŠ€æœ¯é—®é¢˜åŠ ä¸Šæœºå™¨é…ç½®è¾ƒä½æ‰€ä»¥æ‹–äº†åŠä¸ªæœˆéƒ½æœªèƒ½å®ŒæˆğŸ˜«ï¼Œé¢„è®¡ä¸‹ä¸€ç¯‡è¿˜æ˜¯ä»‹ç»é€’å½’æ¨¡å‹çš„ä¾‹å­ï¼Œå†ä¸‹ä¸€ç¯‡å¼€å§‹å°±ä¼šä»‹ç»å¤„ç†å›¾åƒçš„ CNN æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚
