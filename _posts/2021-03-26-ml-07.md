---
layout: post
title: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (ä¸ƒ) - åŒå‘é€’å½’æ¨¡å‹ (BRNN) - æ ¹æ®ä¸Šä¸‹æ–‡è¡¥å…¨å•è¯
tag: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨
---

è¿™ä¸€ç¯‡å°†ä¼šä»‹ç»ä»€ä¹ˆæ˜¯åŒå‘é€’å½’æ¨¡å‹å’Œå¦‚ä½•ä½¿ç”¨åŒå‘é€’å½’æ¨¡å‹å®ç°æ ¹æ®ä¸Šä¸‹æ–‡è¡¥å…¨å¥å­ä¸­çš„å•è¯ã€‚

## åŒå‘é€’å½’æ¨¡å‹

åˆ°è¿™é‡Œä¸ºæ­¢æˆ‘ä»¬çœ‹åˆ°çš„ä¾‹å­éƒ½æ˜¯æŒ‰åŸæœ‰é¡ºåºæŠŠè¾“å…¥ä¼ ç»™é€’å½’æ¨¡å‹çš„ï¼Œä¾‹å¦‚ä¼ é€’ç¬¬ä¸€å¤©è‚¡ä»·ä¼šè¿”å›æ ¹æ®ç¬¬ä¸€å¤©è‚¡ä»·é¢„æµ‹çš„æ¶¨è·Œï¼Œå†ä¼ é€’ç¬¬äºŒå¤©è‚¡ä»·ä¼šè¿”å›æ ¹æ®ç¬¬ä¸€å¤©è‚¡ä»·å’Œç¬¬äºŒå¤©è‚¡ä»·é¢„æµ‹çš„æ¶¨è·Œï¼Œä»¥æ­¤ç±»æ¨ï¼Œè¿™æ ·çš„æ¨¡å‹ä¹Ÿç§°å•å‘é€’å½’æ¨¡å‹ã€‚å¦‚æœæˆ‘ä»¬è¦æ ¹æ®å¥å­çš„ä¸€éƒ¨åˆ†é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå¯ä»¥åƒä¸‹å›¾è¿™æ ·åšï¼Œè¿™æ—¶ `å¤©æ°”` ä¼šæ ¹æ® `ä»Šå¤©` è®¡ç®—, `å¾ˆå¥½` ä¼šæ ¹æ® `ä»Šå¤©` å’Œ `å¤©æ°”` è®¡ç®—ï¼š

![01](./01.png)

é‚£ä¹ˆå¦‚æœæƒ³è¦é¢„æµ‹åœ¨å¥å­ä¸­é—´çš„å•è¯å‘¢ï¼Ÿä¾‹å¦‚ç»™å‡º `ä»Šå¤©` å’Œ `å¾ˆå¥½` é¢„æµ‹ `å¤©æ°”`ï¼Œå› ä¸ºåªèƒ½æ ¹æ®å‰é¢çš„å•è¯é¢„æµ‹ï¼Œå•å‘é€’å½’æ¨¡å‹çš„æ•ˆæœä¼šæ‰“æŠ˜ï¼Œè¿™æ—¶å€™åŒå‘é€’å½’æ¨¡å‹å°±æ´¾ä¸Šç”¨åœºäº†ã€‚åŒå‘é€’å½’æ¨¡å‹ (BRNN, Bidirectional Recurrent Neural Network) ä¼šå…ˆæŒ‰åŸæœ‰é¡ºåºæŠŠè¾“å…¥ä¼ ç»™é€’å½’æ¨¡å‹ï¼Œç„¶åå†æŒ‰åå‘é¡ºåºæŠŠè¾“å…¥ä¼ ç»™é€’å½’æ¨¡å‹ï¼Œç„¶ååˆå¹¶æ­£å‘è¾“å‡ºå’Œåå‘è¾“å‡ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ`hf` ä»£è¡¨æ­£å‘è¾“å‡ºï¼Œ`hb` ä»£è¡¨åå‘è¾“å‡ºï¼ŒæŠŠå®ƒä»¬åˆå¹¶åˆ°ä¸€å—å°±å¯ä»¥å®ç°æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­é—´çš„å†…å®¹ï¼Œ`ä»Šå¤©` ä¼šæ ¹æ®åå‘çš„ `å¤©æ°”` å’Œ `å¾ˆå¥½` è®¡ç®—ï¼Œ`å¤©æ°”` ä¼šæ ¹æ®æ­£å‘çš„ `ä»Šå¤©` å’Œåå‘çš„ `å¾ˆå¥½` è®¡ç®—ï¼Œ`å¾ˆå¥½` ä¼šæ ¹æ®æ­£å‘çš„ `ä»Šå¤©` å’Œ `å¤©æ°”` è®¡ç®—ã€‚

![02](./02.png)

åœ¨ pytorch ä¸­ä½¿ç”¨åŒå‘é€’å½’æ¨¡å‹éå¸¸ç®€å•ï¼Œåªè¦åœ¨åˆ›å»ºçš„æ—¶å€™ä¼ å…¥å‚æ•° `bidirectional = True` å³å¯ï¼š

``` python
self.rnn = nn.GRU(
    input_size = 20,
    hidden_size = 50,
    num_layers = 1,
    batch_first = True,
    bidirectional = True
)
```

å•å‘é€’å½’æ¨¡å‹ä¼šè¿”å›ç»´åº¦ä¸º `æ‰¹æ¬¡å¤§å°,è¾“å…¥æ¬¡æ•°,éšè—å€¼æ•°é‡` çš„ tensorï¼Œè€ŒåŒå‘é€’å½’æ¨¡å‹ä¼šè¿”å›ç»´åº¦ä¸º `æ‰¹æ¬¡å¤§å°,è¾“å…¥æ¬¡æ•°,éšè—å€¼æ•°é‡*2` çš„ tensorã€‚

ä½ å¯èƒ½è¿˜ä¼šæœ‰ç–‘é—®ï¼ŒåŒå‘é€’å½’æ¨¡å‹ä¼šæ€æ ·å¤„ç†æ‰¹æ¬¡å‘¢ï¼Ÿå¦‚æœæ‰¹æ¬¡ä¸­æ¯ç»„æ•°æ®çš„è¾“å…¥æ¬¡æ•°éƒ½ä¸ä¸€æ ·ï¼Œé‚£ä¹ˆåå‘è®¡ç®—çš„æ—¶å€™ä¼šä¸ä¼šä»é‚£äº›å¡«å……çš„ 0 å¼€å§‹è®¡ç®—å‘¢ï¼Ÿä»¥ä¸‹æ˜¯ä¸€ä¸ªå°å®éªŒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åå‘è®¡ç®—çš„æ—¶å€™ pytorch ä¼šè·³è¿‡ç»“å°¾çš„å¡«å……å€¼ï¼Œä¸éœ€è¦åšç‰¹æ®Šçš„å¤„ç†ğŸ¥³ã€‚

``` python
>>> import torch
>>> from torch import nn
>>> x = torch.zeros((3, 3, 1))
>>> lengths = torch.tensor([1, 2, 3])
>>> rnn = torch.nn.GRU(input_size=1, hidden_size=1, batch_first=True, bidirectional=True)
>>> packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
>>> output, hidden = rnn(packed)
>>> unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)
>>> unpacked
tensor([[[0.2916, 0.2377],
         [0.0000, 0.0000],
         [0.0000, 0.0000]],

        [[0.2916, 0.2239],
         [0.3949, 0.2377],
         [0.0000, 0.0000]],

        [[0.2916, 0.2243],
         [0.3949, 0.2239],
         [0.4263, 0.2377]]], grad_fn=<IndexSelectBackward>)
```

æ­¤å¤–ï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨åŒå‘é€’å½’æ¨¡å‹æ¥å®ç°åˆ†ç±»ï¼ˆä¾‹å¦‚æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼‰ï¼Œé‚£ä¹ˆå¯ä»¥åªæŠ½å‡º (torch.gather) æ¯ç»„æ•°æ®çš„æœ€åä¸€ä¸ªæ­£å‘éšè—å€¼å’Œç¬¬ä¸€ä¸ªåå‘éšè—å€¼ï¼Œç„¶åæŠŠå®ƒä»¬ç»„åˆ (torch.cat) ä¸€èµ·ä¼ é€’åˆ°å¤šå±‚çº¿æ€§æ¨¡å‹ï¼Œå°½ç®¡å¤§å¤šæ•°æƒ…å†µä¸‹å•å‘é€’å½’æ¨¡å‹è¶³ä»¥å®ç°åˆ†ç±»åŠŸèƒ½ã€‚æå–ç»„åˆçš„ä»£ç ä¾‹å­å¦‚ä¸‹ (unpacked æ¥æºäºä¸Šä¸€ä¸ªä¾‹å­)ï¼š

``` python
>>> hidden_size = unpacked.shape[2]//2
>>> forward_last = unpacked[:,:,:hidden_size].gather(1, (lengths - 1).reshape(-1, 1, 1).repeat(1, 1, hidden_size))
>>> forward_last
tensor([[[0.2916]],

        [[0.3949]],

        [[0.4263]]], grad_fn=<GatherBackward>)
>>> backward_first = unpacked[:,:1,hidden_size:]
>>> backward_first
tensor([[[0.2377]],

        [[0.2239]],

        [[0.2243]]], grad_fn=<SliceBackward>)
>>> combined = torch.cat((forward_last, backward_first), dim=2)
>>> combined
tensor([[[0.2916, 0.2377]],

        [[0.3949, 0.2239]],

        [[0.4263, 0.2243]]], grad_fn=<CatBackward>)
>>> combined.shape
torch.Size([3, 1, 2])
```

## ä¾‹å­ - æ ¹æ®ä¸Šä¸‹æ–‡è¡¥å…¨å•è¯

è¿˜è®°å¾—æˆ‘ä»¬å°å­¦è¯­æ–‡åšçš„å¡«ç©ºé¢˜å—ï¼Œè¿™å›æˆ‘ä»¬è¯•è¯•å†™ä¸€ä¸ªç¨‹åºå¸®æˆ‘ä»¬è‡ªåŠ¨å¡«ç©ºå§ğŸ‘¦ï¼Œä¸ºäº†è¿™ä¸ªä¾‹å­æˆ‘æ¶ˆè€—äº†ä¸€ä¸ªå¤šæœˆçš„æ—¶é—´ï¼Œèµ°äº†å¾ˆå¤šå†¤æ‰è·¯ï¼Œä¸‹å›¾æ˜¯æœ€ç»ˆä½¿ç”¨çš„è®­ç»ƒæµç¨‹å’Œæ¨¡å‹ç»“æ„ï¼š

![03](./03.png)

ä»¥ä¸‹æ˜¯è¸©è¿‡çš„å‘ä¸€è§ˆğŸ¤•ï¼š

- è¾“å…¥å’Œè¾“å‡ºçš„ç¼–ç éœ€è¦åˆ†å¼€ (ä½¿ç”¨ä¸åŒçš„å‘é‡)
- è¾“å…¥çš„ç¼–ç æœ€å¥½ä¸å›ºå®š (è·Ÿéšè®­ç»ƒè°ƒæ•´)ï¼Œè¾“å‡ºçš„ç¼–ç éœ€è¦å›ºå®š (å¦åˆ™æ¨¡å‹ä¼šä½œå¼Šè®©æ‰€æœ‰å•è¯çš„è¾“å‡ºç¼–ç ç›¸åŒ)
- è¾“å‡ºçš„ç¼–ç åªèƒ½ç”± 0 å’Œ 1 ç»„æˆï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨æµ®ç‚¹æ•°ç»„æˆçš„å‘é‡ (æ¨¡å‹æ— æ³•è°ƒæ•´æ‰€æœ‰è¾“å‡ºåˆ°ç²¾ç¡®çš„å‘é‡ï¼Œåªèƒ½è°ƒæ•´åˆ°æ–¹å‘å¤§è‡´ç›¸åŒçš„å€¼ç„¶åç”¨ Sigmoid å¤„ç†)
- è¾“å‡ºçš„ç¼–ç å‘é‡é•¿åº¦å¤§çº¦ 50 ä»¥ä¸Šå³å¯é¿å… 13000 ä¸ªå•è¯è½¬æ¢åˆ° 0 å’Œ 1 ä»¥åçš„ç¼–ç å†²çªï¼Œå‘é‡é•¿åº¦è¶Šå¤§æ•ˆæœè¶Šå¥½ä½†éœ€è¦æ›´å¤šå†…å­˜å’Œè®­ç»ƒæ—¶é—´
- æ¯ä¸ªå¥å­éœ€è¦æ·»åŠ è¡¨ç¤ºå¼€å¤´å’Œç»“å°¾çš„ç¬¦å· (`<BEG>` ä¸ `<EOF>`)ï¼Œå®ƒä»¬ä¼šå½“ä½œé¢„æµ‹ç¬¬ä¸€ä¸ªå•è¯å’Œæœ€åä¸€ä¸ªå•è¯çš„è¾“å…¥ï¼Œæ¯”ä½¿ç”¨ 0 æ•ˆæœè¦å¥½ä¸€äº›
- è¾“å‡ºä¸­è¡¨ç¤ºå¼€å¤´å’Œç»“å°¾çš„å‘é‡ä¸å‚ä¸æŸå¤±çš„è®¡ç®— (é¢„æµ‹å®ƒä»¬æ²¡æœ‰æ„ä¹‰)
- æ ¹æ®é¢„æµ‹è¾“å‡ºçš„å‘é‡æŸ¥æ‰¾å¯¹åº”çš„å•è¯å¯ä»¥è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œæ‰¾å‡ºè·ç¦»æœ€æ¥è¿‘çš„å•è¯
- å‚æ•°è°ƒæ•´å™¨å¯ä»¥ä½¿ç”¨ Adamï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­æ¯” Adadelta å¿«ä¸€äº›

è¿™ä¸ªä¾‹å­æœ€å¤§çš„ç‰¹ç‚¹æ˜¯è¾“å‡ºçš„ç¼–ç ä½¿ç”¨äº† Embedding çš„å˜ç§ï¼Œä½¿å¾—ç¼–ç è¿‘ä¼¼äº binaryã€‚ä¼ ç»Ÿçš„åšæ³•æ˜¯ä½¿ç”¨ onehot + softmaxï¼Œä½†éšç€å•è¯æ•°é‡å¢å¤šéœ€è¦çš„å¤„ç†æ—¶é—´å’Œå†…å­˜å¤§å°ä¼šæš´å¢ï¼Œæˆ‘ç›®å‰çš„æœºå™¨æ˜¯è®­ç»ƒä¸è¿‡æ¥çš„ã€‚è¾“å‡ºç¼–ç ä½¿ç”¨ Embedding å˜ç§çš„å¥½å¤„è¿˜æœ‰å¯ä»¥åŒæ—¶æ‰¾å‡ºæ¥è¿‘çš„å•è¯ï¼Œä½†è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»çš„æ•ˆç‡ä¼šæ¯” onehot + softmax ç›´æ¥å¾—å‡ºæœ€å¯èƒ½å•è¯ç´¢å¼•çš„æ—¶é—´å·®å¾ˆå¤šã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä½¿ç”¨ word2vec ç”Ÿæˆè¾“å‡ºä½¿ç”¨çš„ç¼–ç ï¼Œæ¥æºæ˜¯äº¬ä¸œå•†å“è¯„è®ºï¼ˆä¸‹è½½åœ°å€è¯·å‚è€ƒä¸Šä¸€ç¯‡æ–‡ç« ï¼‰ï¼Œæ¯ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªé•¿åº¦ 100 çš„å‘é‡ï¼š

``` python
import jieba
f = open('chinese.text8', 'w')
for line in open('goods_zh.txt', 'r'):
    line = "".join(line.split(',')[:-2])
    words = list(jieba.cut(line))
    words = [w for w in words if not (w.isascii() or w in ("ï¼Œ", "ã€‚", "ï¼"))]
    words.insert(0, "<BEG>")
    words.append("<EOF>")
    f.write(" ".join(words))
    f.write(" ")

import torch
from gensim.models import word2vec
sentences = word2vec.Text8Corpus('chinese.text8')
model = word2vec.Word2Vec(sentences, size=100)
```

ç”Ÿæˆç¼–ç ä»¥åæˆ‘ä»¬éœ€è¦æŠŠç¼–ç ä¸­çš„æµ®ç‚¹æ•°è½¬æ¢ä¸º 0 æˆ–è€… 1ï¼Œæ‰§è¡Œä»¥ä¸‹ä»£ç åç¼–ç ä¸­å°äº 0 çš„å€¼ä¼šå½“ä½œ 0ï¼Œå¤§äºæˆ–ç­‰äº 0 çš„å€¼ä¼šå½“ä½œ 1ï¼š

``` python
v = torch.tensor(model.wv.vectors)
v1 = (v > 0).float()
model.wv.vectors = v1.numpy()
```

ç„¶åå†æ¥æµ‹è¯•ä¸€ä¸‹ç¼–ç æ˜¯å¦æœ‰å†²çªï¼ˆä¸¤ä¸ªå•è¯å¯¹åº”å®Œå…¨ç›¸åŒçš„å‘é‡ï¼‰ï¼Œå¦‚æœå®ƒä»¬è¾“å‡ºç›¸åŒé‚£å°±ä»£è¡¨æ²¡æœ‰é—®é¢˜ï¼š

``` python
print("wv shape:", v1.shape)
print("wv unique shape:", v1.unique(dim=0).shape)
```

æœ€åä¿å­˜ç¼–ç æ¨¡å‹åˆ°ç¡¬ç›˜ï¼š

``` python
model.save("chinese.model")
```

æ¥ä¸‹æ¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è®­ç»ƒå’Œä½¿ç”¨æ¨¡å‹ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import jieba
import json
import random
from gensim.models import word2vec
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹å¥å­ä¸­çš„å•è¯"""
    def __init__(self, w2v):
        super().__init__()
        self.hidden_size = 500
        self.embedded_in_size = 100
        self.embedded_out_size = 100
        self.linear_l1_size = 600
        self.linear_l2_size = 300
        self.embedding_in = nn.Embedding(
            num_embeddings=len(w2v.wv.vocab),
            embedding_dim=self.embedded_in_size,
            padding_idx=0
        )
        self.rnn = nn.LSTM(
            input_size = self.embedded_in_size,
            hidden_size = self.hidden_size,
            num_layers = 1,
            batch_first = True,
            bidirectional = True
        )
        self.linear = nn.Sequential(
            nn.Linear(in_features=self.hidden_size*2, out_features=self.linear_l1_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(in_features=self.linear_l1_size, out_features=self.linear_l2_size),
            nn.ReLU(),
            nn.Dropout(0.05),
            nn.Linear(in_features=self.linear_l2_size, out_features=self.embedded_out_size),
            nn.Sigmoid())

    def forward(self, x, lengths):
        # è½¬æ¢å•è¯å¯¹åº”çš„æ•°å€¼åˆ°è¾“å…¥ä½¿ç”¨çš„å‘é‡
        embedded_in = self.embedding_in(x)
        # é™„åŠ é•¿åº¦ä¿¡æ¯ï¼Œé¿å… RNN è®¡ç®—å¡«å……çš„æ•°æ®
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded_in, lengths, batch_first=True, enforce_sorted=False)
        # ä½¿ç”¨é€’å½’æ¨¡å‹è®¡ç®—ï¼Œæ¥ä¸‹æ¥çš„æ­¥éª¤éœ€è¦æ‰€æœ‰è¾“å‡ºï¼Œæ‰€ä»¥å¿½ç•¥æœ€æ–°çš„éšè—çŠ¶æ€
        output, _ = self.rnn(packed)
        # output å†…éƒ¨ä¼šè¿æ¥æ‰€æœ‰éšè—çŠ¶æ€ï¼Œshape = å®é™…è¾“å…¥æ•°é‡åˆè®¡, hidden_size
        # ä¸ºäº†æ¥ä¸‹æ¥çš„å¤„ç†ï¼Œéœ€è¦å…ˆæ•´ç† shape = batch_size, æ¯ç»„çš„æœ€å¤§è¾“å…¥æ•°é‡, hidden_size
        # ç¬¬äºŒä¸ªè¿”å›å€¼æ˜¯å„ä¸ª tensor çš„å®é™…é•¿åº¦ï¼Œå†…å®¹å’Œ lengths ç›¸åŒï¼Œæ‰€ä»¥å¯ä»¥çœç•¥æ‰
        unpacked, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)
        # æ•´ç†æ­£å‘è¾“å‡ºå’Œåå‘è¾“å‡ºï¼Œä¾‹å¦‚æœ‰ 8 ä¸ªå•è¯ï¼Œ2 ä¸ªå¡«å……
        # B 1 2 3 4 5 6 7 8 E 0 0
        # 0 B 1 2 3 4 5 6 7 8 E 0 (å¯¹åº”æ­£å‘)
        # 1 2 3 4 5 6 7 8 E 0 0 0 (å¯¹åº”åå‘)
        h = self.hidden_size
        hidden_forward = torch.cat((torch.zeros(unpacked.shape[0], 1, h), unpacked[:,:,:h]), dim=1)[:,:-1,:]
        hidden_backward = torch.cat((unpacked[:,:,h:], torch.zeros(unpacked.shape[0], 1, h)), dim=1)[:,1:,:]
        hidden = torch.cat((hidden_forward, hidden_backward), dim=2)
        # ä½¿ç”¨å¤šå±‚çº¿æ€§æ¨¡å‹æ¨æµ‹å„ä¸ªå•è¯ä»¥æ¥è¿‘åŸæœ‰å¥å­
        y = self.linear(hidden)
        return y

    def calc_loss(self, loss_function, batch_y, predicted, batch_x_lengths):
        # å‰ªåˆ‡ batch_y ä½¿å¾—ç»´åº¦ä¸ predicted ç›¸åŒï¼Œå› ä¸ºå­æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦å¯èƒ½ä¸æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦ä¸ä¸€è‡´
        batch_y = batch_y[:,:predicted.shape[1],:]
        # æ ¹æ®å®é™…é•¿åº¦æ¸…é›¶å¤´å°¾å’Œå¡«å……çš„éƒ¨åˆ†
        # ä¸èƒ½å°±åœ°ä¿®æ”¹å¦åˆ™ä¼šå¯¼è‡´ gradient computation has been modified by an inplace operation é”™è¯¯
        mask = torch.ones(predicted.shape)
        for index, length in enumerate(batch_x_lengths):
            mask[index,0,:] = 0
            mask[index,length-1:,:] = 0
        predicted = predicted * mask
        batch_y = batch_y * mask
        return loss_function(predicted, batch_y)

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def load_word2vec_model():
    """è¯»å– word2vec ç¼–ç åº“"""
    return word2vec.Word2Vec.load("chinese.model")

def prepare_save_batch(batch, pending_tensors):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æ‰“ä¹±å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    random.shuffle(pending_tensors)

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º tensorï¼Œå¦å¤–ä¿å­˜å„ä¸ªè¾“å…¥ tensor çš„é•¿åº¦
    in_tensor_unpadded = [p[0] for p in pending_tensors]
    in_tensor_lengths = torch.tensor([t.shape[0] for t in in_tensor_unpadded])
    out_tensor_unpadded = [p[1] for p in pending_tensors]

    # æ•´åˆé•¿åº¦ä¸ç­‰çš„ tensor åˆ°å•ä¸ª tensorï¼Œä¸è¶³çš„é•¿åº¦ä¼šå¡«å…… 0
    in_tensor = nn.utils.rnn.pad_sequence(in_tensor_unpadded, batch_first=True)
    out_tensor = nn.utils.rnn.pad_sequence(out_tensor_unpadded, batch_first=True)

    # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
    random_indices = torch.randperm(in_tensor.shape[0])
    training_indices = random_indices[:int(len(random_indices)*0.6)]
    validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
    testing_indices = random_indices[int(len(random_indices)*0.8):]
    training_set = (in_tensor[training_indices], in_tensor_lengths[training_indices], out_tensor[training_indices])
    validating_set = (in_tensor[validating_indices], in_tensor_lengths[validating_indices], out_tensor[validating_indices])
    testing_set = (in_tensor[testing_indices], in_tensor_lengths[testing_indices], out_tensor[testing_indices])

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # å‡†å¤‡è¯è¯­åˆ°æ•°å€¼çš„ç´¢å¼•
    w2v = load_word2vec_model()
    beg_index = w2v.wv.vocab["<BEG>"].index
    eof_index = w2v.wv.vocab["<EOF>"].index

    # æå‰è½¬æ¢è¾“å‡ºçš„ç¼–ç 
    embedding_out = nn.Embedding.from_pretrained(torch.FloatTensor(w2v.wv.vectors))

    # ä» txt è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡å¤„ç† 2000 è¡Œ
    # è¿™é‡Œä½¿ç”¨åŸå§‹æ–¹æ³•è¯»å–ï¼Œæœ€åä¸€ä¸ªæ ‡æ³¨ä¸º 1 ä»£è¡¨å¥½è¯„ï¼Œä¸º 0 ä»£è¡¨å·®è¯„
    batch = 0
    pending_tensors = []
    for line in open("goods_zh.txt", "r"):
        parts = line.split(',')
        phase = ",".join(parts[:-2])
        positive = int(parts[-1])
        # ä½¿ç”¨ jieba åˆ†è¯ï¼Œç„¶åè½¬æ¢å•è¯åˆ°ç´¢å¼•
        words = jieba.cut(phase)
        word_indices = [beg_index] # ä»£è¡¨è¯­å¥å¼€å§‹
        for word in words:
            vocab = w2v.wv.vocab.get(word)
            if vocab:
                word_indices.append(vocab.index)
        word_indices.append(eof_index) # ä»£è¡¨è¯­å¥ç»“æŸ
        if len(word_indices) <= 2:
            continue # æ²¡æœ‰å•è¯åœ¨ç¼–ç åº“ä¸­
        # è¾“å…¥æ˜¯å„ä¸ªå¥å­å¯¹åº”çš„ç´¢å¼•å€¼åˆ—è¡¨ï¼Œè¾“å‡ºæ˜¯å„ä¸ªå„ä¸ªå¥å­å¯¹åº”çš„å‘é‡åˆ—è¡¨
        tensor_in = torch.tensor(word_indices)
        tensor_out = embedding_out(tensor_in)
        pending_tensors.append((tensor_in, tensor_out))
        if len(pending_tensors) >= 2000:
            prepare_save_batch(batch, pending_tensors)
            batch += 1
            pending_tensors.clear()
    if pending_tensors:
        prepare_save_batch(batch, pending_tensors)
        batch += 1
        pending_tensors.clear()

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    w2v = load_word2vec_model()
    model = MyModel(w2v)

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.BCELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = -1
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°ï¼Œé™¤å»å¤´å°¾å’Œå¡«å……å€¼
    def calc_accuracy(actual, predicted, lengths):
        acc = 0
        for x in range(len(lengths)):
            l = lengths[x]
            predicted_record = (predicted[x][1:l-1] > 0.5).int()
            actual_record = actual[x][1:l-1].int()
            acc += (predicted_record == actual_record).sum().item() / predicted_record.numel()
        acc /= len(lengths)
        return acc
 
    # åˆ’åˆ†è¾“å…¥å’Œé•¿åº¦çš„å·¥å…·å‡½æ•°
    def split_batch_xy(batch, begin=None, end=None):
        # shape = batch_size, input_size
        batch_x = batch[0][begin:end]
        # shape = batch_size, 1
        batch_x_lengths = batch[1][begin:end]
        # shape = batch_size. input_size, embedded_size
        batch_y = batch[2][begin:end]
        return batch_x, batch_x_lengths, batch_y

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        for batch_index, batch in enumerate(read_batches("data/training_set")):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
            training_batch_accuracy_list = []
            for index in range(0, batch[0].shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œé•¿åº¦
                batch_x, batch_x_lengths, batch_y = split_batch_xy(batch, index, index+100)
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x, batch_x_lengths)
                # è®¡ç®—æŸå¤±
                loss = model.calc_loss(loss_function, batch_y, predicted, batch_x_lengths)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    training_batch_accuracy_list.append(calc_accuracy(batch_y, predicted, batch_x_lengths))
            # è¾“å‡ºæ‰¹æ¬¡æ­£ç¡®ç‡
            training_batch_accuracy = sum(training_batch_accuracy_list) / len(training_batch_accuracy_list)
            training_accuracy_list.append(training_batch_accuracy)
            print(f"epoch: {epoch}, batch: {batch_index}: batch accuracy: {training_batch_accuracy}")
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
            predicted = model(batch_x, batch_x_lengths)
            validating_accuracy_list.append(calc_accuracy(batch_y, predicted, batch_x_lengths))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
        predicted = model(batch_x, batch_x_lengths)
        testing_accuracy_list.append(calc_accuracy(batch_y, predicted, batch_x_lengths))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # è¯»å– word2vec ç¼–ç åº“
    w2v = load_word2vec_model()

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel(w2v)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è·å–å•è¯ç´¢å¼•åˆ°å‘é‡çš„ tensor
    embedding_tensor = torch.tensor(w2v.wv.vectors)

    # æŸ¥æ‰¾æœ€æ¥è¿‘å•è¯æ•°é‡çš„å‡½æ•°ï¼Œæ ¹æ®æ¬§å‡ é‡Œå¾—è·ç¦»æ¯”è¾ƒ
    # ä¹Ÿå¯ä»¥ä½¿ç”¨ w2v.wv.similar_by_vector
    def find_similar_words(target_tensor):
        top_words = 10
        similar_words = []
        for word, vocab in w2v.wv.vocab.items():
            index = vocab.index
            distance = torch.dist(embedding_tensor[index], target_tensor, 2).item()
            if len(similar_words) < top_words or distance < similar_words[-1][1]:
                similar_words.append((word, distance))
                similar_words.sort(key=lambda v: v[1])
                if len(similar_words) > top_words:
                    similar_words.pop()
        return similar_words

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    # __ ä¸ºé¢„æµ‹ç›®æ ‡ï¼Œä¾‹å¦‚ä¸‹æ¬¡è¿˜æ¥__è´­ä¹° è¡¨ç¤ºé¢„æµ‹ __ å¤„çš„å•è¯ï¼Œåªæ”¯æŒä¸€ä¸ªé¢„æµ‹ç›®æ ‡
    while True:
        try:
            phase = input("Sentence: ")
            phase = phase.replace("\t", "").replace("__", "\t")
            if "\t" not in phase:
                raise ValueError("Please use __ to represent predict target")
            if phase.count("\t") > 1:
                raise ValueError("Please only use one predict target")
            # åˆ†è¯
            words = list(jieba.cut(phase))
            # è½¬æ¢åˆ°æ•°å€¼åˆ—è¡¨
            word_indices = [1] # ä»£è¡¨è¯­å¥å¼€å§‹
            for word in words:
                if word == '\t':
                    word_indices.append(0) # é¢„æµ‹ç›®æ ‡
                    continue
                vocab = w2v.wv.vocab.get(word)
                if vocab:
                    word_indices.append(vocab.index)
            word_indices.append(2) # ä»£è¡¨è¯­å¥ç»“æŸ
            if len(word_indices) <= 2:
                raise ValueError("No known words")
            # æ„å»ºè¾“å…¥
            x = torch.tensor(word_indices).reshape(1, -1)
            lengths = torch.tensor([len(word_indices)])
            # é¢„æµ‹è¾“å‡º
            predicted = model(x, lengths)
            # æ‰¾å‡ºæœ€æ¥è¿‘çš„å•è¯ä¸€è§ˆ
            target_index = word_indices.index(0)
            target_tensor = (predicted[0, target_index] > 0.5).float()
            similar_words = find_similar_words(target_tensor)
            for word, distance in similar_words:
                print(word, distance)
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å‡†å¤‡è®­ç»ƒéœ€è¦çš„æ•°æ®å’Œå¼€å§‹è®­ç»ƒï¼š

``` text
python3 example.py prepare
python3 example.py train
```

è®­ç»ƒç»“æœå¦‚ä¸‹ï¼ˆä½¿ç”¨ CPU è®­ç»ƒéœ€è¦å¤§çº¦ä¸¤å¤©æ—¶é—´ğŸ¤¢ï¼‰ï¼Œè¿™é‡Œçš„æ­£ç¡®ç‡ä»£è¡¨é¢„æµ‹è¾“å‡ºå’Œå®é™…è¾“å‡ºå‘é‡ä¸­æœ‰å¤šå°‘ä¸ªå€¼æ˜¯ç›¸ç­‰çš„ï¼š

``` text
training accuracy: 0.8106725109454498
validating accuracy: 0.7361285656628191
stop training because highest validating accuracy not updated in 20 epoches
highest validating accuracy: 0.7382469316157465 from epoch 18
testing accuracy: 0.7378169895469142
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š

``` text
python3 example.py eval
```

ä»¥ä¸‹æ˜¯ä¸€äº›ä½¿ç”¨ä¾‹å­ï¼Œ`__` ï¼ˆä¸¤ä¸ªä¸‹åˆ’çº¿ï¼‰ä»£è¡¨é¢„æµ‹ç›®æ ‡çš„å•è¯ï¼Œä¼šè¾“å‡ºæœ€æ¥è¿‘çš„ 10 ä¸ªå•è¯ï¼š

``` text
Sentence: è¡£æœè´¨é‡__å“¦
ä¸é”™ 0.0
å¾ˆæ£’ 3.872983455657959
æŒºä¸é”™ 4.0
ç‰©æœ‰æ‰€å€¼ 4.582575798034668
ç‰©è¶…æ‰€å€¼ 4.795831680297852
å¾ˆèµ 4.795831680297852
è¶…å¥½ 4.795831680297852
å¤ªå¥½äº† 4.795831680297852
å¥½ 5.0
å¤ªæ£’äº† 5.0
Sentence: é‹å­è½»ä¾¿__ï¼Œå¥½ç©¿ï¼Œå€¼å¾—æ¨èã€‚
ä¿®èº« 3.316624879837036
èº«æ 3.464101552963257
æ˜¾ 3.464101552963257
è´´èº« 3.464101552963257
ä¼‘é—² 3.605551242828369
è½¯å’Œ 3.605551242828369
ä¿æš– 3.7416574954986572
å‡‰å¿« 3.7416574954986572
æŸ”è½¯ 3.7416574954986572
è½»å¿« 3.7416574954986572
Sentence: é‹å­è½»ä¾¿èˆ’æœï¼Œå¥½ç©¿ï¼Œå€¼å¾—__ã€‚
æ‹¥æœ‰ 3.316624879837036
å¤Ÿä¹° 3.605551242828369
ä¿¡èµ– 3.7416574954986572
è´­ä¹° 4.242640495300293
ä¿¡è€ 4.582575798034668
æ¨è 4.795831680297852
å…¥æ‰‹ 4.795831680297852
è¡¨æ‰¬ 4.795831680297852
ç‚¹èµ 5.0
ä¸‹æ‰‹ 5.0
Sentence: é‹å­è½»ä¾¿èˆ’æœï¼Œå¥½ç©¿ï¼Œ__æ¨èã€‚
å€¼å¾— 1.4142135381698608
æ”¾å¿ƒ 4.690415859222412
å€¼ 4.795831680297852
ç‰©ç¾ä»·å»‰ 5.099019527435303
ä»·å»‰ç‰©ç¾ 5.099019527435303
ä»·æ ¼ä¾¿å®œ 5.196152210235596
åŠ æ²¹ 5.196152210235596
ä¸€ç™¾åˆ† 5.196152210235596
å¾ˆèµ 5.196152210235596
èµèµèµ 5.196152210235596
Sentence: å‘è´§__å¾ˆèµï¼Œä¸œè¥¿ä¹ŸæŒºå¥½
é€Ÿåº¦ 2.4494898319244385
è¿…é€Ÿ 4.898979663848877
ç»™åŠ› 5.0
åŠ› 5.0
ä»·æ ¼ä¾¿å®œ 5.0
æ²¡å¾—è¯´ 5.196152210235596
è¶…å€¼ 5.196152210235596
å¾ˆèµ 5.196152210235596
å°å“¥ 5.291502475738525
å°å·§ 5.291502475738525
Sentence: åŠä¸ªæœˆå°±å‡ºç°è¿™é—®é¢˜ ï¼Œ__ç›´æ¥è¯´æ‰¾é™„è¿‘ç«™ç‚¹å”®å ï¼Œæµªè´¹æ—¶é—´ï¼Œè¿˜å¾—è‡ªå·±ä¿®ï¼Œå·®è¯„ä¸€ä¸ª
å®¢æœ 0.0
å•†å®¶ 4.690415859222412
å–å®¶ 4.898979663848877
å”®å 5.099019527435303
æ²¡äºº 5.099019527435303
åº—å®¶ 5.196152210235596
è¡¥å‘ 5.291502475738525
äººå·¥ 5.291502475738525
å®¢æˆ· 5.385164737701416
æœºå™¨äºº 5.385164737701416
Sentence: ä¸é”™ç»™è€å…¬ä¹°äº†å¥½å‡ ä¸ªäº†ï¼Œç©¿ç€ç‰¹åˆ«__
èˆ’æœ 0.0
èˆ’é€‚ 3.316624879837036
æŒºèˆ’æœ 4.242640495300293
å¸…æ°” 4.690415859222412
è„šç–¼ 4.690415859222412
å¾ˆå¸… 4.795831680297852
å‡‰å¿« 4.898979663848877
åˆèº« 5.0
æš–å’Œ 5.099019527435303
è€å…¬ 5.291502475738525
Sentence: ä¸é”™ç»™__ä¹°äº†å¥½å‡ ä¸ªäº†ï¼Œç©¿ç€ç‰¹åˆ«èˆ’æœ
è€çˆ¸ 2.8284270763397217
çˆ¸çˆ¸ 3.0
å¼Ÿå¼Ÿ 3.0
å¦¹å¦¹ 3.0
å¥³æœ‹å‹ 3.0
ç”·æœ‹å‹ 3.1622776985168457
è€å¦ˆ 3.1622776985168457
å¥³å„¿ 3.316624879837036
è¡¨å¼Ÿ 3.316624879837036
å®¶äºº 3.316624879837036
```

å¯ä»¥çœ‹åˆ°é¢„æµ‹å‡ºæ¥çš„æ•ˆæœè¿˜ä¸é”™ğŸ˜ˆï¼Œå°½ç®¡éƒ¨åˆ†è¯­å¥æ²¡æœ‰å®Œå…¨å‡†ç¡®çš„é¢„æµ‹å‡ºåŸæœ‰çš„å•è¯ä½†æ˜¯è¯­ä¹‰å¾ˆæ¥è¿‘ã€‚å¦‚æœä½ æƒ³å¾—åˆ°æ›´å¥½çš„æ•ˆæœï¼Œå¯ä»¥å¢åŠ è¾“å‡ºå‘é‡é•¿åº¦ (word2vec ç”Ÿæˆæ—¶çš„ size å‚æ•°ï¼Œå¯¹åº” embedded_out_size)ï¼Œè¾“å…¥å‘é‡é•¿åº¦ï¼ˆembedded_in_sizeï¼‰ï¼Œå’Œæ¨¡å‹çš„éšè—å€¼æ•°é‡ï¼ˆhidden_size, linear_l1_size, linear_l2_sizeï¼‰ï¼Œä½†ä¼šéœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´å’Œå†…å­˜ğŸ¤¢ã€‚

## å†™åœ¨æœ€å

å…³äºé€’å½’æ¨¡å‹å°±ä»‹ç»åˆ°è¿™é‡Œäº†ï¼Œä¸‹ä¸€ç¯‡å¼€å§‹å°†ä¼šä»‹ç»é€‚åˆå¤„ç†å›¾åƒçš„å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¨¡å‹ï¼Œæ•¬è¯·æœŸå¾…ã€‚

æœ¬æ¥æƒ³ä¹°å°å¸¦æ˜¾å¡ (æ”¯æŒ CUDA) çš„æœºå™¨å‡å°‘è®­ç»ƒæ‰€éœ€çš„æ—¶é—´ï¼Œä½†æ˜¯é»„è„¸å©†ä¸å…è®¸ğŸ¥µï¼Œä¼°è®¡ä¸€æ®µæ—¶é—´å†…åªèƒ½ç»§ç»­ç”¨ CPU è®­ç»ƒäº†ã€‚
