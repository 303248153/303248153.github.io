---
layout: post
title: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (åä¸€) - å¯¹è±¡è¯†åˆ« YOLO - è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©
tag: å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨
---

è¿™ç¯‡å°†ä¼šä»‹ç»ç›®å‰æœ€æµè¡Œçš„å¯¹è±¡è¯†åˆ«æ¨¡å‹ YOLOï¼ŒYOLO çš„ç‰¹å¾æ˜¯å¿«ï¼Œè¯†åˆ«é€Ÿåº¦éå¸¸å¿«ğŸ¤—ï¼Œç„¶è€Œç²¾åº¦ç›¸å¯¹ Faster-RCNN åªå·®ä¸€ç‚¹ç‚¹ (YOLOv3 ä¹‹å)ã€‚é˜…è¯»è¿™ç¯‡éœ€è¦å…ˆäº†è§£å¯¹è±¡è¯†åˆ«çš„åŸç†ï¼Œå¦‚æœä½ æ²¡çœ‹è¿‡è¿™ä¸ªç³»åˆ—çš„å‰å‡ ç¯‡æ–‡ç«  (ä»‹ç» RCNN, Fast-RCNN, Faster-RCNN çš„æ–‡ç« )ï¼Œè¯·å…ˆé˜…è¯»å®ƒä»¬ã€‚

## YOLO æ¨¡å‹æ¦‚è§ˆ

YOLO çš„ç¼©å†™æ˜¯ You only look onceï¼Œç¿»è¯‘æˆä¸­æ–‡æ˜¯å®è´ä½ åªéœ€è¦çœ‹ä¸€æ¬¡å–”ğŸ˜˜ã€‚YOLO æ¨¡å‹å¯ä»¥ç›´æ¥æ ¹æ®å›¾ç‰‡è¾“å‡ºåŒ…å«å¯¹è±¡çš„åŒºåŸŸä¸åŒºåŸŸå¯¹åº”çš„åˆ†ç±»ï¼Œä¸€æ­¥åˆ°ä½ï¼Œä¸åƒ RCNN ç³»åˆ—çš„æ¨¡å‹éœ€è¦å…ˆè®¡ç®—åŒ…å«å¯¹è±¡çš„åŒºåŸŸï¼Œå†æ ¹æ®åŒºåŸŸåˆ¤æ–­å¯¹åº”çš„åˆ†ç±»ï¼ŒYOLO æ¨¡å‹çš„é€Ÿåº¦æ¯” RCNN ç³»åˆ—çš„æ¨¡å‹è¦å¿«å¾ˆå¤šã€‚

YOLO æ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼š

![01](./01.png)

æ˜¯ä¸æ˜¯è§‰å¾—æœ‰ç‚¹ç†Ÿæ‚‰ï¼Ÿçœ‹ä¸Šå»å°±åƒ Faster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œ (RPN) å•Šã€‚çš„ç¡®ï¼ŒYOLO æ¨¡å‹åŸç†ä¸Šå°±æ˜¯å¯»æ‰¾åŒºåŸŸçš„åŒæ—¶åˆ¤æ–­åŒºåŸŸåŒ…å«çš„å¯¹è±¡åˆ†ç±»ï¼ŒYOLO æ¨¡å‹ä¸åŒºåŸŸç”Ÿæˆç½‘ç»œæœ‰ä»¥ä¸‹çš„ä¸åŒï¼š

- YOLO æ¨¡å‹ä¼šè¾“å‡ºå„ä¸ªåŒºåŸŸæ˜¯å¦åŒ…å«**å¯¹è±¡ä¸­å¿ƒ**ï¼Œè€Œä¸æ˜¯åŒ…å«å¯¹è±¡çš„ä¸€éƒ¨åˆ†
- YOLO æ¨¡å‹ä¼šåŒæ—¶è¾“å‡ºå¯¹è±¡åˆ†ç±»
- YOLO æ¨¡å‹è¾“å‡ºçš„åŒºåŸŸåç§»ä¼šæ ¹æ®å¯¹è±¡ä¸­å¿ƒç‚¹è®¡ç®—ï¼Œå…·ä½“ç®—æ³•åœ¨ä¸‹é¢è¯´æ˜

YOLO æ¨¡å‹ä¸ Faster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œæœ€å¤§çš„ä¸åŒæ˜¯ä¼šåˆ¤æ–­å„ä¸ªåŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ä¸­å¿ƒï¼Œå¦‚ä¸‹å›¾ä¸­ç‹—è„¸è¦†ç›–äº†å››ä¸ªåŒºåŸŸï¼Œä½†åªæœ‰å·¦ä¸‹è§’çš„åŒºåŸŸåŒ…å«äº†ç‹—è„¸çš„ä¸­å¿ƒï¼ŒYOLO æ¨¡å‹åº”è¯¥åªåˆ¤æ–­è¿™ä¸ªåŒºåŸŸåŒ…å«å¯¹è±¡ã€‚

![02](./02.png)

å½“ç„¶ï¼Œå¦‚æœå¯¹è±¡ä¸­å¿ƒéå¸¸æ¥è¿‘åŒºåŸŸçš„è¾¹ç•Œï¼Œé‚£ä¹ˆåˆ¤æ–­èµ·æ¥å°†ä¼šå¾ˆå›°éš¾ï¼ŒYOLO æ¨¡å‹åœ¨è®­ç»ƒçš„æ—¶å€™ä¼šå¿½ç•¥å¯¹è±¡é‡å ç‡é«˜äºä¸€å®šæ°´å¹³çš„åŒºåŸŸï¼Œå…·ä½“å¯ä»¥å‚è€ƒåé¢ç»™å‡ºçš„ä»£ç ã€‚

YOLO æ¨¡å‹ä¼šé’ˆå¯¹å„ä¸ªåŒºåŸŸè¾“å‡ºä»¥ä¸‹çš„ç»“æœï¼Œè¿™é‡Œå‡è®¾æœ‰ä¸‰ä¸ªåˆ†ç±»ï¼š

- æ˜¯å¦åŒ…å«å¯¹è±¡ä¸­å¿ƒ (æ˜¯ä¸º 1, å¦ä¸º 0)
- åŒºåŸŸåç§» x
- åŒºåŸŸåç§» y
- åŒºåŸŸåç§» w
- åŒºåŸŸåç§» h
- åˆ†ç±» 1 çš„å¯èƒ½æ€§ (0 ~ 1)
- åˆ†ç±» 2 çš„å¯èƒ½æ€§ (0 ~ 1)
- åˆ†ç±» 3 çš„å¯èƒ½æ€§ (0 ~ 1)

è¾“å‡ºç»“æœçš„ç»´åº¦æ˜¯ `æ‰¹æ¬¡å¤§å°, åŒºåŸŸæ•°é‡, 5 + åˆ†ç±»æ•°é‡`ã€‚

åŒºåŸŸåç§»ç”¨äºè°ƒæ•´è¾“å‡ºçš„åŒºåŸŸèŒƒå›´ï¼Œä¾‹å¦‚ä¸Šå›¾ä¸­ç‹—è„¸çš„ä¸­å¿ƒç‚¹å¤§çº¦åœ¨åŒºåŸŸçš„å³ä¸Šè§’ï¼Œå¦‚æœæŠŠåŒºåŸŸå·¦ä¸Šè§’çœ‹ä½œ (0, 0)ï¼Œå³ä¸‹è§’çœ‹ä½œ (1, 1)ï¼Œé‚£ä¹ˆç‹—è„¸ä¸­å¿ƒç‚¹åº”è¯¥åœ¨ (0.95, 0.1) çš„ä½ç½®ï¼Œè€Œç‹—è„¸å¤§å°ç›¸å¯¹äºåŒºåŸŸé•¿å®½å¤§æ¦‚æ˜¯ (1.3, 1.5) å€ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®çš„æ—¶å€™ä¼šæ ¹æ®è¿™ 4 ä¸ªå€¼è®¡ç®—åŒºåŸŸåç§»ï¼Œå…·ä½“è®¡ç®—ä»£ç åœ¨ä¸‹é¢ç»™å‡ºã€‚

![03](./03.png)

çœ‹åˆ°è¿™é‡Œä½ å¯èƒ½ä¼šæƒ³ï¼ŒYOLO æ¨¡å‹çœ‹èµ·æ¥å¾ˆç®€å•å•Šï¼Œæˆ‘å¯ä»¥ä¸¢æ‰æ“è›‹çš„ Faster-RCNN æ¨¡å‹äº†ğŸ¤¢ã€‚ä¸ï¼Œæ²¡é‚£ä¹ˆç®€å•ï¼Œä»¥ä¸Šä»‹ç»çš„åªæ˜¯ YOLO**v1** æ¨¡å‹ï¼ŒYOLOv1 æ¨¡å‹çš„ç²¾åº¦éå¸¸ä½ï¼Œåé¢ä¸ºäº†æ”¹è¿›è¯†åˆ«ç²¾åº¦è¿˜å‘å±•å‡º YOLOv2, YOLOv3, YOLOv4, YOLOv5 æ¨¡å‹ğŸ˜®ï¼Œæ¥ä¸‹æ¥å°†ä¼šä»‹ç» YOLOv2, YOLOv3 æ¨¡å‹ä¸»è¦æ”¹è¿›äº†ä»€ä¹ˆéƒ¨åˆ†ï¼Œå†ç»™å‡º YOLOv3 æ¨¡å‹çš„å®ç°ã€‚YOLOv4 å’Œ YOLOv5 æ¨¡å‹ä¸»è¦æ”¹è¿›äº†æå–ç‰¹å¾ç”¨çš„ CNN æ¨¡å‹ (ä¹Ÿç§°éª¨å¹²ç½‘ç»œ Backbone Network)ï¼ŒåŸå§‹çš„ YOLO æ¨¡å‹ä½¿ç”¨äº† C è¯­è¨€ç¼–å†™çš„ Darknet ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œè€Œè¿™ç¯‡ä½¿ç”¨ Resnet ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæ‰€ä»¥åªä»‹ç»åˆ° YOLOv3ã€‚

### YOLOv2

YOLOv2 æœ€ä¸»è¦çš„æ”¹è¿›ç‚¹æ˜¯å¼•å…¥äº†é”šç‚¹ (Anchor)ï¼Œå¦‚æœä½ å·²ç»çœ‹å®Œå‰å‡ ç¯‡æ–‡ç« é‚£ä¹ˆåº”è¯¥å¾ˆäº†è§£é”šç‚¹æ˜¯ä»€ä¹ˆï¼Œé”šç‚¹ä¼šä»æ¯ä¸ªåŒºåŸŸçš„ä¸­å¿ƒç‚¹è¡ç”Ÿå‡ºä¸åŒå½¢çŠ¶çš„å¤šä¸ªé”šç‚¹åŒºåŸŸï¼š

![04](./04.png)

Faster-RCNN ä½¿ç”¨é”šç‚¹ä¸»è¦ä¸ºäº†æå‡åŒºåŸŸé‡å ç‡ä»¥é¿å…æ¼æ‰éƒ¨åˆ†å¯¹è±¡ (Faster-RCNN è®­ç»ƒæ—¶ä¼šæ ¹æ®é‡å ç‡åˆ¤æ–­åŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œå¦‚æœå¯¹è±¡å¾ˆé•¿æˆ–è€…å¾ˆå®½ä½†å½¢çŠ¶åªæœ‰æ­£æ–¹å½¢ï¼Œé‚£ä¹ˆé‡å ç‡å°±ä¼šæ¯”è¾ƒä½å¯¼è‡´è¯¥å¯¹è±¡è¢«æ¼æ‰)ï¼Œç„¶è€Œ YOLO ä½¿ç”¨å¯¹è±¡ä¸­å¿ƒç‚¹ï¼Œå¹¶ä¸ä¼šå­˜åœ¨å› é‡å ç‡ä¸è¶³è€Œæ¼æ‰å¯¹è±¡çš„é—®é¢˜ï¼ŒYOLO ä½¿ç”¨é”šç‚¹æ˜¯ä¸ºäº†æ”¯æŒè¯†åˆ«ä¸­å¿ƒä½äºåŒä¸€ä¸ªåŒºåŸŸçš„å¤šä¸ªå¯¹è±¡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![05](./05.png)

å¦‚æœå¯¹è±¡ä¸­å¿ƒè½åœ¨æŸä¸ªåŒºåŸŸï¼ŒYOLO ä¼šè®¡ç®—è¯¥åŒºåŸŸå¯¹åº”çš„å„ä¸ªå½¢çŠ¶çš„é‡å ç‡ï¼Œå¹¶ä½¿ç”¨é‡å ç‡æœ€é«˜çš„å½¢çŠ¶ï¼Œè¿™æ ·å¦‚æœå¤šä¸ªå¯¹è±¡ä¸­å¿ƒè½åœ¨åŒä¸€ä¸ªåŒºåŸŸä½†å®ƒä»¬çš„å½¢çŠ¶ä¸åŒï¼Œå°±ä¼šåˆ†åˆ«åˆ¤æ–­å‡ºä¸åŒçš„åˆ†ç±»ã€‚YOLOv2 çš„è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![06](./06.png)

è¾“å‡ºç»“æœçš„ç»´åº¦æ˜¯ `æ‰¹æ¬¡å¤§å°, åŒºåŸŸæ•°é‡ * å½¢çŠ¶æ•°é‡, 5 + åˆ†ç±»æ•°é‡`ã€‚

YOLOv2 è¿˜æœ‰ä¸€äº›é’ˆå¯¹éª¨å¹²ç½‘ç»œå’Œè®­ç»ƒæ–¹æ³•çš„æ”¹è¿›ç‚¹ï¼Œä½†è¿™ç¯‡æ–‡ç« éƒ½æ²¡ç”¨åˆ°æ‰€ä»¥å°±ä¸ä»‹ç»äº†ï¼Œå¦‚æœä½ æœ‰å…´è¶£å¯ä»¥å‚è€ƒåé¢ç»™å‡ºçš„è®ºæ–‡é“¾æ¥ã€‚

ä½ å¯èƒ½ä¼šæ³¨æ„åˆ° YOLO åˆ’åˆ†çš„åŒºåŸŸæ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸”åˆ¤æ–­åŒºåŸŸæ˜¯å¦å­˜åœ¨å¯¹è±¡å’Œå¯¹è±¡çš„åˆ†ç±»æ—¶åªä¼šä½¿ç”¨è¯¥åŒºåŸŸä¸­çš„æ•°æ®ï¼Œè¿™æ ·ä¼šå¯¼è‡´ä»¥ä¸‹çš„é—®é¢˜ï¼š

- å¦‚æœå¯¹è±¡ç›¸å¯¹åŒºåŸŸè¿‡å¤§ï¼Œåˆ™æ¨¡å‹å¾ˆéš¾ç¡®å®šå“ªä¸ªåŒºåŸŸåŒ…å«ä¸­å¿ƒç‚¹
    - Faster-RCNN æŒ‰é”šç‚¹åŒºåŸŸçš„é‡å ç‡è€Œä¸æ˜¯ä¸­å¿ƒç‚¹åˆ¤æ–­æ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œæ‰€ä»¥ä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜
- å¦‚æœå¯¹è±¡ç›¸å¯¹åŒºåŸŸè¿‡å¤§ï¼Œåˆ™æ¯ä¸ªåŒºåŸŸéƒ½åªåŒ…å«å¯¹è±¡çš„ä¸€å°éƒ¨åˆ†ï¼Œå¾ˆéš¾ä¾æ®è¿™ä¸€å°éƒ¨åˆ†æ¥åˆ¤æ–­å¯¹è±¡åˆ†ç±» (ä¾‹å¦‚åŒºåŸŸåªåŒ…å«é¼»å­çš„æ—¶å€™æ¨¡å‹éœ€è¦åªæ ¹æ®é¼»å­åˆ¤æ–­æ˜¯å¦äººè„¸)
    - Faster-RCNN åˆ†ä¸¤æ­¥èµ°ï¼Œæ ‡ç­¾åˆ†ç±»ç½‘ç»œä¼šæ ¹æ®åŒºåŸŸç”Ÿæˆç½‘ç»œç»™å‡ºçš„åŒºåŸŸæˆªå–ç‰¹å¾å†åˆ¤æ–­åˆ†ç±»ï¼Œæ‰€ä»¥ä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜
- å¦‚æœå¯¹è±¡ç›¸å¯¹åŒºåŸŸè¿‡å°ï¼Œåˆ™å¤šä¸ªå¯¹è±¡æœ‰å¯èƒ½å¤„äºåŒä¸€ä¸ªåŒºåŸŸä¸­
    - å› ä¸º Faster-RCNN ä¸ä¼šæœ‰ä»¥ä¸Šä¸¤ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥å¯ä»¥ç”¨æ›´å°çš„åŒºåŸŸ

![07](./07.png)

å› æ­¤ï¼ŒYOLOv2 åªé€‚åˆå¯¹è±¡å¤§å°å’ŒåŒºåŸŸå¤§å°æ¯”è¾ƒæ¥è¿‘çš„åœºæ™¯ã€‚

### YOLOv3

ä¸ºäº†æ›´å¥½çš„æ”¯æŒä¸åŒå¤§å°çš„å¯¹è±¡ï¼ŒYOLOv3 å¼•å…¥äº†å¤šå°ºåº¦æ£€æµ‹æœºåˆ¶ (Multi-Scale Detection)ï¼Œè¿™ä¸ªæœºåˆ¶å¯ä»¥è¯´æ˜¯ YOLO æ¨¡å‹çš„ç²¾åï¼Œå¼•å…¥è¿™ä¸ªæœºåˆ¶ä¹‹å‰ YOLO æ¨¡å‹çš„ç²¾åº¦å¾ˆä¸ç†æƒ³ï¼Œè€Œå¼•å…¥ä¹‹å YOLO æ¨¡å‹è¾¾åˆ°äº†æ¥è¿‘ Faster-RCNN çš„ç²¾åº¦ï¼Œå¹¶ä¸”é€Ÿåº¦è¿˜æ˜¯æ¯” Faster-RCNN è¦å¿«ã€‚

å¤šå°ºåº¦æ£€æµ‹æœºåˆ¶ç®€å•çš„æ¥è¯´å°±æ˜¯æŒ‰ä¸åŒçš„å°ºåº¦åˆ’åˆ†åŒºåŸŸï¼Œç„¶åå†æ£€æµ‹è¿™äº›ä¸åŒå¤§å°çš„åŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œ**æ£€æµ‹çš„æ—¶å€™å¤§åŒºåŸŸçš„ç‰¹å¾ä¼šæ··åˆåˆ°å°åŒºåŸŸä¸­**ï¼Œä½¿å¾—å°åŒºåŸŸåˆ¤æ–­æ—¶æ‹¥æœ‰ä¸€å®šç¨‹åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

![08](./08.png)

å®ç°å¤šå°ºåº¦æ£€æµ‹æœºåˆ¶é¦–å…ˆè¦è®© CNN æ¨¡å‹è¾“å‡ºä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œæˆ‘ä»¬ä¹‹å‰å·²ç»çœ‹è¿‡ CNN æ¨¡å‹ä¸­çš„å·ç§¯å±‚å¯ä»¥è¾“å‡ºæ¯”åŸæœ‰å¤§å°æ›´å°çš„ç‰¹å¾ (å‚è€ƒç¬¬ 8 ç¯‡)ï¼Œä¾‹å¦‚æŒ‡å®šå†…æ ¸å¤§å° (kernel_size) ä¸º 3ï¼Œå¤„ç†é—´éš” (stride) ä¸º 2ï¼Œå¡«å……å¤§å° (padding) ä¸º 1 çš„æ—¶å€™ï¼Œè¾“å‡ºå¤§å°åˆšå¥½æ˜¯è¾“å…¥å¤§å°çš„ä¸€åŠï¼ŒæŠŠè¿™æ ·çš„å·ç§¯å±‚æ”¾åˆ° CNN æ¨¡å‹çš„æœ«å°¾ï¼Œç„¶åä¿ç•™å„ä¸ªå·ç§¯å±‚çš„è¾“å‡ºï¼Œå°±å¯ä»¥å¾—å‡ºä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚ä¾‹å¦‚æŒ‡å®š 3 ä¸ªå°ºåº¦çš„æ—¶å€™ï¼Œå¯èƒ½ä¼šå¾—åˆ°ä»¥ä¸‹å¤§å°çš„ 3 ä¸ªç‰¹å¾ï¼š

- æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡, 8, 8
- æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡, 4, 4
- æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡, 2, 2

ä¹‹åå†åå‘å¤„ç†è¿™ä¸‰ä¸ªç‰¹å¾ï¼Œé¦–å…ˆæŠŠ `æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡, 2, 2` äº¤ç»™è¿›ä¸€æ­¥å¤„ç†ç‰¹å¾çš„ CNN æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹ä¼šè®©è¾“å‡ºé•¿å®½ç­‰äºè¾“å…¥é•¿å®½ï¼Œæ‰€ä»¥è¾“å‡ºå¤§å°å’ŒåŸæœ‰å¤§å°ç›¸åŒï¼Œå†æ‰©å¤§ç‰¹å¾åˆ° `æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡, 4, 4`ï¼Œä¾‹å¦‚ï¼š

``` text
a b
c d
```

æ‰©å¤§ä»¥åä¼šå˜ä¸º


``` text
a a b b
a a b b
c c d d
c c d d
```

ä¹‹åå†åˆå¹¶è¿™ä¸ªç‰¹å¾åˆ°å¤§å°ä¸º `æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡, 4, 4` çš„ç‰¹å¾ï¼Œå¾—å‡º `æ‰¹æ¬¡å¤§å°, é€šé“æ•°é‡ * 2, 4, 4` çš„ç‰¹å¾ï¼ŒæŠŠè¿™ä¸ªç‰¹å¾äº¤ç»™è¿›ä¸€æ­¥å¤„ç†ç‰¹å¾çš„ CNN æ¨¡å‹ï¼Œä¹‹åçš„æµç¨‹å°±å¦‚ä¸Šå›¾æ‰€ç¤ºäº†ï¼Œæœ€ç»ˆä¼šå¾—å‡ºä»¥ä¸‹å¤§å°çš„ 3 ä¸ªç»“æœï¼š

- æ‰¹æ¬¡å¤§å°, å½¢çŠ¶æ•°é‡ * (5 + åˆ†ç±»æ•°é‡), 8, 8
- æ‰¹æ¬¡å¤§å°, å½¢çŠ¶æ•°é‡ * (5 + åˆ†ç±»æ•°é‡), 4, 4
- æ‰¹æ¬¡å¤§å°, å½¢çŠ¶æ•°é‡ * (5 + åˆ†ç±»æ•°é‡), 2, 2

å˜å½¢ä»¥åå¾—å‡ºï¼š

- æ‰¹æ¬¡å¤§å°, 8 * 8 * å½¢çŠ¶æ•°é‡, 5 + åˆ†ç±»æ•°é‡
- æ‰¹æ¬¡å¤§å°, 4 * 4 * å½¢çŠ¶æ•°é‡, 5 + åˆ†ç±»æ•°é‡
- æ‰¹æ¬¡å¤§å°, 2 * 2 * å½¢çŠ¶æ•°é‡, 5 + åˆ†ç±»æ•°é‡

æ€»ç»“èµ·æ¥ï¼ŒYOLOv3 æ¨¡å‹çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![09](./09.png)

## YOLO æ¨¡å‹çš„å®ç°

æ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹ YOLO æ¨¡å‹çš„å®ç°ç»†èŠ‚ï¼Œåé¢ä¼šç»™å‡ºå®Œæ•´ä»£ç ã€‚æ³¨æ„è¿™ç¯‡çš„å®ç°ä¸å®˜æ–¹å®ç°ä¸å®Œå…¨ä¸€æ ·ğŸ¤•ï¼Œè¿™ç¯‡ä¼šç”¨ Resnet ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶ä¸”ä¼šä»¥è¯†åˆ«äººè„¸ä½ç½®ä¸ºç›®æ ‡è°ƒæ•´å‚æ•°ã€‚

### å®šä¹‰é”šç‚¹ (Anchor)

é¦–å…ˆæ˜¯ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨ï¼Œä»£ç çœ‹èµ·æ¥å’Œ Faster-RCNN ä½¿ç”¨çš„å·®ä¸å¤šï¼š

``` python
IMAGE_SIZE = (256, 256) # ç¼©æ”¾å›¾ç‰‡çš„å¤§å°

Anchors = None # é”šç‚¹åˆ—è¡¨ï¼ŒåŒ…å« é”šç‚¹æ•°é‡ * å½¢çŠ¶æ•°é‡ çš„èŒƒå›´
AnchorSpans = (16, 32, 64) # å°ºåº¦åˆ—è¡¨ï¼Œå€¼ä¸ºé”šç‚¹ä¹‹é—´çš„è·ç¦»
AnchorAspects = ((1, 1), (1, 2), (2, 1)) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨

def generate_anchors():
    """æ ¹æ®é”šç‚¹å’Œå½¢çŠ¶ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨"""
    w, h = IMAGE_SIZE
    anchors = []
    for span in AnchorSpans:
        for x in range(0, w, span):
            for y in range(0, h, span):
                xcenter, ycenter = x + span / 2, y + span / 2
                for ratio in AnchorAspects:
                    ww = span * ratio[0]
                    hh = span * ratio[1]
                    xx = xcenter - ww / 2
                    yy = ycenter - hh / 2
                    xx = max(int(xx), 0)
                    yy = max(int(yy), 0)
                    ww = min(int(ww), w - xx)
                    hh = min(int(hh), h - yy)
                    anchors.append((xx, yy, ww, hh))
    return anchors

Anchors = generate_anchors()
```
ä½† YOLO éœ€è¦åˆ†åˆ«å¤„ç†æ¯ä¸ªå°ºåº¦ï¼Œæ‰€ä»¥ç”Ÿæˆçš„é”šç‚¹èŒƒå›´åˆ—è¡¨ä¼šé¦–å…ˆæŒ‰å°ºåº¦æ’åºï¼Œç”Ÿæˆå‡ºæ¥çš„ç»“æ„å¦‚ä¸‹ï¼š

``` text
[
    å°ºåº¦1åŒºåŸŸ1å½¢çŠ¶1çš„èŒƒå›´,
    å°ºåº¦1åŒºåŸŸ1å½¢çŠ¶2çš„èŒƒå›´,
    å°ºåº¦1åŒºåŸŸ1å½¢çŠ¶3çš„èŒƒå›´,
    å°ºåº¦1åŒºåŸŸ2å½¢çŠ¶1çš„èŒƒå›´,
    å°ºåº¦1åŒºåŸŸ2å½¢çŠ¶2çš„èŒƒå›´,
    å°ºåº¦1åŒºåŸŸ2å½¢çŠ¶3çš„èŒƒå›´,
    ...
    å°ºåº¦2åŒºåŸŸ1å½¢çŠ¶1çš„èŒƒå›´,
    å°ºåº¦2åŒºåŸŸ1å½¢çŠ¶2çš„èŒƒå›´,
    å°ºåº¦2åŒºåŸŸ1å½¢çŠ¶3çš„èŒƒå›´,
    ...
    å°ºåº¦3åŒºåŸŸ1å½¢çŠ¶1çš„èŒƒå›´,
    å°ºåº¦3åŒºåŸŸ1å½¢çŠ¶2çš„èŒƒå›´,
    å°ºåº¦3åŒºåŸŸ1å½¢çŠ¶3çš„èŒƒå›´,
    ...
]
```

æœ€ç»ˆä¼šåŒ…å« `(256/16)^2*3 + (256/32)^2*3 + (256/64)^2*3 = 768 + 192 + 48 = 1008` ä¸ªé”šç‚¹èŒƒå›´ã€‚

è¿™ç¯‡æ–‡ç« ä¼šç”¨ YOLO æ¨¡å‹å®ç°è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦å¸¦å£ç½©ï¼Œè€Œäººè„¸çš„å½¢çŠ¶é€šå¸¸æ¥è¿‘ 1:1ï¼Œæ‰€ä»¥ä¸‹é¢çš„ä»£ç ä¼šä½¿ç”¨ä»¥ä¸‹çš„å‚æ•°ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨ï¼š

``` python
AnchorSpans = (16, 32, 64) # å°ºåº¦åˆ—è¡¨ï¼Œå€¼ä¸ºé”šç‚¹ä¹‹é—´çš„è·ç¦»
AnchorAspects = ((1, 1), (1.5, 1.5)) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
```

å¦‚æœä½ æƒ³ç”¨æ¥æ£€æµ‹å…¶ä»–ç‰©ä½“ï¼Œå¯ä»¥ä¿®æ”¹å‚æ•°ä½¿å¾—é”šç‚¹èŒƒå›´çš„å½¢çŠ¶æ›´åŒ¹é…ç‰©ä½“å½¢çŠ¶ï¼Œä»¥æå‡æ£€æµ‹ç‡ã€‚

### è°ƒæ•´åŒºåŸŸèŒƒå›´çš„ç®—æ³•

åœ¨æœ‰äº†é”šç‚¹èŒƒå›´ä¹‹åï¼Œæˆ‘ä»¬è¿˜éœ€è¦å†³å®šä¸€ä¸ªæŠŠé”šç‚¹èŒƒå›´è°ƒæ•´åˆ°ç‰©ä½“èŒƒå›´çš„ç®—æ³•ï¼Œä¸€å…±éœ€è¦å››ä¸ªå‚æ•°ï¼Œè®¡ç®—è§„åˆ™å¦‚ä¸‹ï¼š

- åŒºåŸŸåç§» x: ç‰©ä½“çš„ä¸­å¿ƒç‚¹åœ¨é”šç‚¹èŒƒå›´ä¸­çš„ x è½´ä½ç½®ï¼Œ0~1 ä¹‹é—´
- åŒºåŸŸåç§» y: ç‰©ä½“çš„ä¸­å¿ƒç‚¹åœ¨é”šç‚¹èŒƒå›´ä¸­çš„ y è½´ä½ç½®ï¼Œ0~1 ä¹‹é—´
- åŒºåŸŸåç§» w: log(ç‰©ä½“çš„é•¿åº¦ä¸é”šç‚¹èŒƒå›´é•¿åº¦çš„æ¯”ä¾‹)
- åŒºåŸŸåç§» h: log(ç‰©ä½“çš„é«˜åº¦ä¸é”šç‚¹èŒƒå›´é«˜åº¦çš„æ¯”ä¾‹)

![10](./10.png)

çœ‹èµ·æ¥æ¯”è¾ƒç®€å•å§ğŸ˜ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯è¿™æ ·è°ƒæ•´å‡ºæ¥çš„ç‰©ä½“èŒƒå›´ä¸­å¿ƒç‚¹ä¸€å®šä¼šåœ¨é”šç‚¹èŒƒå›´ä¸­ï¼Œè¿™ç‚¹è·Ÿ Faster-RCNN ä½¿ç”¨çš„ç®—æ³•ä¸ä¸€æ ·ã€‚

ä»¥ä¸‹æ˜¯è®¡ç®—ä½¿ç”¨çš„ä»£ç ï¼Œæ³¨é‡Šä¸­çš„ "å®é™…åŒºåŸŸ" ä»£è¡¨ç‰©ä½“èŒƒå›´ï¼Œ"å€™é€‰åŒºåŸŸ" ä»£è¡¨é”šç‚¹èŒƒå›´ã€‚

``` python
def calc_box_offset(candidate_box, true_box):
    """è®¡ç®—å€™é€‰åŒºåŸŸä¸å®é™…åŒºåŸŸçš„åç§»å€¼ï¼Œè¦æ±‚å®é™…åŒºåŸŸçš„ä¸­å¿ƒç‚¹å¿…é¡»åœ¨å€™é€‰åŒºåŸŸä¸­"""
    # è®¡ç®—å®é™…åŒºåŸŸçš„ä¸­å¿ƒç‚¹åœ¨å€™é€‰åŒºåŸŸä¸­çš„ä½ç½®ï¼ŒèŒƒå›´ä¼šåœ¨ 0 ~ 1 ä¹‹é—´
    x1, y1, w1, h1 = candidate_box
    x2, y2, w2, h2 = true_box
    x_offset = ((x2 + w2 // 2) - x1) / w1
    y_offset = ((y2 + h2 // 2) - y1) / h1
    # è®¡ç®—å®é™…åŒºåŸŸé•¿å®½ç›¸å¯¹äºå€™é€‰åŒºåŸŸé•¿å®½çš„æ¯”ä¾‹ï¼Œä½¿ç”¨ log å‡å°‘è¿‡å¤§çš„å€¼
    w_offset = math.log(w2 / w1)
    h_offset = math.log(h2 / h1)
    return (x_offset, y_offset, w_offset, h_offset)

def adjust_box_by_offset(candidate_box, offset):
    """æ ¹æ®åç§»å€¼è°ƒæ•´å€™é€‰åŒºåŸŸ"""
    x1, y1, w1, h1 = candidate_box
    x_offset, y_offset, w_offset, h_offset = offset
    w2 = math.exp(w_offset) * w1
    h2 = math.exp(h_offset) * h1
    x2 = x1 + w1 * x_offset - w2 // 2
    y2 = y1 + h1 * y_offset - h2 // 2
    x2 = min(IMAGE_SIZE[0]-1,  x2)
    y2 = min(IMAGE_SIZE[1]-1,  y2)
    w2 = min(IMAGE_SIZE[0]-x2, w2)
    h2 = min(IMAGE_SIZE[1]-y2, h2)
    return (x2, y2, w2, h2)
```

### ç”Ÿæˆç”¨äºè®­ç»ƒçš„å®é™…è¾“å‡º

å†³å®šäº†é”šç‚¹ä¸è°ƒæ•´åŒºåŸŸèŒƒå›´çš„ç®—æ³•ä»¥åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†ç”Ÿæˆå®é™…çš„è¾“å‡ºç»“æœï¼Œè®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†éœ€è¦åŒ…å«ï¼š

- å›¾ç‰‡
- åŒ…å«çš„å¯¹è±¡ï¼Œå¯ä»¥æœ‰å¤šä¸ª
    - å¯¹è±¡çš„èŒƒå›´
    - å¯¹è±¡çš„åˆ†ç±»

æ•°æ®é›†å‡†å¤‡å¥½ä»¥åï¼Œæˆ‘ä»¬æ¯”å¯¹é”šç‚¹èŒƒå›´åˆ—è¡¨ä¸æ•°æ®é›†ä¸­å¯¹è±¡çš„èŒƒå›´ï¼Œç„¶åé’ˆå¯¹æ¯å¼ å›¾ç‰‡çš„æ¯ä¸ªé”šç‚¹èŒƒå›´ç”Ÿæˆä»¥ä¸‹æ•°æ®ï¼š

- æ˜¯å¦å¯¹è±¡
- åŒºåŸŸåç§» x
- åŒºåŸŸåç§» y
- åŒºåŸŸåç§» w
- åŒºåŸŸåç§» h
- åˆ†ç±» 1 çš„å¯èƒ½æ€§
- åˆ†ç±» 2 çš„å¯èƒ½æ€§
- åˆ†ç±» 3 çš„å¯èƒ½æ€§

æ˜¯å¦å¯¹è±¡åªæœ‰ 0 æˆ– 1 ä¸¤ä¸ªå€¼ï¼Œå¦‚æœé”šç‚¹èŒƒå›´åŒ…å«å¯¹è±¡ä¸­å¿ƒå¹¶ä¸”é”šç‚¹èŒƒå›´ä¸å¯¹è±¡èŒƒå›´çš„é‡å ç‡ (IOU) å¤§äºé˜ˆå€¼ (ä¾‹å¦‚ 30%)ï¼Œåˆ™ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚æ³¨æ„å¦‚æœæ˜¯å¦å¯¹è±¡ä¸º 0ï¼Œé‚£ä¹ˆåé¢çš„åŒºåŸŸåç§»å’Œå„ä¸ªåˆ†ç±»çš„å¯èƒ½æ€§ä¸éœ€è¦è®¡ç®— (ä¾‹å¦‚è®¾ç½®ä¸º 0)ï¼Œè®¡ç®—æŸå¤±çš„æ—¶å€™ä¹Ÿä¼šé™¤æ‰å®ƒä»¬ã€‚

å››ä¸ªåŒºåŸŸåç§»ä¼šæ ¹æ®é”šç‚¹èŒƒå›´ä¸å¯¹è±¡èŒƒå›´è®¡ç®—ï¼Œç®—æ³•å‚è€ƒä¸Šé¢çš„è¯´æ˜ã€‚

å„ä¸ªåˆ†ç±»çš„å¯èƒ½æ€§æŒ‰å¯¹è±¡çš„åˆ†ç±»è®¡ç®—ï¼Œå¦‚æœå¯¹è±¡çš„åˆ†ç±»ä¸º "äºº"ï¼Œè€Œä¸‰ä¸ªåˆ†ç±»åˆ†åˆ«ä¸º "äºº çŒ« ç‹—" é‚£ä¹ˆåˆ†ç±» 1 çš„å¯èƒ½æ€§ä¸º 1ï¼Œåˆ†ç±» 2 ä¸åˆ†ç±» 3 çš„å¯èƒ½æ€§ä¸º 0ã€‚æ­¤å¤– YOLO è¿˜æ”¯æŒå¤šåˆ†ç±» (è¦æ±‚è®¡ç®—æŸå¤±çš„æ—¶å€™ç”¨ BinaryCrossEntropy)ï¼Œå¦‚æœåˆ†ç±»ä¸º "äºº ç”·äºº å¥³äºº çŒª å…¬çŒª æ¯çŒª" å¹¶ä¸”å¯¹è±¡æ˜¯ "æ¯çŒª" æ—¶ï¼Œé‚£ä¹ˆå„ä¸ªåˆ†ç±»çš„å¯èƒ½æ€§å°±æ˜¯ "0 0 0 1 0 1"ã€‚éœ€è¦æ³¨æ„è¿™é‡Œè®¡ç®—å‡ºæ¥çš„å€¼æ˜¯ä¾›æ¨¡å‹å­¦ä¹ çš„ï¼Œæ¨¡å‹å­¦ä¹ å®Œä»¥åå¯èƒ½ä¼šè¾“å‡º "0.9 0.2 0.0" è¿™æ ·çš„æµ®ç‚¹æ•°ï¼Œéœ€è¦åˆ¤æ–­æœ€å¤§çš„å€¼æ‰¾å‡ºæœ€å¯èƒ½çš„åˆ†ç±»ï¼Œå¹¶ä¸”æ ¹æ®å€¼çš„å¤§å°åˆ¤æ–­æ¨¡å‹å¯¹ç»“æœæœ‰å¤šå°‘æŠŠæ¡ã€‚

å¦‚æœä½ è®°å¾—å‰ä¸€ç¯‡ä»‹ç» Faster-RCNN æ¨¡å‹çš„æ–‡ç« ï¼Œåº”è¯¥ä¼šæƒ³åˆ°æœ‰ä¸€ä¸ªè¡¨ç¤º "éå¯¹è±¡" çš„åˆ†ç±»ï¼ŒFaster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œé¦–å…ˆä¼šåˆ¤æ–­ä¸€æ¬¡æ˜¯å¦å¯¹è±¡ï¼Œä¹‹åçš„æ ‡ç­¾åˆ†ç±»ç½‘ç»œä¼šå†æ¬¡å»æ‰å½’ä¸ºéå¯¹è±¡åˆ†ç±»çš„ç»“æœï¼Œè¿™æ ·çš„åšæ³•è®©è¯†åˆ«çš„ç²¾åº¦æå‡äº†å¾ˆå¤šã€‚ç„¶è€Œ YOLO æ¨¡å‹åªæœ‰å•æ­¥ï¼ŒåŸåˆ™ä¸Šæ˜¯ä¸éœ€è¦éå¯¹è±¡åˆ†ç±»çš„ï¼Œå³ä½¿åŠ ä¸Šéå¯¹è±¡åˆ†ç±»ä¹Ÿä¸ä¼šæå‡åˆ¤æ–­ "æ˜¯å¦å¯¹è±¡" çš„ç²¾åº¦ã€‚ä½†å¦‚æœæ•°æ®é‡ä¸è¶³ï¼Œæ·»åŠ éå¯¹è±¡åˆ†ç±»å¯ä»¥å¸®åŠ©æ›´å¥½çš„è¯†åˆ«åˆ†ç±»ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œä¾‹å¦‚å›¾ç‰‡ä¸­æœ‰æ£•è‰²çš„çŒ«å’Œçº¢è‰²çš„çŒªï¼Œæ¨¡å‹å¯èƒ½ä¼šåˆ¤æ–­æ£•è‰²çš„éƒ½æ˜¯çŒ«ï¼Œçº¢è‰²çš„éƒ½æ˜¯çŒªï¼Œä½†æ·»åŠ éå¯¹è±¡åˆ†ç±»ä»¥åï¼Œå¦‚æœå›¾ç‰‡è¿˜åŒ…å«æ£•è‰²çš„å‡³å­å’Œçº¢è‰²çš„ç”µé¥­é”…ï¼Œé‚£ä¹ˆæ¨¡å‹å°±ä¸ä¼šåªæ ¹æ®é¢œè‰²æ¥åˆ¤æ–­ã€‚å› æ­¤ï¼Œä¸‹é¢è¯†åˆ«äººè„¸ä½ç½®çš„ä¾‹å­ä¼šæ·»åŠ éå¯¹è±¡åˆ†ç±»ã€‚

å…·ä½“çš„ä»£ç å‚è€ƒåé¢çš„ prepare å‡½æ•°å§ğŸ¤’ã€‚

### è®¡ç®—ç‰¹å¾

åŸå§‹çš„ YOLO æ¨¡å‹è®¡ç®—ç‰¹å¾ä½¿ç”¨çš„æ˜¯å«åš Darknet çš„ç½‘ç»œï¼Œè¿™ä¸ªç½‘ç»œæ˜¯ YOLO ä½œè€…ç”¨ C è¯­è¨€å®ç°çš„ï¼Œç®—æ˜¯ YOLO ä½œè€…å¯¹è‡ªå·±å†™çš„æ¡†æ¶çš„å®£ä¼ å§ğŸ˜¤ã€‚ä¸è¿‡åªè¦ç†è§£ YOLO æ¨¡å‹çš„åŸç†ï¼Œç”¨å…¶ä»–ç½‘ç»œä¹Ÿå¯ä»¥å®ç°å·®ä¸å¤šçš„æ•ˆæœ (è™½ç„¶ä½œè€…ä¸ºäº†åˆ·åˆ†åšå‡ºäº†å¾ˆå¤šè°ƒæ•´ï¼Œåªæ˜¯å¥—ç”¨å…¶ä»–ç½‘ç»œçš„è¯æ­£ç¡®åº¦è¿½ä¸ä¸Š)ï¼Œè¿™é‡Œæˆ‘ç”¨äº†ç›®å‰ç”¨çš„æœ€å¹¿æ³›çš„ Resnet æ¨¡å‹ï¼Œä»£ç å¦‚ä¸‹ï¼š

``` python
self.previous_channels_out = 4
self.resnet_models = nn.ModuleList([
    nn.Sequential(
        nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(self.previous_channels_out),
        nn.ReLU(inplace=True),
        self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=1),
        self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
        self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
        self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2),
        self._make_layer(BasicBlock, channels_out=256, num_blocks=2, stride=2)),
    self._make_layer(BasicBlock, channels_out=256, num_blocks=2, stride=2),
    self._make_layer(BasicBlock, channels_out=256, num_blocks=2, stride=2)
])
```

`_make_layer` ä¸ `BasicBlock` çš„ä»£ç å’Œä¹‹å‰æ–‡ç« ç»™å‡ºçš„ä¸€æ ·ï¼Œä½ ä¹Ÿå¯ä»¥å‚è€ƒä¸‹é¢çš„å®Œæ•´ä»£ç ã€‚

è¿™é‡Œå®šä¹‰çš„ `resnet_models` åŒ…å«äº†ä¸‰ä¸ªå­æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªæ¨¡å‹ä¼šè¾“å‡ºç»´åº¦ä¸º `æ‰¹æ¬¡å¤§å°,256,å›¾ç‰‡å®½åº¦/16,å›¾ç‰‡é«˜åº¦/16` çš„ç»“æœï¼Œç¬¬äºŒä¸ªæ¨¡å‹ä¼šæ¥æ”¶ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç»“æœç„¶åè¾“å‡ºç»´åº¦ä¸º `æ‰¹æ¬¡å¤§å°,256,å›¾ç‰‡å®½åº¦/32,å›¾ç‰‡é«˜åº¦/32` çš„ç»“æœï¼Œç¬¬ä¸‰ä¸ªæ¨¡å‹ä¼šæ¥æ”¶ç¬¬äºŒä¸ªæ¨¡å‹çš„ç»“æœç„¶åè¾“å‡ºç»´åº¦ä¸º `æ‰¹æ¬¡å¤§å°,256,å›¾ç‰‡å®½åº¦/64,å›¾ç‰‡é«˜åº¦/64` çš„ç»“æœã€‚è¿™ä¸‰ä¸ªç»“æœåˆ†åˆ«ä»£è¡¨æŠŠå›¾ç‰‡åˆ†å‰²ä¸º `16x16`ï¼Œ`32x32`ï¼Œ`64x64` ä¸ªåŒºåŸŸä»¥åï¼Œå„ä¸ªåŒºåŸŸå¯¹åº”çš„ç‰¹å¾ã€‚

è¾“å‡ºä¸‰ä¸ªç‰¹å¾çš„ä½¿ç”¨çš„ä»£ç å¦‚ä¸‹ï¼š

``` python
def forward(self, x):
    features_list = []
    resnet_input = x
    for m in self.resnet_models:
        resnet_input = m(resnet_input)
        features_list.append(resnet_input)
```

### æ ¹æ®ç‰¹å¾é¢„æµ‹è¾“å‡º

ä¸Šä¸€æ­¥æˆ‘ä»¬å¾—å‡ºäº†ä¸‰ä¸ªç‰¹å¾ï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥æ ¹æ®è¿™ä¸‰ä¸ªç‰¹å¾é¢„æµ‹ä¸‰ä¸ªå°ºåº¦ä¸­çš„å„ä¸ªåŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ä¸å¯¹è±¡çš„åˆ†ç±»äº†ã€‚æµç¨‹å’Œä¸Šé¢ä»‹ç»çš„ä¸€æ ·ï¼Œéœ€è¦åˆ†æˆä¸‰æ­¥ï¼š

- è¿›ä¸€æ­¥å¤„ç†ç‰¹å¾ (é•¿å®½ä¸å˜)
- æ‰©å¤§ç‰¹å¾é•¿å®½ï¼Œå¹¶ä¸”åˆå¹¶åˆ°ä¸‹ä¸€ä¸ªå°ºåº¦ (æ›´ç»†çš„å°ºåº¦) çš„ç‰¹å¾
- åˆ¤æ–­æ˜¯å¦å¯¹è±¡ä¸­å¿ƒä¸æ ‡ç­¾åˆ†ç±»

æ¨¡å‹ä»£ç ï¼š

``` python
self.yolo_detectors = nn.ModuleList([
    # è¿›ä¸€æ­¥å¤„ç†ç‰¹å¾
    nn.ModuleList([nn.Sequential(
        nn.Conv2d(256 if index == 0 else 512, 256, kernel_size=1, stride=1, padding=0, bias=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0, bias=True),
        nn.ReLU(inplace=True)),
    # æ‰©å¤§ç‰¹å¾é•¿å®½
    nn.Upsample(scale_factor=2, mode="nearest"),
    # åˆ¤æ–­æ˜¯å¦å¯¹è±¡ä¸­å¿ƒä¸æ ‡ç­¾åˆ†ç±»
    nn.Sequential(
        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, bias=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, MyModel.AnchorTotalOutputs, kernel_size=1, stride=1, padding=0, bias=True))])
    for index in range(len(self.resnet_models))
])
```

"åˆ¤æ–­æ˜¯å¦å¯¹è±¡ä¸­å¿ƒä¸æ ‡ç­¾åˆ†ç±»" çš„éƒ¨åˆ†å¯ä»¥ç”¨ CNN æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨çº¿æ€§æ¨¡å‹ï¼Œå¤šä¸ªä¸æ”¹å˜é•¿å®½çš„å·ç§¯å±‚ç»„åˆèµ·æ¥å¯ä»¥åšåˆ°ä¸å¤šå±‚çº¿æ€§æ¨¡å‹æ¥è¿‘çš„æ•ˆæœã€‚å¦‚æœç”¨ CNN æ¨¡å‹å¯ä»¥æŠŠç»´åº¦æ˜¯ `(B, C, W, H)` çš„è¾“å…¥è½¬æ¢åˆ°ç»´åº¦æ˜¯ `(B, O, W, H)` çš„ç»“æœï¼Œå¦‚æœç”¨çº¿æ€§æ¨¡å‹åˆ™éœ€è¦å…ˆæŠŠè¾“å…¥å˜å½¢åˆ° `(B*W*H, C)` ç„¶åå†é€šè¿‡çº¿æ€§æ¨¡å‹è½¬æ¢åˆ°ç»´åº¦æ˜¯ `(B*W*H, O)` çš„ç»“æœï¼Œå†å˜å½¢åˆ° `(B, O, W, H)`ã€‚å‰ä¸€ç¯‡æ–‡ç« ä»‹ç»çš„ Faster-RCNN å®ç°ç”¨äº†çº¿æ€§æ¨¡å‹ï¼Œè€Œè¿™ç¯‡ä½¿ç”¨ CNN æ¨¡å‹ï¼ŒåŸåˆ™ä¸Šç”¨å“ªç§éƒ½å¯ä»¥ğŸ¤’ã€‚

å¤„ç†ç‰¹å¾çš„ä»£ç ï¼š

``` python
previous_upsampled_feature = None
outputs = []
for index, feature in enumerate(reversed(features_list)):
    if previous_upsampled_feature is not None:
        # åˆå¹¶å¤§çš„é”šç‚¹è·ç¦»æŠ½å–çš„ç‰¹å¾åˆ°å°çš„é”šç‚¹è·ç¦»æŠ½å–çš„ç‰¹å¾
        feature = torch.cat((feature, previous_upsampled_feature), dim=1)
    # è®¡ç®—ç”¨äºåˆå¹¶çš„ç‰¹å¾
    hidden = self.yolo_detectors[index][0](feature)
    # æ”¾å¤§ç‰¹å¾ (ç”¨äºä¸‹ä¸€æ¬¡å¤„ç†æ—¶åˆå¹¶)
    upsampled = self.yolo_detectors[index][1](hidden)
    # è®¡ç®—æœ€ç»ˆçš„é¢„æµ‹è¾“å‡º
    output = self.yolo_detectors[index][2](hidden)
    previous_upsampled_feature = upsampled
    outputs.append(output)
```

ä¹‹å `outputs` ä¼šåŒ…å«ä¸‰ä¸ªç»“æœï¼Œç»´åº¦æ˜¯ `(æ‰¹æ¬¡å¤§å°, (5+åˆ†ç±»æ•°é‡)*å½¢çŠ¶æ•°é‡, å°ºåº¦å¯¹åº”çš„å®½åº¦, å°ºåº¦å¯¹åº”çš„é«˜åº¦)`ï¼ŒæŠŠè¿™ä¸‰ä¸ªç»“æœè¿èµ·æ¥æ•°é‡ä¼šåˆšå¥½ç­‰äºä¹‹å‰ç”Ÿæˆçš„é”šç‚¹æ•°é‡ã€‚è¿æ¥ä¸‰ä¸ªç»“æœçš„ä»£ç å¦‚ä¸‹ï¼Œæ³¨æ„é¡ºåºéœ€è¦ä¸ç”Ÿæˆé”šç‚¹æ—¶ä½¿ç”¨çš„é¡ºåºä¸€æ ·ï¼Œè¿™æ ·è¿æ¥åçš„ç»“æœå’Œé”šç‚¹èŒƒå›´å°±å¯ä»¥æœ‰ä¸€å¯¹ä¸€çš„å…³ç³»ã€‚

``` python
outputs_flatten = []
# å‰é¢å¤„ç†ç‰¹å¾çš„æ—¶å€™ç”¨äº† reversedï¼Œè¿™é‡Œéœ€è¦å†æ¬¡ç”¨ reversed æŠŠé¡ºåºè°ƒæ¢å›æ¥
# è°ƒæ¢ä»¥åçš„ä¸‰ä¸ªç»“æœé¡ºåºåº”è¯¥ä¸ AnchorSpans ä¸€è‡´
for output in reversed(outputs):
    # å˜å½¢åˆ° (æ‰¹æ¬¡å¤§å°, å°ºåº¦å¯¹åº”çš„å®½åº¦, å°ºåº¦å¯¹åº”çš„é«˜åº¦, (5+åˆ†ç±»æ•°é‡)*å½¢çŠ¶æ•°é‡)
    output = output.permute(0, 2, 3, 1)
    # å˜å½¢åˆ° (æ‰¹æ¬¡å¤§å°, å®½åº¦*é«˜åº¦*å½¢çŠ¶æ•°é‡, 5+åˆ†ç±»æ•°é‡)
    # ç”Ÿæˆé”šç‚¹æ—¶ä½¿ç”¨çš„é¡ºåºæ˜¯ å®½åº¦ => é«˜åº¦ => å½¢çŠ¶
    output = output.reshape(output.shape[0], -1, MyModel.AnchorOutputs)
    outputs_flatten.append(output)
# è¿æ¥ä»¥åç»´åº¦æ˜¯ (æ‰¹æ¬¡å¤§å°, å°ºåº¦æ•°é‡*å®½åº¦*é«˜åº¦*å½¢çŠ¶æ•°é‡, 5+åˆ†ç±»æ•°é‡)
# å³ (æ‰¹æ¬¡å¤§å°, é”šç‚¹æ•°é‡, 5+åˆ†ç±»æ•°é‡)
outputs_all = torch.cat(outputs_flatten, dim=1)
```

åœ¨è¿”å› outputs_all ä¹‹å‰ï¼Œè¿˜éœ€è¦ç”¨ sigmoid æ¥è®©æ˜¯å¦å¯¹è±¡ä¸­å¿ƒä¸å„ä¸ªåˆ†ç±»çš„å¯èƒ½æ€§å¯¹åº”çš„å€¼è½åœ¨ 0 ~ 1 ä¹‹é—´ã€‚æ³¨æ„éƒ¨åˆ† YOLO çš„å®ç°ä¼šç”¨ sigmoid æ¥å¤„ç†åŒºåŸŸåç§» x å’ŒåŒºåŸŸåç§» yï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå€¼ä¹Ÿåº”è¯¥è½åœ¨ 0 ~ 1 ä¹‹é—´ï¼Œä½†æˆ‘ä¸ªäººè®¤ä¸º sigmoid åªé€‚åˆå¤„ç†é¢„æœŸç»“æœæ˜¯äºŒè¿›åˆ¶ (0 æˆ– 1) çš„å€¼ï¼Œè€ŒåŒºåŸŸåç§»é¢„æœŸç»“æœå¹³å‡åˆ†å¸ƒåœ¨ 0 ~ 1 ä¹‹é—´ï¼Œä¸èƒ½èµ·åˆ°å½’å¹¶çš„ä½œç”¨ï¼Œæ•ˆæœä¼šè·Ÿ hardtanh å·®ä¸å¤šã€‚

``` python
# æ˜¯å¦å¯¹è±¡ä¸­å¿ƒåº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´ï¼Œä½¿ç”¨ sigmoid å¤„ç†
outputs_all[:,:,:1] = self.sigmoid(outputs_all[:,:,:1])
# åˆ†ç±»åº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´ï¼Œä½¿ç”¨ sigmoid å¤„ç†
outputs_all[:,:,5:] = self.sigmoid(outputs_all[:,:,5:])
```

å¤„ç†å®Œä»¥åï¼Œ`outputs_all` å°±æ˜¯ YOLO æ¨¡å‹è¿”å›çš„ç»“æœäº†ï¼Œå®ƒåœ¨è®­ç»ƒçš„æ—¶å€™ä¼šç”¨äºè®¡ç®—æŸå¤±å¹¶è°ƒæ•´å‚æ•°ï¼Œåœ¨å®é™…é¢„æµ‹çš„æ—¶å€™ä¼šé…åˆä¹‹å‰ç”Ÿæˆçš„é”šç‚¹åˆ—è¡¨å¾—å‡ºåŒ…å«å¯¹è±¡çš„åŒºåŸŸä¸å¯¹è±¡åˆ†ç±»ï¼Œå¹¶æ ‡è®°åˆ°å›¾ç‰‡æˆ–è€…è§†é¢‘ä¸Šã€‚

### è®¡ç®—æŸå¤±

åˆåˆ°è®¡ç®—æŸå¤±çš„æ—¶é—´äº†ğŸ˜©ï¼ŒYOLO çš„é¢„æµ‹è¾“å‡ºå’Œå®é™…è¾“å‡ºç»´åº¦æ˜¯ä¸€æ ·çš„ï¼Œä½†æˆ‘ä»¬ä¸èƒ½åªç”¨ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥è®¡ç®—å®ƒä»¬ï¼ŒYOLO åŒæ ·éœ€è¦è®¡ç®—å¤šä¸ªæŸå¤±å¹¶åˆå¹¶å®ƒä»¬ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦åŒºåˆ†æ­£æ ·æœ¬ (åŒ…å«å¯¹è±¡ä¸­å¿ƒçš„åŒºåŸŸ) å’Œè´Ÿæ ·æœ¬ (ä¸åŒ…å«å¯¹è±¡ä¸­å¿ƒçš„åŒºåŸŸ)ï¼Œæ–¹æ³•åœ¨å‰é¢ä¹Ÿæåˆ°è¿‡äº†ï¼š

- æ­£æ ·æœ¬ï¼šåŒ…å«å¯¹è±¡ä¸­å¿ƒå¹¶ä¸”é‡å ç‡å¤§äºæŸä¸ªé˜ˆå€¼
- è´Ÿæ ·æœ¬ï¼šä¸åŒ…å«å¯¹è±¡ä¸­å¿ƒå¹¶ä¸”ä¸ä»»æ„å¯¹è±¡çš„é‡å ç‡å‡å°äºæŸä¸ªé˜ˆå€¼

è´Ÿæ ·æœ¬è¦æ±‚é‡å ç‡ä½äºé˜ˆå€¼æ˜¯ä¸ºäº†ç…§é¡¾å¯¹è±¡ä¸­å¿ƒéå¸¸æ¥è¿‘åŒºåŸŸè¾¹ç¼˜çš„å¯¹è±¡ï¼Œè¿™æ—¶æ¨¡å‹å¾ˆéš¾åˆ¤æ–­å¯¹è±¡ä¸­å¿ƒå…·ä½“åœ¨å“ªä¸ªåŒºåŸŸï¼ŒæŠŠè¿™äº›æ ·æœ¬ä»è´Ÿæ ·æœ¬ä¸­æ’é™¤æ‰å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å®¹æ˜“çš„å­¦ä¹ ï¼Œæœ€ç»ˆæ¨¡å‹å¯ä»¥åˆ¤æ–­å¯¹è±¡ä¸­å¿ƒåœ¨ç›¸é‚»çš„ä¸¤ä¸ªåŒºåŸŸä½†ä¸ä¼šè¢«è°ƒæ•´ã€‚

YOLO æ¨¡å‹ä¼šè®¡ç®—ä¸åˆå¹¶ä»¥ä¸‹çš„æŸå¤±ï¼š

- æ­£æ ·æœ¬çš„æ˜¯å¦å¯¹è±¡ä¸­å¿ƒï¼Œä½¿ç”¨ MSELoss
- è´Ÿæ ·æœ¬çš„æ˜¯å¦å¯¹è±¡ä¸­å¿ƒ * 0.5ï¼Œä½¿ç”¨ MSELoss
    - å› ä¸ºå¤§éƒ¨åˆ†åŒºåŸŸä¸åŒ…å«å¯¹è±¡ä¸­å¿ƒï¼Œè¿™é‡Œä¹˜ä»¥ 0.5 ä»¥å‡å°‘è´Ÿæ ·æœ¬çš„æŸå¤±å¯¹è°ƒæ•´å‚æ•°çš„å½±å“
- æ­£æ ·æœ¬çš„åŒºåŸŸåç§»ï¼Œä½¿ç”¨ MSELoss
    - éæ­£æ ·æœ¬çš„åŒºåŸŸåç§»ä¼šè¢«å¿½ç•¥ï¼Œè®¡ç®—èµ·æ¥æ²¡æ„ä¹‰
- æ­£æ ·æœ¬çš„æ ‡ç­¾åˆ†ç±»æŸå¤±ï¼Œä½¿ç”¨ BCELoss
    - BinaryCrossEntropy æŸå¤±å‡½æ•°æ”¯æŒå¤šåˆ†ç±»ï¼Œè™½ç„¶æœ¬ç¯‡çš„ä¾‹å­åªæœ‰å•åˆ†ç±»
- å¦‚æœæœ‰éå¯¹è±¡åˆ†ç±»ï¼Œåˆ™è®¡ç®—è´Ÿæ ·æœ¬çš„æ ‡ç­¾åˆ†ç±»æŸå¤±ï¼Œä½¿ç”¨ BCELoss
    - å¦‚æœä¸ä½¿ç”¨éå¯¹è±¡åˆ†ç±»ï¼Œåˆ™ä¸éœ€è¦è®¡ç®—

å…·ä½“è®¡ç®—ä»£ç å¦‚ä¸‹ï¼š

``` python
def loss_function(predicted, actual):
    """YOLO ä½¿ç”¨çš„å¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨"""
    result_tensor, result_isobject_masks, result_nonobject_masks = actual
    objectness_losses = []
    offsets_losses = []
    labels_losses = []
    for x in range(result_tensor.shape[0]):
        mask_positive = result_isobject_masks[x]
        mask_negative = result_nonobject_masks[x]
        # è®¡ç®—æ˜¯å¦å¯¹è±¡ä¸­å¿ƒçš„æŸå¤±ï¼Œåˆ†åˆ«é’ˆå¯¹æ­£è´Ÿæ ·æœ¬è®¡ç®—
        # å› ä¸ºå¤§éƒ¨åˆ†åŒºåŸŸä¸åŒ…å«å¯¹è±¡ä¸­å¿ƒï¼Œè¿™é‡Œå‡å°‘è´Ÿæ ·æœ¬çš„æŸå¤±å¯¹è°ƒæ•´å‚æ•°çš„å½±å“
        objectness_loss_positive = nn.functional.mse_loss(
            predicted[x,mask_positive,0], result_tensor[x,mask_positive,0])
        objectness_loss_negative = nn.functional.mse_loss(
            predicted[x,mask_negative,0], result_tensor[x,mask_negative,0]) * 0.5
        objectness_losses.append(objectness_loss_positive)
        objectness_losses.append(objectness_loss_negative)
        # è®¡ç®—åŒºåŸŸåç§»çš„æŸå¤±ï¼Œåªé’ˆå¯¹æ­£æ ·æœ¬è®¡ç®—
        offsets_loss = nn.functional.mse_loss(
            predicted[x,mask_positive,1:5], result_tensor[x,mask_positive,1:5])
        offsets_losses.append(offsets_loss)
        # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æŸå¤±ï¼Œåˆ†åˆ«é’ˆå¯¹æ­£è´Ÿæ ·æœ¬è®¡ç®—
        labels_loss_positive = nn.functional.binary_cross_entropy(
            predicted[x,mask_positive,5:], result_tensor[x,mask_positive,5:])
        labels_loss_negative = nn.functional.binary_cross_entropy(
            predicted[x,mask_negative,5:], result_tensor[x,mask_negative,5:]) * 0.5
        labels_losses.append(labels_loss_positive)
        labels_losses.append(labels_loss_negative)
    loss = (
        torch.mean(torch.stack(objectness_losses)) +
        torch.mean(torch.stack(offsets_losses)) +
        torch.mean(torch.stack(labels_losses)))
    return loss
```

### åˆå¹¶ç»“æœåŒºåŸŸ

æœ€åå°±æ˜¯æŠŠ YOLO æ¨¡å‹è¿”å›çš„é¢„æµ‹ç»“æœè½¬æ¢åˆ°å…·ä½“çš„åŒºåŸŸåˆ—è¡¨äº†ï¼Œç®—æ³•æ˜¯å‰å‡ ç¯‡ä»‹ç»è¿‡çš„ NMS ç®—æ³•ï¼Œä»£ç å¦‚ä¸‹ï¼š

``` python
ObjScoreThreshold = 0.9 # è®¤ä¸ºæ˜¯å¯¹è±¡ä¸­å¿ƒæ‰€éœ€è¦çš„æœ€å°åˆ†æ•°
IOUMergeThreshold = 0.3 # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶é‡å åŒºåŸŸçš„é‡å ç‡é˜ˆå€¼

def convert_predicted_result(predicted):
    """è½¬æ¢é¢„æµ‹ç»“æœåˆ° (æ ‡ç­¾, åŒºåŸŸ, å¯¹è±¡ä¸­å¿ƒåˆ†æ•°, æ ‡ç­¾è¯†åˆ«åˆ†æ•°) çš„åˆ—è¡¨ï¼Œé‡å åŒºåŸŸä½¿ç”¨ NMS ç®—æ³•åˆå¹¶"""
    # è®°å½•é‡å çš„ç»“æœåŒºåŸŸ, ç»“æœæ˜¯ [ [(æ ‡ç­¾, åŒºåŸŸ, RPN åˆ†æ•°, æ ‡ç­¾è¯†åˆ«åˆ†æ•°)], ... ]
    final_result = []
    for anchor, tensor in zip(Anchors, predicted):
        obj_score = tensor[0].item()
        if obj_score <= ObjScoreThreshold:
            # è¦æ±‚å¯¹è±¡ä¸­å¿ƒåˆ†æ•°è¶…è¿‡ä¸€å®šå€¼
            continue
        offset = tensor[1:5].tolist()
        offset[0] = max(min(offset[0], 1), 0) # ä¸­å¿ƒç‚¹ x çš„åç§»åº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´
        offset[1] = max(min(offset[1], 1), 0) # ä¸­å¿ƒç‚¹ y çš„åç§»åº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´
        box = adjust_box_by_offset(anchor, offset)
        label_max = tensor[5:].max(dim=0)
        cls_score = label_max.values.item()
        label = label_max.indices.item()
        if label == 0:
            # è·³è¿‡éå¯¹è±¡åˆ†ç±»
            continue
        for index in range(len(final_result)):
            exists_results = final_result[index]
            if any(calc_iou(box, r[1]) > IOUMergeThreshold for r in exists_results):
                exists_results.append((label, box, obj_score, cls_score))
                break
        else:
            final_result.append([(label, box, obj_score, cls_score)])
    # åˆå¹¶é‡å çš„ç»“æœåŒºåŸŸ (ä½¿ç”¨ å¯¹è±¡ä¸­å¿ƒåˆ†æ•° * æ ‡ç­¾è¯†åˆ«åˆ†æ•° æœ€é«˜çš„åŒºåŸŸä¸ºç»“æœåŒºåŸŸ)
    for index in range(len(final_result)):
        exists_results = final_result[index]
        exists_results.sort(key=lambda r: r[2]*r[3])
        final_result[index] = exists_results[-1]
    return final_result
```

è¿™ç¯‡çš„ä¾‹å­ç”¨äº†éå¯¹è±¡åˆ†ç±»ï¼Œæ‰€ä»¥ä¼šè·³è¿‡éå¯¹è±¡åˆ†ç±»çš„åŒºåŸŸï¼Œå¦‚æœä¸ä½¿ç”¨åˆ™ä¸éœ€è¦è¿™æ ·å¤„ç†ã€‚

## YOLO æ¨¡å‹çš„è®ºæ–‡

å¦‚æœä½ æƒ³çœ‹åŸå§‹çš„ YOLO è®ºæ–‡å¯ä»¥ç‚¹ä¸‹é¢çš„é“¾æ¥ğŸ¤’ï¼Œå¾ˆéš¾å–”ï¼š

- YOLO: https://arxiv.org/pdf/1506.02640.pdf
- YOLOv2: https://arxiv.org/pdf/1612.08242.pdf (è¿™ç¯‡åŒæ—¶ä»‹ç»äº†æ€æ ·é¢„æµ‹ 9000 å¤šä¸ªåˆ†ç±»çš„æ–¹æ³•)
- YOLOv3: https://arxiv.org/pdf/1804.02767.pdf

## ä½¿ç”¨ YOLO æ¨¡å‹è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©

æ¥ä¸‹æ¥æˆ‘ä»¬ç”¨ YOLO æ¨¡å‹æŠŠæ²¡å¸¦å£ç½©çš„å®¶ä¼™æŠ“å‡ºæ¥å§ğŸ¤—ï¼Œå’Œä¸Šä¸€ç¯‡ä¸€æ ·ä¼šç”¨ä¸¤ä¸ªæ•°æ®é›†ã€‚

https://www.kaggle.com/andrewmvd/face-mask-detection

è¿™ä¸ªæ•°æ®é›†åŒ…å«äº† 853 å¼ å›¾ç‰‡ (éƒ¨åˆ†å›¾ç‰‡æ²¡æœ‰ä½¿ç”¨)ï¼Œå…¶ä¸­å„ä¸ªåˆ†ç±»çš„æ•°é‡å¦‚ä¸‹ï¼š

- æˆ´å£ç½©çš„åŒºåŸŸ (with_mask): 3232 ä¸ª
- ä¸æˆ´å£ç½©çš„åŒºåŸŸ (without_mask): 717 ä¸ª
- å¸¦äº†å£ç½©ä½†å§¿åŠ¿ä¸æ­£ç¡®çš„åŒºåŸŸ (mask_weared_incorrect): 123 ä¸ª

å› ä¸ºå¸¦äº†å£ç½©ä½†å§¿åŠ¿ä¸æ­£ç¡®çš„æ ·æœ¬æ•°é‡å¾ˆå°‘ï¼Œæ‰€ä»¥éƒ½å½’åˆ°æˆ´å£ç½©é‡Œé¢å»ğŸ˜ ã€‚

https://www.kaggle.com/vin1234/count-the-number-of-faces-present-in-an-image

è¿™ä¸ªæ•°æ®é›†ä¸€å…±æœ‰ 24533 ä¸ªåŒºåŸŸï¼Œéƒ½æ˜¯ä¸æˆ´å£ç½©çš„ã€‚

åŠ èµ·æ¥æ•°é‡å¦‚ä¸‹ï¼š

- æˆ´å£ç½©çš„åŒºåŸŸ (with_mask): 3232+123=3355 ä¸ª
- ä¸æˆ´å£ç½©çš„åŒºåŸŸ (without_mask): 717+24533 = 25250 ä¸ª

ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†è®­ç»ƒï¼Œå¹¶ä¸”è®­ç»ƒæˆåŠŸä»¥åä½¿ç”¨æ¨¡å‹è¯†åˆ«å›¾ç‰‡æˆ–è§†é¢‘çš„å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import random
import numpy
import math
import pandas
import json
from PIL import Image
from PIL import ImageDraw
from PIL import ImageFont
from torch import nn
from matplotlib import pyplot
from collections import defaultdict
from collections import deque
import xml.etree.cElementTree as ET

# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (256, 192)
# è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†è·¯å¾„
DATASET_1_IMAGE_DIR = "./archive/images"
DATASET_1_ANNOTATION_DIR = "./archive/annotations"
DATASET_2_IMAGE_DIR = "./784145_1347673_bundle_archive/train/image_data"
DATASET_2_BOX_CSV_PATH = "./784145_1347673_bundle_archive/train/bbox_train.csv"
# åˆ†ç±»åˆ—è¡¨
# YOLO åŸåˆ™ä¸Šä¸éœ€è¦ other åˆ†ç±»ï¼Œä½†å®æµ‹ä¸­æ·»åŠ è¿™ä¸ªåˆ†ç±»æœ‰åŠ©äºæå‡æ ‡ç­¾åˆ†ç±»çš„ç²¾ç¡®åº¦
CLASSES = [ "other", "with_mask", "without_mask" ]
CLASSES_MAPPING = { c: index for index, c in enumerate(CLASSES) }
# åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¯¹è±¡ä½¿ç”¨çš„åŒºåŸŸé‡å ç‡çš„é˜ˆå€¼ (å¦å¤–è¦æ±‚å¯¹è±¡ä¸­å¿ƒåœ¨åŒºåŸŸå†…)
IOU_POSITIVE_THRESHOLD = 0.30
IOU_NEGATIVE_THRESHOLD = 0.30

# ç”¨äºå¯ç”¨ GPU æ”¯æŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BasicBlock(nn.Module):
    """ResNet ä½¿ç”¨çš„åŸºç¡€å—"""
    expansion = 1 # å®šä¹‰è¿™ä¸ªå—çš„å®é™…å‡ºé€šé“æ˜¯ channels_out çš„å‡ å€ï¼Œè¿™é‡Œçš„å®ç°å›ºå®šæ˜¯ä¸€å€
    def __init__(self, channels_in, channels_out, stride):
        super().__init__()
        # ç”Ÿæˆ 3x3 çš„å·ç§¯å±‚
        # å¤„ç†é—´éš” stride = 1 æ—¶ï¼Œè¾“å‡ºçš„é•¿å®½ä¼šç­‰äºè¾“å…¥çš„é•¿å®½ï¼Œä¾‹å¦‚ (32-3+2)//1+1 == 32
        # å¤„ç†é—´éš” stride = 2 æ—¶ï¼Œè¾“å‡ºçš„é•¿å®½ä¼šç­‰äºè¾“å…¥çš„é•¿å®½çš„ä¸€åŠï¼Œä¾‹å¦‚ (32-3+2)//2+1 == 16
        # æ­¤å¤– resnet çš„ 3x3 å·ç§¯å±‚ä¸ä½¿ç”¨åç§»å€¼ bias
        self.conv1 = nn.Sequential(
            nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(channels_out))
        # å†å®šä¹‰ä¸€ä¸ªè®©è¾“å‡ºå’Œè¾“å…¥ç»´åº¦ç›¸åŒçš„ 3x3 å·ç§¯å±‚
        self.conv2 = nn.Sequential(
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(channels_out))
        # è®©åŸå§‹è¾“å…¥å’Œè¾“å‡ºç›¸åŠ çš„æ—¶å€™ï¼Œéœ€è¦ç»´åº¦ä¸€è‡´ï¼Œå¦‚æœç»´åº¦ä¸ä¸€è‡´åˆ™éœ€è¦æ•´åˆ
        self.identity = nn.Sequential()
        if stride != 1 or channels_in != channels_out * self.expansion:
            self.identity = nn.Sequential(
                nn.Conv2d(channels_in, channels_out * self.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(channels_out * self.expansion))

    def forward(self, x):
        # x => conv1 => relu => conv2 => + => relu
        # |                              ^
        # |==============================|
        tmp = self.conv1(x)
        tmp = nn.functional.relu(tmp, inplace=True)
        tmp = self.conv2(tmp)
        tmp += self.identity(x)
        y = nn.functional.relu(tmp, inplace=True)
        return y

class MyModel(nn.Module):
    """YOLO (åŸºäº ResNet çš„å˜ç§)"""
    Anchors = None # é”šç‚¹åˆ—è¡¨ï¼ŒåŒ…å« é”šç‚¹æ•°é‡ * å½¢çŠ¶æ•°é‡ çš„èŒƒå›´
    AnchorSpans = (16, 32, 64) # å°ºåº¦åˆ—è¡¨ï¼Œå€¼ä¸ºé”šç‚¹ä¹‹é—´çš„è·ç¦»
    AnchorAspects = ((1, 1), (1.5, 1.5)) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
    AnchorOutputs = 1 + 4 + len(CLASSES) # æ¯ä¸ªé”šç‚¹èŒƒå›´å¯¹åº”çš„è¾“å‡ºæ•°é‡ï¼Œæ˜¯å¦å¯¹è±¡ä¸­å¿ƒ (1) + åŒºåŸŸåç§» (4) + åˆ†ç±»æ•°é‡
    AnchorTotalOutputs = AnchorOutputs * len(AnchorAspects) # æ¯ä¸ªé”šç‚¹å¯¹åº”çš„è¾“å‡ºæ•°é‡
    ObjScoreThreshold = 0.9 # è®¤ä¸ºæ˜¯å¯¹è±¡ä¸­å¿ƒæ‰€éœ€è¦çš„æœ€å°åˆ†æ•°
    IOUMergeThreshold = 0.3 # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶é‡å åŒºåŸŸçš„é‡å ç‡é˜ˆå€¼

    def __init__(self):
        super().__init__()
        # æŠ½å–å›¾ç‰‡ç‰¹å¾çš„ ResNet
        # å› ä¸ºé”šç‚¹è·ç¦»æœ‰ä¸‰ä¸ªï¼Œè¿™é‡Œæœ€åä¼šè¾“å‡ºå„ä¸ªé”šç‚¹è·ç¦»å¯¹åº”çš„ç‰¹å¾
        self.previous_channels_out = 4
        self.resnet_models = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(self.previous_channels_out),
                nn.ReLU(inplace=True),
                self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=1),
                self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
                self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
                self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2),
                self._make_layer(BasicBlock, channels_out=256, num_blocks=2, stride=2)),
            self._make_layer(BasicBlock, channels_out=256, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=256, num_blocks=2, stride=2)
        ])
        # æ ¹æ®å„ä¸ªé”šç‚¹è·ç¦»å¯¹åº”çš„ç‰¹å¾é¢„æµ‹è¾“å‡ºçš„å·ç§¯å±‚
        # å¤§çš„é”šç‚¹è·ç¦»æŠ½å–çš„ç‰¹å¾ä¼šåˆå¹¶åˆ°å°çš„é”šç‚¹è·ç¦»æŠ½å–çš„ç‰¹å¾
        # è¿™é‡Œçš„ä¸‰ä¸ªå­æ¨¡å‹æ„ä¹‰åˆ†åˆ«æ˜¯:
        # - è®¡ç®—ç”¨äºåˆå¹¶çš„ç‰¹å¾
        # - æ”¾å¤§ç‰¹å¾
        # - è®¡ç®—æœ€ç»ˆçš„é¢„æµ‹è¾“å‡º
        self.yolo_detectors = nn.ModuleList([
            nn.ModuleList([nn.Sequential(
                nn.Conv2d(256 if index == 0 else 512, 256, kernel_size=1, stride=1, padding=0, bias=True),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0, bias=True),
                nn.ReLU(inplace=True)),
            nn.Upsample(scale_factor=2, mode="nearest"),
            nn.Sequential(
                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, bias=True),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, MyModel.AnchorTotalOutputs, kernel_size=1, stride=1, padding=0, bias=True))])
            for index in range(len(self.resnet_models))
        ])
        # å¤„ç†ç»“æœèŒƒå›´çš„å‡½æ•°
        self.sigmoid = nn.Sigmoid()

    def _make_layer(self, block_type, channels_out, num_blocks, stride):
        """åˆ›å»º resnet ä½¿ç”¨çš„å±‚"""
        blocks = []
        # æ·»åŠ ç¬¬ä¸€ä¸ªå—
        blocks.append(block_type(self.previous_channels_out, channels_out, stride))
        self.previous_channels_out = channels_out * block_type.expansion
        # æ·»åŠ å‰©ä½™çš„å—ï¼Œå‰©ä½™çš„å—å›ºå®šå¤„ç†é—´éš”ä¸º 1ï¼Œä¸ä¼šæ”¹å˜é•¿å®½
        for _ in range(num_blocks-1):
            blocks.append(block_type(self.previous_channels_out, self.previous_channels_out, 1))
            self.previous_channels_out *= block_type.expansion
        return nn.Sequential(*blocks)

    @staticmethod
    def _generate_anchors():
        """æ ¹æ®é”šç‚¹å’Œå½¢çŠ¶ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨"""
        w, h = IMAGE_SIZE
        anchors = []
        for span in MyModel.AnchorSpans:
            for x in range(0, w, span):
                for y in range(0, h, span):
                    xcenter, ycenter = x + span / 2, y + span / 2
                    for ratio in MyModel.AnchorAspects:
                        ww = span * ratio[0]
                        hh = span * ratio[1]
                        xx = xcenter - ww / 2
                        yy = ycenter - hh / 2
                        xx = max(int(xx), 0)
                        yy = max(int(yy), 0)
                        ww = min(int(ww), w - xx)
                        hh = min(int(hh), h - yy)
                        anchors.append((xx, yy, ww, hh))
        return anchors

    def forward(self, x):
        # æŠ½å–å„ä¸ªé”šç‚¹è·ç¦»å¯¹åº”çš„ç‰¹å¾
        # ç»´åº¦åˆ†åˆ«æ˜¯:
        # torch.Size([16, 256, 16, 12])
        # torch.Size([16, 256, 8, 6])
        # torch.Size([16, 256, 4, 3])
        features_list = []
        resnet_input = x
        for m in self.resnet_models:
            resnet_input = m(resnet_input)
            features_list.append(resnet_input)
        # æ ¹æ®ç‰¹å¾é¢„æµ‹è¾“å‡º
        # ç»´åº¦åˆ†åˆ«æ˜¯:
        # torch.Size([16, 16, 4, 3])
        # torch.Size([16, 16, 8, 6])
        # torch.Size([16, 16, 16, 12])
        # 16 æ˜¯ (5 + åˆ†ç±»3) * å½¢çŠ¶2
        previous_upsampled_feature = None
        outputs = []
        for index, feature in enumerate(reversed(features_list)):
            if previous_upsampled_feature is not None:
                # åˆå¹¶å¤§çš„é”šç‚¹è·ç¦»æŠ½å–çš„ç‰¹å¾åˆ°å°çš„é”šç‚¹è·ç¦»æŠ½å–çš„ç‰¹å¾
                feature = torch.cat((feature, previous_upsampled_feature), dim=1)
            # è®¡ç®—ç”¨äºåˆå¹¶çš„ç‰¹å¾
            hidden = self.yolo_detectors[index][0](feature)
            # æ”¾å¤§ç‰¹å¾ (ç”¨äºä¸‹ä¸€æ¬¡å¤„ç†æ—¶åˆå¹¶)
            upsampled = self.yolo_detectors[index][1](hidden)
            # è®¡ç®—æœ€ç»ˆçš„é¢„æµ‹è¾“å‡º
            output = self.yolo_detectors[index][2](hidden)
            previous_upsampled_feature = upsampled
            outputs.append(output)
        # è¿æ¥æ‰€æœ‰è¾“å‡º
        # æ³¨æ„é¡ºåºéœ€è¦ä¸ Anchors ä¸€è‡´
        outputs_flatten = []
        for output in reversed(outputs):
            output = output.permute(0, 2, 3, 1)
            output = output.reshape(output.shape[0], -1, MyModel.AnchorOutputs)
            outputs_flatten.append(output)
        outputs_all = torch.cat(outputs_flatten, dim=1)
        # æ˜¯å¦å¯¹è±¡ä¸­å¿ƒåº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´ï¼Œä½¿ç”¨ sigmoid å¤„ç†
        outputs_all[:,:,:1] = self.sigmoid(outputs_all[:,:,:1])
        # åˆ†ç±»åº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´ï¼Œä½¿ç”¨ sigmoid å¤„ç†
        outputs_all[:,:,5:] = self.sigmoid(outputs_all[:,:,5:])
        return outputs_all

    @staticmethod
    def loss_function(predicted, actual):
        """YOLO ä½¿ç”¨çš„å¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨"""
        result_tensor, result_isobject_masks, result_nonobject_masks = actual
        objectness_losses = []
        offsets_losses = []
        labels_losses = []
        for x in range(result_tensor.shape[0]):
            mask_positive = result_isobject_masks[x]
            mask_negative = result_nonobject_masks[x]
            # è®¡ç®—æ˜¯å¦å¯¹è±¡ä¸­å¿ƒçš„æŸå¤±ï¼Œåˆ†åˆ«é’ˆå¯¹æ­£è´Ÿæ ·æœ¬è®¡ç®—
            # å› ä¸ºå¤§éƒ¨åˆ†åŒºåŸŸä¸åŒ…å«å¯¹è±¡ä¸­å¿ƒï¼Œè¿™é‡Œå‡å°‘è´Ÿæ ·æœ¬çš„æŸå¤±å¯¹è°ƒæ•´å‚æ•°çš„å½±å“
            objectness_loss_positive = nn.functional.mse_loss(
                predicted[x,mask_positive,0], result_tensor[x,mask_positive,0])
            objectness_loss_negative = nn.functional.mse_loss(
                predicted[x,mask_negative,0], result_tensor[x,mask_negative,0]) * 0.5
            objectness_losses.append(objectness_loss_positive)
            objectness_losses.append(objectness_loss_negative)
            # è®¡ç®—åŒºåŸŸåç§»çš„æŸå¤±ï¼Œåªé’ˆå¯¹æ­£æ ·æœ¬è®¡ç®—
            offsets_loss = nn.functional.mse_loss(
                predicted[x,mask_positive,1:5], result_tensor[x,mask_positive,1:5])
            offsets_losses.append(offsets_loss)
            # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æŸå¤±ï¼Œåˆ†åˆ«é’ˆå¯¹æ­£è´Ÿæ ·æœ¬è®¡ç®—
            labels_loss_positive = nn.functional.binary_cross_entropy(
                predicted[x,mask_positive,5:], result_tensor[x,mask_positive,5:])
            labels_loss_negative = nn.functional.binary_cross_entropy(
                predicted[x,mask_negative,5:], result_tensor[x,mask_negative,5:]) * 0.5
            labels_losses.append(labels_loss_positive)
            labels_losses.append(labels_loss_negative)
        loss = (
            torch.mean(torch.stack(objectness_losses)) +
            torch.mean(torch.stack(offsets_losses)) +
            torch.mean(torch.stack(labels_losses)))
        return loss

    @staticmethod
    def calc_accuracy(actual, predicted):
        """YOLO ä½¿ç”¨çš„æ­£ç¡®ç‡è®¡ç®—å™¨ï¼Œè¿™é‡Œåªè®¡ç®—æ˜¯å¦å¯¹è±¡ä¸­å¿ƒä¸æ ‡ç­¾åˆ†ç±»çš„æ­£ç¡®ç‡ï¼ŒåŒºåŸŸåç§»ä¸è®¡ç®—"""
        result_tensor, result_isobject_masks, result_nonobject_masks = actual
        # è®¡ç®—æ˜¯å¦å¯¹è±¡ä¸­å¿ƒçš„æ­£ç¡®ç‡ï¼Œæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æ­£ç¡®ç‡åˆ†åˆ«è®¡ç®—å†å¹³å‡
        a = result_tensor[:,:,0]
        p = predicted[:,:,0] > MyModel.ObjScoreThreshold
        obj_acc_positive = ((a == 1) & (p == 1)).sum().item() / ((a == 1).sum().item() + 0.00001)
        obj_acc_negative = ((a == 0) & (p == 0)).sum().item() / ((a == 0).sum().item() + 0.00001)
        obj_acc = (obj_acc_positive + obj_acc_negative) / 2
        # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æ­£ç¡®ç‡
        cls_total = 0
        cls_correct = 0
        for x in range(result_tensor.shape[0]):
            mask = list(sorted(result_isobject_masks[x] + result_nonobject_masks[x]))
            actual_classes = result_tensor[x,mask,5:].max(dim=1).indices
            predicted_classes = predicted[x,mask,5:].max(dim=1).indices
            cls_total += len(mask)
            cls_correct += (actual_classes == predicted_classes).sum().item()
        cls_acc = cls_correct / cls_total
        return obj_acc, cls_acc

    @staticmethod
    def convert_predicted_result(predicted):
        """è½¬æ¢é¢„æµ‹ç»“æœåˆ° (æ ‡ç­¾, åŒºåŸŸ, å¯¹è±¡ä¸­å¿ƒåˆ†æ•°, æ ‡ç­¾è¯†åˆ«åˆ†æ•°) çš„åˆ—è¡¨ï¼Œé‡å åŒºåŸŸä½¿ç”¨ NMS ç®—æ³•åˆå¹¶"""
        # è®°å½•é‡å çš„ç»“æœåŒºåŸŸ, ç»“æœæ˜¯ [ [(æ ‡ç­¾, åŒºåŸŸ, RPN åˆ†æ•°, æ ‡ç­¾è¯†åˆ«åˆ†æ•°)], ... ]
        final_result = []
        for anchor, tensor in zip(MyModel.Anchors, predicted):
            obj_score = tensor[0].item()
            if obj_score <= MyModel.ObjScoreThreshold:
                # è¦æ±‚å¯¹è±¡ä¸­å¿ƒåˆ†æ•°è¶…è¿‡ä¸€å®šå€¼
                continue
            offset = tensor[1:5].tolist()
            offset[0] = max(min(offset[0], 1), 0) # ä¸­å¿ƒç‚¹ x çš„åç§»åº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´
            offset[1] = max(min(offset[1], 1), 0) # ä¸­å¿ƒç‚¹ y çš„åç§»åº”è¯¥åœ¨ 0 ~ 1 ä¹‹é—´
            box = adjust_box_by_offset(anchor, offset)
            label_max = tensor[5:].max(dim=0)
            cls_score = label_max.values.item()
            label = label_max.indices.item()
            if label == 0:
                # è·³è¿‡éå¯¹è±¡åˆ†ç±»
                continue
            for index in range(len(final_result)):
                exists_results = final_result[index]
                if any(calc_iou(box, r[1]) > MyModel.IOUMergeThreshold for r in exists_results):
                    exists_results.append((label, box, obj_score, cls_score))
                    break
            else:
                final_result.append([(label, box, obj_score, cls_score)])
        # åˆå¹¶é‡å çš„ç»“æœåŒºåŸŸ (ä½¿ç”¨ å¯¹è±¡ä¸­å¿ƒåˆ†æ•° * æ ‡ç­¾è¯†åˆ«åˆ†æ•° æœ€é«˜çš„åŒºåŸŸä¸ºç»“æœåŒºåŸŸ)
        for index in range(len(final_result)):
            exists_results = final_result[index]
            exists_results.sort(key=lambda r: r[2]*r[3])
            final_result[index] = exists_results[-1]
        return final_result

    @staticmethod
    def fix_predicted_result_from_history(cls_result, history_results):
        """æ ¹æ®å†å²ç»“æœå‡å°‘é¢„æµ‹ç»“æœä¸­çš„è¯¯åˆ¤ï¼Œé€‚ç”¨äºè§†é¢‘è¯†åˆ«ï¼Œhistory_results åº”ä¸ºæŒ‡å®šäº† maxlen çš„ deque"""
        # è¦æ±‚å†å²ç»“æœä¸­ 50% ä»¥ä¸Šå­˜åœ¨ç±»ä¼¼åŒºåŸŸï¼Œå¹¶ä¸”é€‰å–å†å²ç»“æœä¸­æœ€å¤šçš„åˆ†ç±»
        history_results.append(cls_result)
        final_result = []
        if len(history_results) < history_results.maxlen:
            # å†å²ç»“æœä¸è¶³ï¼Œä¸è¿”å›ä»»ä½•è¯†åˆ«ç»“æœ
            return final_result
        for label, box, rpn_score, cls_score in cls_result:
            # æŸ¥æ‰¾å†å²ä¸­çš„è¿‘ä¼¼åŒºåŸŸ
            similar_results = []
            for history_result in history_results:
                history_result = [(calc_iou(r[1], box), r) for r in history_result]
                history_result.sort(key = lambda r: r[0])
                if history_result and history_result[-1][0] > MyModel.IOUMergeThreshold:
                    similar_results.append(history_result[-1][1])
            # åˆ¤æ–­è¿‘ä¼¼åŒºåŸŸæ•°é‡æ˜¯å¦è¿‡åŠ
            if len(similar_results) < history_results.maxlen // 2:
                continue
            # é€‰å–å†å²ç»“æœä¸­æœ€å¤šçš„åˆ†ç±»
            cls_groups = defaultdict(lambda: [])
            for r in similar_results:
                cls_groups[r[0]].append(r)
            most_common = sorted(cls_groups.values(), key=len)[-1]
            # æ·»åŠ æœ€å¤šçš„åˆ†ç±»ä¸­çš„æœ€æ–°çš„ç»“æœ
            final_result.append(most_common[-1])
        return final_result

MyModel.Anchors = MyModel._generate_anchors()

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def calc_resize_parameters(sw, sh):
    """è®¡ç®—ç¼©æ”¾å›¾ç‰‡çš„å‚æ•°"""
    sw_new, sh_new = sw, sh
    dw, dh = IMAGE_SIZE
    pad_w, pad_h = 0, 0
    if sw / sh < dw / dh:
        sw_new = int(dw / dh * sh)
        pad_w = (sw_new - sw) // 2 # å¡«å……å·¦å³
    else:
        sh_new = int(dh / dw * sw)
        pad_h = (sh_new - sh) // 2 # å¡«å……ä¸Šä¸‹
    return sw_new, sh_new, pad_w, pad_h

def resize_image(img):
    """ç¼©æ”¾å›¾ç‰‡ï¼Œæ¯”ä¾‹ä¸ä¸€è‡´æ—¶å¡«å……"""
    sw, sh = img.size
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    img_new = Image.new("RGB", (sw_new, sh_new))
    img_new.paste(img, (pad_w, pad_h))
    img_new = img_new.resize(IMAGE_SIZE)
    return img_new

def image_to_tensor(img):
    """è½¬æ¢å›¾ç‰‡å¯¹è±¡åˆ° tensor å¯¹è±¡"""
    arr = numpy.asarray(img)
    t = torch.from_numpy(arr)
    t = t.transpose(0, 2) # è½¬æ¢ç»´åº¦ H,W,C åˆ° C,W,H
    t = t / 255.0 # æ­£è§„åŒ–æ•°å€¼ä½¿å¾—èŒƒå›´åœ¨ 0 ~ 1
    return t

def map_box_to_resized_image(box, sw, sh):
    """æŠŠåŸå§‹åŒºåŸŸè½¬æ¢åˆ°ç¼©æ”¾åçš„å›¾ç‰‡å¯¹åº”çš„åŒºåŸŸ"""
    x, y, w, h = box
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int((x + pad_w) * scale)
    y = int((y + pad_h) * scale)
    w = int(w * scale)
    h = int(h * scale)
    if x + w > IMAGE_SIZE[0] or y + h > IMAGE_SIZE[1] or w == 0 or h == 0:
        return 0, 0, 0, 0
    return x, y, w, h

def map_box_to_original_image(box, sw, sh):
    """æŠŠç¼©æ”¾åå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸè½¬æ¢åˆ°ç¼©æ”¾å‰çš„åŸå§‹åŒºåŸŸ"""
    x, y, w, h = box
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int(x / scale - pad_w)
    y = int(y / scale - pad_h)
    w = int(w / scale)
    h = int(h / scale)
    if x + w > sw or y + h > sh or x < 0 or y < 0 or w == 0 or h == 0:
        return 0, 0, 0, 0
    return x, y, w, h

def calc_iou(rect1, rect2):
    """è®¡ç®—ä¸¤ä¸ªåŒºåŸŸé‡å éƒ¨åˆ† / åˆå¹¶éƒ¨åˆ†çš„æ¯”ç‡ (intersection over union)"""
    x1, y1, w1, h1 = rect1
    x2, y2, w2, h2 = rect2
    xi = max(x1, x2)
    yi = max(y1, y2)
    wi = min(x1+w1, x2+w2) - xi
    hi = min(y1+h1, y2+h2) - yi
    if wi > 0 and hi > 0: # æœ‰é‡å éƒ¨åˆ†
        area_overlap = wi*hi
        area_all = w1*h1 + w2*h2 - area_overlap
        iou = area_overlap / area_all
    else: # æ²¡æœ‰é‡å éƒ¨åˆ†
        iou = 0
    return iou

def calc_box_offset(candidate_box, true_box):
    """è®¡ç®—å€™é€‰åŒºåŸŸä¸å®é™…åŒºåŸŸçš„åç§»å€¼ï¼Œè¦æ±‚å®é™…åŒºåŸŸçš„ä¸­å¿ƒç‚¹å¿…é¡»åœ¨å€™é€‰åŒºåŸŸä¸­"""
    # è®¡ç®—å®é™…åŒºåŸŸçš„ä¸­å¿ƒç‚¹åœ¨å€™é€‰åŒºåŸŸä¸­çš„ä½ç½®ï¼ŒèŒƒå›´ä¼šåœ¨ 0 ~ 1 ä¹‹é—´
    x1, y1, w1, h1 = candidate_box
    x2, y2, w2, h2 = true_box
    x_offset = ((x2 + w2 // 2) - x1) / w1
    y_offset = ((y2 + h2 // 2) - y1) / h1
    # è®¡ç®—å®é™…åŒºåŸŸé•¿å®½ç›¸å¯¹äºå€™é€‰åŒºåŸŸé•¿å®½çš„æ¯”ä¾‹ï¼Œä½¿ç”¨ log å‡å°‘è¿‡å¤§çš„å€¼
    w_offset = math.log(w2 / w1)
    h_offset = math.log(h2 / h1)
    return (x_offset, y_offset, w_offset, h_offset)

def adjust_box_by_offset(candidate_box, offset):
    """æ ¹æ®åç§»å€¼è°ƒæ•´å€™é€‰åŒºåŸŸ"""
    x1, y1, w1, h1 = candidate_box
    x_offset, y_offset, w_offset, h_offset = offset
    w2 = math.exp(w_offset) * w1
    h2 = math.exp(h_offset) * h1
    x2 = x1 + w1 * x_offset - w2 // 2
    y2 = y1 + h1 * y_offset - h2 // 2
    x2 = min(IMAGE_SIZE[0]-1,  x2)
    y2 = min(IMAGE_SIZE[1]-1,  y2)
    w2 = min(IMAGE_SIZE[0]-x2, w2)
    h2 = min(IMAGE_SIZE[1]-y2, h2)
    return (x2, y2, w2, h2)

def prepare_save_batch(batch, image_tensors, result_tensors, result_isobject_masks, result_nonobject_masks):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æŒ‰ç´¢å¼•å€¼åˆ—è¡¨ç”Ÿæˆè¾“å…¥å’Œè¾“å‡º tensor å¯¹è±¡çš„å‡½æ•°
    def split_dataset(indices):
        indices_list = indices.tolist()
        image_tensors_splited = torch.stack([image_tensors[x] for x in indices_list])
        result_tensors_splited = torch.stack([result_tensors[x] for x in indices_list])
        result_isobject_masks_splited = [result_isobject_masks[x] for x in indices_list]
        result_nonobject_masks_splited = [result_nonobject_masks[x] for x in indices_list]
        return image_tensors_splited, (
            result_tensors_splited, result_isobject_masks_splited, result_nonobject_masks_splited)

    # åˆ‡åˆ†è®­ç»ƒé›† (80%)ï¼ŒéªŒè¯é›† (10%) å’Œæµ‹è¯•é›† (10%)
    random_indices = torch.randperm(len(image_tensors))
    training_indices = random_indices[:int(len(random_indices)*0.8)]
    validating_indices = random_indices[int(len(random_indices)*0.8):int(len(random_indices)*0.9):]
    testing_indices = random_indices[int(len(random_indices)*0.9):]
    training_set = split_dataset(training_indices)
    validating_set = split_dataset(validating_indices)
    testing_set = split_dataset(testing_indices)

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # åŠ è½½å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
    # { (è·¯å¾„, æ˜¯å¦å·¦å³ç¿»è½¬): [ åŒºåŸŸä¸åˆ†ç±», åŒºåŸŸä¸åˆ†ç±», .. ] }
    # åŒä¸€å¼ å›¾ç‰‡å·¦å³ç¿»è½¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ–°çš„æ•°æ®ï¼Œè®©æ•°æ®é‡ç¿»å€
    box_map = defaultdict(lambda: [])
    for filename in os.listdir(DATASET_1_IMAGE_DIR):
        # ä»ç¬¬ä¸€ä¸ªæ•°æ®é›†åŠ è½½
        xml_path = os.path.join(DATASET_1_ANNOTATION_DIR, filename.split(".")[0] + ".xml")
        if not os.path.isfile(xml_path):
            continue
        tree = ET.ElementTree(file=xml_path)
        objects = tree.findall("object")
        path = os.path.join(DATASET_1_IMAGE_DIR, filename)
        for obj in objects:
            class_name = obj.find("name").text
            x1 = int(obj.find("bndbox/xmin").text)
            x2 = int(obj.find("bndbox/xmax").text)
            y1 = int(obj.find("bndbox/ymin").text)
            y2 = int(obj.find("bndbox/ymax").text)
            if class_name == "mask_weared_incorrect":
                # ä½©æˆ´å£ç½©ä¸æ­£ç¡®çš„æ ·æœ¬æ•°é‡å¤ªå°‘ (åªæœ‰ 123)ï¼Œæ¨¡å‹æ— æ³•å­¦ä¹ ï¼Œè¿™é‡Œå…¨åˆå¹¶åˆ°æˆ´å£ç½©çš„æ ·æœ¬
                class_name = "with_mask"
            box_map[(path, False)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))
            box_map[(path, True)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))
    df = pandas.read_csv(DATASET_2_BOX_CSV_PATH)
    for row in df.values:
        # ä»ç¬¬äºŒä¸ªæ•°æ®é›†åŠ è½½ï¼Œè¿™ä¸ªæ•°æ®é›†åªåŒ…å«æ²¡æœ‰å¸¦å£ç½©çš„å›¾ç‰‡
        filename, width, height, x1, y1, x2, y2 = row[:7]
        path = os.path.join(DATASET_2_IMAGE_DIR, filename)
        box_map[(path, False)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING["without_mask"]))
        box_map[(path, True)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING["without_mask"]))
    # æ‰“ä¹±æ•°æ®é›† (å› ä¸ºç¬¬äºŒä¸ªæ•°æ®é›†åªæœ‰ä¸æˆ´å£ç½©çš„å›¾ç‰‡)
    box_list = list(box_map.items())
    random.shuffle(box_list)
    print(f"found {len(box_list)} images")

    # ä¿å­˜å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åˆ†ç±»ä¸åŒºåŸŸåˆ—è¡¨
    batch_size = 20
    batch = 0
    image_tensors = [] # å›¾ç‰‡åˆ—è¡¨
    result_tensors = [] # å›¾ç‰‡å¯¹åº”çš„è¾“å‡ºç»“æœåˆ—è¡¨ï¼ŒåŒ…å« [ æ˜¯å¦å¯¹è±¡ä¸­å¿ƒ, åŒºåŸŸåç§», å„ä¸ªåˆ†ç±»çš„å¯èƒ½æ€§ ]
    result_isobject_masks = [] # å„ä¸ªå›¾ç‰‡çš„åŒ…å«å¯¹è±¡çš„åŒºåŸŸåœ¨ Anchors ä¸­çš„ç´¢å¼•
    result_nonobject_masks = [] # å„ä¸ªå›¾ç‰‡ä¸åŒ…å«å¯¹è±¡çš„åŒºåŸŸåœ¨ Anchors ä¸­çš„ç´¢å¼• (é‡å ç‡ä½äºé˜ˆå€¼çš„åŒºåŸŸ)
    for (image_path, flip), original_boxes_labels in box_list:
        with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
            sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
            if flip:
                img = resize_image(img_original.transpose(Image.FLIP_LEFT_RIGHT)) # ç¿»è½¬ç„¶åç¼©æ”¾å›¾ç‰‡
            else:
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
            image_tensors.append(image_to_tensor(img)) # æ·»åŠ å›¾ç‰‡åˆ°åˆ—è¡¨
        # ç”Ÿæˆè¾“å‡ºç»“æœçš„ tensor
        result_tensor = torch.zeros((len(MyModel.Anchors), MyModel.AnchorOutputs), dtype=torch.float)
        result_tensor[:,5] = 1 # é»˜è®¤åˆ†ç±»ä¸º other
        result_tensors.append(result_tensor)
        # åŒ…å«å¯¹è±¡çš„åŒºåŸŸåœ¨ Anchors ä¸­çš„ç´¢å¼•
        result_isobject_mask = []
        result_isobject_masks.append(result_isobject_mask)
        # ä¸åŒ…å«å¯¹è±¡çš„åŒºåŸŸåœ¨ Anchors ä¸­çš„ç´¢å¼•
        result_nonobject_mask = []
        result_nonobject_masks.append(result_nonobject_mask)
        # æ ¹æ®çœŸå®åŒºåŸŸå®šä½æ‰€å±çš„é”šç‚¹ï¼Œç„¶åè®¾ç½®è¾“å‡ºç»“æœ
        negative_mapping = [1] * len(MyModel.Anchors)
        for box_label in original_boxes_labels:
            x, y, w, h, label = box_label
            if flip: # ç¿»è½¬åæ ‡
                x = sw - x - w
            x, y, w, h = map_box_to_resized_image((x, y, w, h), sw, sh) # ç¼©æ”¾å®é™…åŒºåŸŸ
            if w < 20 or h < 20:
                continue # ç¼©æ”¾ååŒºåŸŸè¿‡å°
            # æ£€æŸ¥è®¡ç®—æ˜¯å¦æœ‰é—®é¢˜
            # child_img = img.copy().crop((x, y, x+w, y+h))
            # child_img.save(f"{os.path.basename(image_path)}_{x}_{y}_{w}_{h}_{label}.png")
            # å®šä½æ‰€å±çš„é”šç‚¹
            # è¦æ±‚:
            # - ä¸­å¿ƒç‚¹è½åœ¨é”šç‚¹å¯¹åº”çš„åŒºåŸŸä¸­
            # - é‡å ç‡è¶…è¿‡ä¸€å®šå€¼
            x_center = x + w // 2
            y_center = y + h // 2
            matched_anchors = []
            for index, anchor in enumerate(MyModel.Anchors):
                ax, ay, aw, ah = anchor
                is_center = (x_center >= ax and x_center < ax + aw and
                    y_center >= ay and y_center < ay + ah)
                iou = calc_iou(anchor, (x, y, w, h))
                if is_center and iou > IOU_POSITIVE_THRESHOLD:
                    matched_anchors.append((index, anchor)) # åŒºåŸŸåŒ…å«å¯¹è±¡ä¸­å¿ƒå¹¶ä¸”é‡å ç‡è¶…è¿‡ä¸€å®šå€¼
                    negative_mapping[index] = 0
                elif iou > IOU_NEGATIVE_THRESHOLD:
                    negative_mapping[index] = 0 # åŒºåŸŸä¸æŸä¸ªå¯¹è±¡é‡å ç‡è¶…è¿‡ä¸€å®šå€¼ï¼Œä¸åº”è¯¥å½“ä½œè´Ÿæ ·æœ¬
            for matched_index, matched_box in matched_anchors:
                # è®¡ç®—åŒºåŸŸåç§»
                offset = calc_box_offset(matched_box, (x, y, w, h))
                # ä¿®æ”¹è¾“å‡ºç»“æœçš„ tensor
                result_tensor[matched_index] = torch.tensor((
                    1, # æ˜¯å¦å¯¹è±¡ä¸­å¿ƒ
                    *offset, # åŒºåŸŸåç§»
                    *[int(c == label) for c in range(len(CLASSES))] # å¯¹åº”åˆ†ç±»
                ), dtype=torch.float)
                # æ·»åŠ ç´¢å¼•å€¼
                # æ³¨æ„å¦‚æœä¸¤ä¸ªå¯¹è±¡åŒæ—¶å®šä½åˆ°ç›¸åŒçš„é”šç‚¹ï¼Œé‚£ä¹ˆåªæœ‰ä¸€ä¸ªå¯¹è±¡å¯ä»¥è¢«è¯†åˆ«ï¼Œè¿™é‡Œåé¢çš„å¯¹è±¡ä¼šè¦†ç›–å‰é¢çš„å¯¹è±¡
                if matched_index not in result_isobject_mask:
                    result_isobject_mask.append(matched_index)
        # æ²¡æœ‰æ‰¾åˆ°å¯è¯†åˆ«çš„å¯¹è±¡æ—¶è·³è¿‡å›¾ç‰‡
        if not result_isobject_mask:
            image_tensors.pop()
            result_tensors.pop()
            result_isobject_masks.pop()
            result_nonobject_masks.pop()
            continue
        # æ·»åŠ ä¸åŒ…å«å¯¹è±¡çš„åŒºåŸŸåœ¨ Anchors ä¸­çš„ç´¢å¼•
        for index, value in enumerate(negative_mapping):
            if value:
                result_nonobject_mask.append(index)
        # æ’åºç´¢å¼•åˆ—è¡¨
        result_isobject_mask.sort()
        # ä¿å­˜æ‰¹æ¬¡
        if len(image_tensors) >= batch_size:
            prepare_save_batch(batch, image_tensors, result_tensors,
                result_isobject_masks, result_nonobject_masks)
            image_tensors.clear()
            result_tensors.clear()
            result_isobject_masks.clear()
            result_nonobject_masks.clear()
            batch += 1
    # ä¿å­˜å‰©ä½™çš„æ‰¹æ¬¡
    if len(image_tensors) > 10:
        prepare_save_batch(batch, image_tensors, result_tensors,
            result_isobject_masks, result_nonobject_masks)

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel().to(device)

    # åˆ›å»ºå¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨
    loss_function = MyModel.loss_function

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_obj_accuracy_history = []
    training_cls_accuracy_history = []
    validating_obj_accuracy_history = []
    validating_cls_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_obj_accuracy_highest = -1
    validating_cls_accuracy_highest = -1
    validating_accuracy_highest = -1
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            x, (y, mask1, mask2) = load_tensor(path)
            yield x.to(device), (y.to(device), mask1, mask2)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    calc_accuracy = MyModel.calc_accuracy

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_obj_accuracy_list = []
        training_cls_accuracy_list = []
        for batch_index, batch in enumerate(read_batches("data/training_set")):
            # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
            batch_x, batch_y = batch
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted, batch_y)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_batch_obj_accuracy, training_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
            # è¾“å‡ºæ‰¹æ¬¡æ­£ç¡®ç‡
            training_obj_accuracy_list.append(training_batch_obj_accuracy)
            training_cls_accuracy_list.append(training_batch_cls_accuracy)
            print(f"epoch: {epoch}, batch: {batch_index}: " +
                f"batch obj accuracy: {training_batch_obj_accuracy}, cls accuracy: {training_batch_cls_accuracy}")
        training_obj_accuracy = sum(training_obj_accuracy_list) / len(training_obj_accuracy_list)
        training_cls_accuracy = sum(training_cls_accuracy_list) / len(training_cls_accuracy_list)
        training_obj_accuracy_history.append(training_obj_accuracy)
        training_cls_accuracy_history.append(training_cls_accuracy)
        print(f"training obj accuracy: {training_obj_accuracy}, cls accuracy: {training_cls_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_obj_accuracy_list = []
        validating_cls_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_y = batch
            predicted = model(batch_x)
            validating_batch_obj_accuracy, validating_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
            validating_obj_accuracy_list.append(validating_batch_obj_accuracy)
            validating_cls_accuracy_list.append(validating_batch_cls_accuracy)
            # é‡Šæ”¾ predicted å ç”¨çš„æ˜¾å­˜é¿å…æ˜¾å­˜ä¸è¶³çš„é”™è¯¯
            predicted = None
        validating_obj_accuracy = sum(validating_obj_accuracy_list) / len(validating_obj_accuracy_list)
        validating_cls_accuracy = sum(validating_cls_accuracy_list) / len(validating_cls_accuracy_list)
        validating_obj_accuracy_history.append(validating_obj_accuracy)
        validating_cls_accuracy_history.append(validating_cls_accuracy)
        print(f"validating obj accuracy: {validating_obj_accuracy}, cls accuracy: {validating_cls_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        validating_accuracy = validating_obj_accuracy * validating_cls_accuracy
        if validating_accuracy > validating_accuracy_highest:
            validating_obj_accuracy_highest = validating_obj_accuracy
            validating_cls_accuracy_highest = validating_cls_accuracy
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest obj validating accuracy: {validating_obj_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    print(f"highest cls validating accuracy: {validating_cls_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_obj_accuracy_list = []
    testing_cls_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_y = batch
        predicted = model(batch_x)
        testing_batch_obj_accuracy, testing_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
        testing_obj_accuracy_list.append(testing_batch_obj_accuracy)
        testing_cls_accuracy_list.append(testing_batch_cls_accuracy)
    testing_obj_accuracy = sum(testing_obj_accuracy_list) / len(testing_obj_accuracy_list)
    testing_cls_accuracy = sum(testing_cls_accuracy_list) / len(testing_cls_accuracy_list)
    print(f"testing obj accuracy: {testing_obj_accuracy}, cls accuracy: {testing_cls_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_obj_accuracy_history, label="training_obj_accuracy")
    pyplot.plot(training_cls_accuracy_history, label="training_cls_accuracy")
    pyplot.plot(validating_obj_accuracy_history, label="validating_obj_accuracy")
    pyplot.plot(validating_cls_accuracy_history, label="validating_cls_accuracy")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«å›¾ç‰‡"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®å›¾ç‰‡è·¯å¾„ï¼Œå¹¶æ˜¾ç¤ºæ‰€æœ‰å¯èƒ½æ˜¯äººè„¸çš„åŒºåŸŸ
    while True:
        try:
            image_path = input("Image path: ")
            if not image_path:
                continue
            # æ„å»ºè¾“å…¥
            with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
                sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
                img_output = img_original.copy() # å¤åˆ¶å›¾ç‰‡ï¼Œç”¨äºåé¢æ·»åŠ æ ‡è®°
                tensor_in = image_to_tensor(img)
            # é¢„æµ‹è¾“å‡º
            predicted = model(tensor_in.unsqueeze(0).to(device))[0]
            final_result = MyModel.convert_predicted_result(predicted)
            # æ ‡è®°åœ¨å›¾ç‰‡ä¸Š
            draw = ImageDraw.Draw(img_output)
            for label, box, obj_score, cls_score in final_result:
                x, y, w, h = map_box_to_original_image(box, sw, sh)
                score = obj_score * cls_score
                color = "#00FF00" if CLASSES[label] == "with_mask" else "#FF0000"
                draw.rectangle((x, y, x+w, y+h), outline=color)
                draw.text((x, y-10), CLASSES[label], fill=color)
                draw.text((x, y+h), f"{score:.2f}", fill=color)
                print((x, y, w, h), CLASSES[label], obj_score, cls_score)
            img_output.save("img_output.png")
            print("saved to img_output.png")
            print()
        except Exception as e:
            print("error:", e)

def eval_video():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«è§†é¢‘"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è§†é¢‘è·¯å¾„ï¼Œç»™å¯èƒ½æ˜¯äººè„¸çš„åŒºåŸŸæ·»åŠ æ ‡è®°å¹¶ä¿å­˜æ–°è§†é¢‘
    import cv2
    font = ImageFont.truetype("FreeMonoBold.ttf", 20)
    while True:
        try:
            video_path = input("Video path: ")
            if not video_path:
                continue
            # è¯»å–è¾“å…¥è§†é¢‘
            video = cv2.VideoCapture(video_path)
            # è·å–æ¯ç§’çš„å¸§æ•°
            fps = int(video.get(cv2.CAP_PROP_FPS))
            # è·å–è§†é¢‘é•¿å®½
            size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))
            # åˆ›å»ºè¾“å‡ºè§†é¢‘
            video_output_path = os.path.join(
                os.path.dirname(video_path),
                os.path.splitext(os.path.basename(video_path))[0] + ".output.avi")
            result = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*"XVID"), fps, size)
            # ç”¨äºå‡å°‘è¯¯åˆ¤çš„å†å²ç»“æœ
            history_results = deque(maxlen = fps // 2)
            # é€å¸§å¤„ç†
            count = 0
            while(True):
                ret, frame = video.read()
                if not ret:
                    break
                # opencv ä½¿ç”¨çš„æ˜¯ BGR, Pillow ä½¿ç”¨çš„æ˜¯ RGB, éœ€è¦è½¬æ¢é€šé“é¡ºåº
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # æ„å»ºè¾“å…¥
                img_original = Image.fromarray(frame_rgb) # åŠ è½½åŸå§‹å›¾ç‰‡
                sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
                img_output = img_original.copy() # å¤åˆ¶å›¾ç‰‡ï¼Œç”¨äºåé¢æ·»åŠ æ ‡è®°
                tensor_in = image_to_tensor(img)
                # é¢„æµ‹è¾“å‡º
                predicted = model(tensor_in.unsqueeze(0).to(device))[0]
                cls_result = MyModel.convert_predicted_result(predicted)
                # æ ¹æ®å†å²ç»“æœå‡å°‘è¯¯åˆ¤
                final_result = MyModel.fix_predicted_result_from_history(cls_result, history_results)
                # æ ‡è®°åœ¨å›¾ç‰‡ä¸Š
                draw = ImageDraw.Draw(img_output)
                for label, box, obj_score, cls_score in final_result:
                    x, y, w, h = map_box_to_original_image(box, sw, sh)
                    score = obj_score * cls_score
                    color = "#00FF00" if CLASSES[label] == "with_mask" else "#FF0000"
                    draw.rectangle((x, y, x+w, y+h), outline=color, width=3)
                    draw.text((x, y-20), CLASSES[label], fill=color, font=font)
                    draw.text((x, y+h), f"{score:.2f}", fill=color, font=font)
                # å†™å…¥å¸§åˆ°è¾“å‡ºè§†é¢‘
                frame_rgb_annotated = numpy.asarray(img_output)
                frame_bgr_annotated = cv2.cvtColor(frame_rgb_annotated, cv2.COLOR_RGB2BGR)
                result.write(frame_bgr_annotated)
                count += 1
                if count % fps == 0:
                    print(f"handled {count//fps}s")
            video.release()
            result.release()
            cv2.destroyAllWindows()
            print(f"saved to {video_output_path}")
            print()
        except Exception as e:
            raise
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    elif operation == "eval-video":
        eval_video()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

é¢„å¤„ç†æ•°æ®é›†å¹¶ä¸”æ‰§è¡Œè®­ç»ƒçš„å‘½ä»¤ï¼š

``` text
python3 example.py prepare
python3 example.py train
```

è®­ç»ƒç»“æœï¼š

``` text
epoch: 42, batch: 555: batch obj accuracy: 0.9909388836542586, cls accuracy: 0.983006698089804
epoch: 42, batch: 556: batch obj accuracy: 0.9814650010596331, cls accuracy: 0.9774137503102507
epoch: 42, batch: 557: batch obj accuracy: 0.9878546962973783, cls accuracy: 0.9791485664639444
epoch: 42, batch: 558: batch obj accuracy: 0.9804549878809472, cls accuracy: 0.9869710882243454
epoch: 42, batch: 559: batch obj accuracy: 0.9874521037216837, cls accuracy: 0.9825083736509118
epoch: 42, batch: 560: batch obj accuracy: 0.9686452380905726, cls accuracy: 0.9792752544055597
epoch: 42, batch: 561: batch obj accuracy: 0.9850456887221628, cls accuracy: 0.981502172563625
epoch: 42, batch: 562: batch obj accuracy: 0.9667773027084426, cls accuracy: 0.979282967373775
epoch: 42, batch: 563: batch obj accuracy: 0.9744239536970148, cls accuracy: 0.9843711237906226
training obj accuracy: 0.9823339177948931, cls accuracy: 0.9797140932720472
validating obj accuracy: 0.9166056052234632, cls accuracy: 0.9772082398493264
stop training because highest validating accuracy not updated in 20 epoches
highest obj validating accuracy: 0.94078897076641 from epoch 21
highest cls validating accuracy: 0.9635325289895568 from epoch 21
testing obj accuracy: 0.9438541768431002, cls accuracy: 0.9637055484080282
```

çœ‹èµ·æ¥æ­£ç¡®ç‡ä¸é”™ï¼Œä½†å› ä¸º YOLO åªæœ‰å•æ­¥ï¼Œå®é™…ä¸Šå¯¹æ˜¯å¦åŒ…å«å¯¹è±¡çš„è¯¯åˆ¤ç‡æ¯” Faster-RCNN è¦é«˜ä¸€äº›ğŸ™ã€‚

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«å›¾ç‰‡çš„å‘½ä»¤ï¼Œè¾“å…¥å›¾ç‰‡è·¯å¾„å¹¶å›è½¦å³å¯ç”Ÿæˆæ ‡è®°è¿‡çš„å›¾ç‰‡ï¼š

``` text
python3 example.py eval
```

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«è§†é¢‘çš„å‘½ä»¤ï¼Œè¾“å…¥è§†é¢‘è·¯å¾„å¹¶å›è½¦å³å¯ç”Ÿæˆæ ‡è®°è¿‡çš„è§†é¢‘ï¼š

``` text
python3 example.py eval-video
```

æ ‡è®°åçš„ä¾‹å­å¦‚ä¸‹ï¼Œèƒ½ç”¨ï¼Œä½†å’Œå‰ä¸€ç¯‡ç›¸æ¯”æ•ˆæœå·®ä¸€ç‚¹ğŸ¤’ã€‚

![](wuhan.output.gif)

![](shanghai.output.gif)

## å†™åœ¨æœ€å

å¦ˆè›‹ï¼Œå†™äº†å¥½å‡ ç¯‡è¯†åˆ«äººè„¸ä½ç½®çš„æ–‡ç« ï¼Œè¿™ç¯‡æ˜¯æœ€åä¸€ç¯‡äº†ã€‚ä¸‹ä¸€ç¯‡å°†ä¼šä»‹ç»æ ¹æ®äººè„¸æ‰¾å‡ºæ˜¯å“ªä¸€ä¸ªäººçš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥å®ç°æ‰“å¡ï¼Œä¹Ÿå¯ä»¥ç”¨æ¥æŠ“é€ƒçŠ¯ğŸ˜¡ã€‚

æœ€åç¥å¤§å®¶ç‰›å¹´åŠ å·¥èµ„ï¼Œä¸­å›½è‚¡å¸‚ç‰›å¹´ç‰›é€¼ğŸ®é‡å› 6000 ç‚¹ã€‚
