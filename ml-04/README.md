# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (å››) - è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„æŠ€å·§

è¿™ç¯‡å°†ä¼šç€é‡ä»‹ç»ä½¿ç”¨ pytorch è¿›è¡Œæœºå™¨å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€äº›å¸¸è§æŠ€å·§ï¼ŒæŒæ¡å®ƒä»¬å¯ä»¥è®©ä½ äº‹åŠåŠŸå€ã€‚

ä½¿ç”¨çš„ä»£ç å¤§éƒ¨åˆ†ä¼šåŸºäºä¸Šä¸€ç¯‡æœ€åä¸€ä¸ªä¾‹å­ï¼Œå³æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„ğŸ™€ï¼Œå¦‚æœä½ æ²¡çœ‹ä¸Šä¸€ç¯‡è¯·ç‚¹å‡»[è¿™é‡Œ](https://www.cnblogs.com/zkweb/p/12761743.html)æŸ¥çœ‹ã€‚

## ä¿å­˜å’Œè¯»å–æ¨¡å‹çŠ¶æ€

åœ¨ pytorch ä¸­å„ç§æ“ä½œéƒ½æ˜¯å›´ç»• tensor å¯¹è±¡æ¥çš„ï¼Œæ¨¡å‹çš„å‚æ•°ä¹Ÿæ˜¯ tensorï¼Œå¦‚æœæˆ‘ä»¬æŠŠè®­ç»ƒå¥½çš„ tensor ä¿å­˜åˆ°ç¡¬ç›˜ç„¶åä¸‹æ¬¡å†ä»ç¡¬ç›˜è¯»å–å°±å¯ä»¥ç›´æ¥ä½¿ç”¨äº†ã€‚

æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å¦‚ä½•ä¿å­˜å•ä¸ª tensorï¼Œä»¥ä¸‹ä»£ç è¿è¡Œåœ¨ python çš„ REPL ä¸­ï¼š

``` python
# å¼•ç”¨ pytorch
>>> import torch

# æ–°å»ºä¸€ä¸ª tensor å¯¹è±¡
>>> a = torch.tensor([1, 2, 3], dtype=torch.float)

# ä¿å­˜ tensor åˆ°æ–‡ä»¶ 1.pt
>>> torch.save(a, "1.pt")

# ä»æ–‡ä»¶ 1.pt è¯»å– tensor
>>> b = torch.load("1.pt")
>>> b
tensor([1., 2., 3.])
```

torch.save ä¿å­˜ tensor çš„æ—¶å€™ä¼šä½¿ç”¨ python çš„ pickle æ ¼å¼ï¼Œè¿™ä¸ªæ ¼å¼ä¿è¯åœ¨ä¸åŒçš„ python ç‰ˆæœ¬é—´å…¼å®¹ï¼Œä½†ä¸æ”¯æŒå‹ç¼©å†…å®¹ï¼Œæ‰€ä»¥å¦‚æœ tensor éå¸¸å¤§ä¿å­˜çš„æ–‡ä»¶å°†ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¿å­˜å‰å‹ç¼©ï¼Œè¯»å–å‰è§£å‹ç¼©ä»¥å‡å°‘æ–‡ä»¶å¤§å°ï¼š

``` python
# å¼•ç”¨å‹ç¼©åº“
>>> import gzip

# ä¿å­˜ tensor åˆ°æ–‡ä»¶ 1.ptï¼Œä¿å­˜æ—¶ä½¿ç”¨ gzip å‹ç¼©
>>> torch.save(a, gzip.GzipFile("1.pt.gz", "wb"))

# ä»æ–‡ä»¶ 1.pt è¯»å– tensorï¼Œè¯»å–æ—¶ä½¿ç”¨ gzip è§£å‹ç¼©
>>> b = torch.load(gzip.GzipFile("1.pt.gz", "rb"))
>>> b
tensor([1., 2., 3.])
```

torch.save ä¸ä»…æ”¯æŒä¿å­˜å•ä¸ª tensor å¯¹è±¡ï¼Œè¿˜æ”¯æŒä¿å­˜ tensor åˆ—è¡¨æˆ–è€…è¯å…¸ (å®é™…ä¸Šå®ƒè¿˜å¯ä»¥ä¿å­˜ tensor ä»¥å¤–çš„ python å¯¹è±¡ï¼Œåªè¦ pickle æ ¼å¼æ”¯æŒ)ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ `state_dict` è·å–ä¸€ä¸ªåŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„é›†åˆï¼Œå†ç”¨ torch.save å°±å¯ä»¥ä¿å­˜æ¨¡å‹çš„çŠ¶æ€ï¼š

``` python
>>> from torch import nn
>>> class MyModel(nn.Module):
...     def __init__(self):
...         super().__init__()
...         self.layer1 = nn.Linear(in_features=8, out_features=100)
...         self.layer2 = nn.Linear(in_features=100, out_features=50)
...         self.layer3 = nn.Linear(in_features=50, out_features=1)
...     def forward(self, x):
...         hidden1 = nn.functional.relu(self.layer1(x))
...         hidden2 = nn.functional.relu(self.layer2(hidden1))
...         y = self.layer3(hidden2)
...         return y
...
>>> model = MyModel()
>>> model.state_dict()
OrderedDict([('layer1.weight', tensor([[ 0.2261,  0.2008,  0.0833, -0.2020, -0.0674,  0.2717, -0.0076,  0.1984],
        çœç•¥é€”ä¸­è¾“å‡º
          0.1347,  0.1356]])), ('layer3.bias', tensor([0.0769]))])

>>> torch.save(model.state_dict(), gzip.GzipFile("model.pt.gz", "wb"))
```

è¯»å–æ¨¡å‹çŠ¶æ€å¯ä»¥ä½¿ç”¨ `load_state_dict` å‡½æ•°ï¼Œä¸è¿‡ä½ éœ€è¦ä¿è¯æ¨¡å‹çš„å‚æ•°å®šä¹‰æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œå¦åˆ™è¯»å–ä¼šå‡ºé”™ï¼š

``` python
>>> new_model = MyModel()
>>> new_model.load_state_dict(torch.load(gzip.GzipFile("model.pt.gz", "rb")))
<All keys matched successfully>
```

ä¸€ä¸ªå¾ˆé‡è¦çš„ç»†èŠ‚æ˜¯ï¼Œå¦‚æœä½ è¯»å–æ¨¡å‹çŠ¶æ€åä¸æ˜¯å‡†å¤‡ç»§ç»­è®­ç»ƒï¼Œè€Œæ˜¯ç”¨äºé¢„æµ‹å…¶ä»–æ•°æ®ï¼Œé‚£ä¹ˆä½ åº”è¯¥è°ƒç”¨ `eval` å‡½æ•°æ¥ç¦æ­¢è‡ªåŠ¨å¾®åˆ†ç­‰åŠŸèƒ½ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«è¿ç®—é€Ÿåº¦ï¼š

``` python
>>> new_model.eval()
```

pytorch ä¸ä»…æ”¯æŒä¿å­˜å’Œè¯»å–æ¨¡å‹çŠ¶æ€ï¼Œè¿˜æ”¯æŒä¿å­˜å’Œè¯»å–æ•´ä¸ªæ¨¡å‹åŒ…æ‹¬ä»£ç å’Œå‚æ•°ï¼Œä½†æˆ‘ä¸æ¨èè¿™ç§åšæ³•ï¼Œå› ä¸ºä½¿ç”¨çš„æ—¶å€™ä¼šçœ‹ä¸åˆ°æ¨¡å‹å®šä¹‰ï¼Œå¹¶ä¸”æ¨¡å‹ä¾èµ–çš„ç±»åº“æˆ–è€…å‡½æ•°ä¸ä¼šä¸€å¹¶ä¿å­˜èµ·æ¥æ‰€ä»¥ä½ è¿˜æ˜¯å¾—é¢„å…ˆåŠ è½½å®ƒä»¬å¦åˆ™ä¼šå‡ºé”™ï¼š

``` python
>>> torch.save(model, gzip.GzipFile("model.pt.gz", "wb"))
>>> new_model = torch.load(gzip.GzipFile("model.pt.gz", "rb"))
```

## è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–

æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–ï¼Œä»¥è§‚å¯Ÿæ˜¯å¦å¯ä»¥æ”¶æ•›ï¼Œè®­ç»ƒé€Ÿåº¦å¦‚ä½•ï¼Œä»¥åŠæ˜¯å¦å‘ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä»¥ä¸‹æ˜¯ä»£ç ä¾‹å­ï¼š

``` python
# å¼•ç”¨ pytorch å’Œ pandas å’Œæ˜¾ç¤ºå›¾è¡¨ä½¿ç”¨çš„ matplotlib
import pandas
import torch
from torch import nn
from matplotlib import pyplot

# å®šä¹‰æ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=100)
        self.layer2 = nn.Linear(in_features=100, out_features=50)
        self.layer3 = nn.Linear(in_features=50, out_features=1)

    def forward(self, x):
        hidden1 = nn.functional.relu(self.layer1(x))
        hidden2 = nn.functional.relu(self.layer2(hidden1))
        y = self.layer3(hidden2)
        return y

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
# è¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
torch.random.manual_seed(0)

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModel()

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD(model.parameters(), lr=0.0000001)

# ä» csv è¯»å–åŸå§‹æ•°æ®é›†
df = pandas.read_csv('salary.csv')
dataset_tensor = torch.tensor(df.values, dtype=torch.float)

# åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
random_indices = torch.randperm(dataset_tensor.shape[0])
traning_indices = random_indices[:int(len(random_indices)*0.6)]
validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
testing_indices = random_indices[int(len(random_indices)*0.8):]
traning_set_x = dataset_tensor[traning_indices][:,:-1]
traning_set_y = dataset_tensor[traning_indices][:,-1:]
validating_set_x = dataset_tensor[validating_indices][:,:-1]
validating_set_y = dataset_tensor[validating_indices][:,-1:]
testing_set_x = dataset_tensor[testing_indices][:,:-1]
testing_set_y = dataset_tensor[testing_indices][:,-1:]

# è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
traning_accuracy_history = []
validating_accuracy_history = []

# å¼€å§‹è®­ç»ƒè¿‡ç¨‹
for epoch in range(1, 500):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.train()

    traning_accuracy_list = []
    for batch in range(0, traning_set_x.shape[0], 100):
        # åˆ‡åˆ†æ‰¹æ¬¡ï¼Œä¸€æ¬¡åªè®¡ç®— 100 ç»„æ•°æ®
        batch_x = traning_set_x[batch:batch+100]
        batch_y = traning_set_y[batch:batch+100]
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = model(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_function(predicted, batch_y)
        # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
        loss.backward()
        # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
        optimizer.step()
        # æ¸…ç©ºå¯¼å‡½æ•°å€¼
        optimizer.zero_grad()
        # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
        with torch.no_grad():
            traning_accuracy_list.append(1 - ((batch_y - predicted).abs() / batch_y).mean().item())
    traning_accuracy = sum(traning_accuracy_list) / len(traning_accuracy_list)
    traning_accuracy_history.append(traning_accuracy)
    print(f"training accuracy: {traning_accuracy}")

    # æ£€æŸ¥éªŒè¯é›†
    # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.eval()
    predicted = model(validating_set_x)
    validating_accuracy = 1 - ((validating_set_y - predicted).abs() / validating_set_y).mean()
    validating_accuracy_history.append(validating_accuracy.item())
    print(f"validating x: {validating_set_x}, y: {validating_set_y}, predicted: {predicted}")
    print(f"validating accuracy: {validating_accuracy}")

# æ£€æŸ¥æµ‹è¯•é›†
predicted = model(testing_set_x)
testing_accuracy = 1 - ((testing_set_y - predicted).abs() / testing_set_y).mean()
print(f"testing x: {testing_set_x}, y: {testing_set_y}, predicted: {predicted}")
print(f"testing accuracy: {testing_accuracy}")

# æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
pyplot.plot(traning_accuracy_history, label="traning")
pyplot.plot(validating_accuracy_history, label="validing")
pyplot.ylim(0, 1)
pyplot.legend()
pyplot.show()

# æ‰‹åŠ¨è¾“å…¥æ•°æ®é¢„æµ‹è¾“å‡º
while True:
    try:
        print("enter input:")
        r = list(map(float, input().split(",")))
        x = torch.tensor(r).view(1, len(r))
        print(model(x)[0,0].item())
    except Exception as e:
        print("error:", e)
```

ç»è¿‡ 500 è½®è®­ç»ƒåä¼šç”Ÿæˆä»¥ä¸‹çš„å›¾è¡¨ï¼š

![01](./01.png)

æˆ‘ä»¬å¯ä»¥ä»å›¾è¡¨çœ‹åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡éƒ½éšç€è®­ç»ƒé€æ¸ä¸Šå‡ï¼Œå¹¶ä¸”ä¸¤ä¸ªæ­£ç¡®ç‡éå¸¸æ¥è¿‘ï¼Œè¿™ä»£è¡¨è®­ç»ƒå¾ˆæˆåŠŸï¼Œæ¨¡å‹é’ˆå¯¹è®­ç»ƒé›†æŒæ¡äº†è§„å¾‹å¹¶ä¸”å¯ä»¥æˆåŠŸé¢„æµ‹æ²¡æœ‰ç»è¿‡è®­ç»ƒçš„éªŒè¯é›†ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬å¾ˆéš¾ä¼šçœ‹åˆ°è¿™æ ·çš„å›¾è¡¨ï¼Œè¿™æ˜¯å› ä¸ºä¾‹å­ä¸­çš„æ•°æ®é›†æ˜¯ç²¾å¿ƒæ„å»ºçš„å¹¶ä¸”ç”Ÿæˆäº†è¶³å¤Ÿå¤§é‡çš„æ•°æ®ã€‚

æˆ‘ä»¬è¿˜å¯èƒ½ä¼šçœ‹åˆ°ä»¥ä¸‹ç±»å‹çš„å›¾è¡¨ï¼Œåˆ†åˆ«ä»£è¡¨ä¸åŒçš„çŠ¶å†µï¼š

![02](./02.png)

å¦‚æœæœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ•°æ®éµä»æŸç§è§„å¾‹å¹¶ä¸”æ‚è´¨è¾ƒå°‘ï¼Œåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ—¶å€™åˆ†å¸ƒå‡åŒ€ï¼Œå¹¶ä¸”ä½¿ç”¨é€‚å½“çš„æ¨¡å‹ï¼Œå³å¯è¾¾åˆ°ç†æƒ³çš„çŠ¶å†µï¼Œä½†å®é™…å¾ˆéš¾åšåˆ°ğŸ˜©ã€‚é€šè¿‡åˆ†æè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–æˆ‘ä»¬å¯ä»¥å®šä½é—®é¢˜å‘ç”Ÿåœ¨å“ªé‡Œï¼Œå…¶ä¸­è¿‡æ‹Ÿåˆé—®é¢˜å¯ä»¥ç”¨ææ—©åœæ­¢ (Early Stopping) çš„æ–¹å¼è§£å†³ (åœ¨ç¬¬ä¸€ç¯‡æ–‡ç« å·²ç»æåˆ°è¿‡)ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å†³å®šä»€ä¹ˆæ—¶å€™åœæ­¢è®­ç»ƒã€‚

## å†³å®šä»€ä¹ˆæ—¶å€™åœæ­¢è®­ç»ƒ

è¿˜è®°å¾—ç¬¬ä¸€ç¯‡æåˆ°çš„è®­ç»ƒæµç¨‹å—ï¼Ÿæˆ‘ä»¬å°†ä¼šäº†è§£å¦‚ä½•åœ¨ä»£ç ä¸­å®ç°è¿™ä¸ªè®­ç»ƒæµç¨‹ï¼š

![03](./03.png)

å®ç°åˆ¤æ–­æ˜¯å¦å‘ç”Ÿè¿‡æ‹Ÿåˆï¼Œå¯ä»¥ç®€å•çš„è®°å½•å†å²æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ï¼Œå¦‚æœç»è¿‡å¾ˆå¤šæ¬¡è®­ç»ƒéƒ½æ²¡æœ‰åˆ·æ–°æœ€é«˜æ­£ç¡®ç‡åˆ™ç»“æŸè®­ç»ƒã€‚è®°å½•æœ€é«˜æ­£ç¡®ç‡çš„åŒæ—¶æˆ‘ä»¬è¿˜éœ€è¦ä¿å­˜æ¨¡å‹çš„çŠ¶æ€ï¼Œè¿™æ—¶æ¨¡å‹æ‘¸ç´¢åˆ°äº†è¶³å¤Ÿå¤šçš„è§„å¾‹ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰ä¿®æ”¹å‚æ•°é€‚åº”è®­ç»ƒé›†ä¸­çš„æ‚è´¨ï¼Œç”¨æ¥é¢„æµ‹æœªçŸ¥æ•°æ®å¯ä»¥è¾¾åˆ°æœ€å¥½çš„æ•ˆæœã€‚è¿™ç§æ‰‹æ³•åˆç§°ææ—©åœæ­¢ (Early Stopping)ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¸­å¾ˆå¸¸è§çš„æ‰‹æ³•ã€‚

ä»£ç å®ç°å¦‚ä¸‹ï¼š

``` python
# å¼•ç”¨ pytorch å’Œ pandas å’Œæ˜¾ç¤ºå›¾è¡¨ä½¿ç”¨çš„ matplotlib
import pandas
import torch
from torch import nn
from matplotlib import pyplot

# å®šä¹‰æ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=100)
        self.layer2 = nn.Linear(in_features=100, out_features=50)
        self.layer3 = nn.Linear(in_features=50, out_features=1)

    def forward(self, x):
        hidden1 = nn.functional.relu(self.layer1(x))
        hidden2 = nn.functional.relu(self.layer2(hidden1))
        y = self.layer3(hidden2)
        return y

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
# è¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
torch.random.manual_seed(0)

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModel()

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD(model.parameters(), lr=0.0000001)

# ä» csv è¯»å–åŸå§‹æ•°æ®é›†
df = pandas.read_csv('salary.csv')
dataset_tensor = torch.tensor(df.values, dtype=torch.float)

# åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
random_indices = torch.randperm(dataset_tensor.shape[0])
traning_indices = random_indices[:int(len(random_indices)*0.6)]
validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
testing_indices = random_indices[int(len(random_indices)*0.8):]
traning_set_x = dataset_tensor[traning_indices][:,:-1]
traning_set_y = dataset_tensor[traning_indices][:,-1:]
validating_set_x = dataset_tensor[validating_indices][:,:-1]
validating_set_y = dataset_tensor[validating_indices][:,-1:]
testing_set_x = dataset_tensor[testing_indices][:,:-1]
testing_set_y = dataset_tensor[testing_indices][:,-1:]

# è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
traning_accuracy_history = []
validating_accuracy_history = []

# è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
validating_accuracy_highest = 0
validating_accuracy_highest_epoch = 0

# å¼€å§‹è®­ç»ƒè¿‡ç¨‹
for epoch in range(1, 10000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.train()

    traning_accuracy_list = []
    for batch in range(0, traning_set_x.shape[0], 100):
        # åˆ‡åˆ†æ‰¹æ¬¡ï¼Œä¸€æ¬¡åªè®¡ç®— 100 ç»„æ•°æ®
        batch_x = traning_set_x[batch:batch+100]
        batch_y = traning_set_y[batch:batch+100]
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = model(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_function(predicted, batch_y)
        # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
        loss.backward()
        # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
        optimizer.step()
        # æ¸…ç©ºå¯¼å‡½æ•°å€¼
        optimizer.zero_grad()
        # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
        with torch.no_grad():
            traning_accuracy_list.append(1 - ((batch_y - predicted).abs() / batch_y).mean().item())
    traning_accuracy = sum(traning_accuracy_list) / len(traning_accuracy_list)
    traning_accuracy_history.append(traning_accuracy)
    print(f"training accuracy: {traning_accuracy}")

    # æ£€æŸ¥éªŒè¯é›†
    # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.eval()
    predicted = model(validating_set_x)
    validating_accuracy = 1 - ((validating_set_y - predicted).abs() / validating_set_y).mean()
    validating_accuracy_history.append(validating_accuracy.item())
    print(f"validating x: {validating_set_x}, y: {validating_set_y}, predicted: {predicted}")
    print(f"validating accuracy: {validating_accuracy}")

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
    if validating_accuracy > validating_accuracy_highest:
        validating_accuracy_highest = validating_accuracy
        validating_accuracy_highest_epoch = epoch
        torch.save(model.state_dict(), "model.pt")
        print("highest validating accuracy updated")
    elif epoch - validating_accuracy_highest_epoch > 100:
        # åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
        print("stop training because highest validating accuracy not updated in 100 epoches")
        break

# ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
print(f"highest validating accuracy: {validating_accuracy_highest}",
    f"from epoch {validating_accuracy_highest_epoch}")
model.load_state_dict(torch.load("model.pt"))

# æ£€æŸ¥æµ‹è¯•é›†
predicted = model(testing_set_x)
testing_accuracy = 1 - ((testing_set_y - predicted).abs() / testing_set_y).mean()
print(f"testing x: {testing_set_x}, y: {testing_set_y}, predicted: {predicted}")
print(f"testing accuracy: {testing_accuracy}")

# æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
pyplot.plot(traning_accuracy_history, label="traning")
pyplot.plot(validating_accuracy_history, label="validing")
pyplot.ylim(0, 1)
pyplot.legend()
pyplot.show()

# æ‰‹åŠ¨è¾“å…¥æ•°æ®é¢„æµ‹è¾“å‡º
while True:
    try:
        print("enter input:")
        r = list(map(float, input().split(",")))
        x = torch.tensor(r).view(1, len(r))
        print(model(x)[0,0].item())
    except Exception as e:
        print("error:", e)
```

æœ€ç»ˆè¾“å‡ºå¦‚ä¸‹ï¼š

``` text
çœç•¥å¼€å§‹çš„è¾“å‡º

stop training because highest validating accuracy not updated in 100 epoches
highest validating accuracy: 0.93173748254776 from epoch 645
testing x: tensor([[48.,  1., 18.,  ...,  5.,  0.,  5.],
        [22.,  1.,  2.,  ...,  2.,  1.,  2.],
        [24.,  0.,  1.,  ...,  3.,  2.,  0.],
        ...,
        [24.,  0.,  4.,  ...,  0.,  1.,  1.],
        [39.,  0.,  0.,  ...,  0.,  5.,  5.],
        [36.,  0.,  5.,  ...,  3.,  0.,  3.]]), y: tensor([[14000.],
        [10500.],
        [13000.],
        ...,
        [15500.],
        [12000.],
        [19000.]]), predicted: tensor([[15612.1895],
        [10705.9873],
        [12577.7988],
        ...,
        [16281.9277],
        [10780.5996],
        [19780.3281]], grad_fn=<AddmmBackward>)
testing accuracy: 0.9330222606658936
```

è®­ç»ƒé›†ä¸éªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–å¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬åœåœ¨äº†ä¸€ä¸ªå¾ˆå¥½çš„åœ°æ–¹ğŸ˜¸ï¼Œç»§ç»­è®­ç»ƒä¸‹å»ä¹Ÿä¸ä¼šæœ‰ä»€ä¹ˆæ”¹è¿›ï¼š

![04](./04.png)

## æ”¹è¿›ç¨‹åºç»“æ„

æˆ‘ä»¬è¿˜å¯ä»¥å¯¹ç¨‹åºç»“æ„è¿›è¡Œä»¥ä¸‹çš„æ”¹è¿›ï¼š

- åˆ†ç¦»å‡†å¤‡æ•°æ®é›†å’Œè®­ç»ƒçš„è¿‡ç¨‹
- è®­ç»ƒè¿‡ç¨‹ä¸­åˆ†æ‰¹è¯»å–æ•°æ®
- æä¾›æ¥å£ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

è‡³æ­¤ä¸ºæ­¢æˆ‘ä»¬çœ‹åˆ°çš„è®­ç»ƒä»£ç éƒ½æ˜¯æŠŠå‡†å¤‡æ•°æ®é›†ï¼Œè®­ç»ƒï¼Œè®­ç»ƒåè¯„ä»·å’Œä½¿ç”¨å†™åœ¨ä¸€ä¸ªç¨‹åºé‡Œé¢çš„ï¼Œè¿™æ ·åšå®¹æ˜“ç†è§£ä½†åœ¨å®é™…ä¸šåŠ¡ä¸­ä¼šæ¯”è¾ƒæµªè´¹æ—¶é—´ï¼Œå¦‚æœä½ å‘ç°ä¸€ä¸ªæ¨¡å‹ä¸é€‚åˆï¼Œéœ€è¦ä¿®æ”¹æ¨¡å‹é‚£ä¹ˆä½ å¾—ä»å¤´å¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥åˆ†ç¦»å‡†å¤‡æ•°æ®é›†å’Œè®­ç»ƒçš„è¿‡ç¨‹ï¼Œé¦–å…ˆè¯»å–åŸå§‹æ•°æ®å¹¶ä¸”è½¬æ¢åˆ° tensor å¯¹è±¡å†ä¿å­˜åˆ°ç¡¬ç›˜ï¼Œç„¶åå†ä»ç¡¬ç›˜è¯»å– tensor å¯¹è±¡è¿›è¡Œè®­ç»ƒï¼Œè¿™æ ·å¦‚æœéœ€è¦ä¿®æ”¹æ¨¡å‹ä½†ä¸éœ€è¦ä¿®æ”¹è¾“å…¥è¾“å‡ºè½¬æ¢åˆ° tensor çš„ç¼–ç æ—¶ï¼Œå¯ä»¥èŠ‚çœæ‰ç¬¬ä¸€æ­¥ã€‚

åœ¨å®é™…ä¸šåŠ¡ä¸Šæ•°æ®å¯èƒ½ä¼šéå¸¸åºå¤§ï¼Œåšä¸åˆ°å…¨éƒ¨è¯»å–åˆ°å†…å­˜ä¸­å†åˆ†æ‰¹æ¬¡ï¼Œè¿™æ—¶æˆ‘ä»¬å¯ä»¥åœ¨è¯»å–åŸå§‹æ•°æ®å¹¶ä¸”è½¬æ¢åˆ° tensor å¯¹è±¡çš„æ—¶å€™è¿›è¡Œåˆ†æ‰¹ï¼Œç„¶åè®­ç»ƒçš„è¿‡ç¨‹ä¸­é€æ‰¹ä»ç¡¬ç›˜è¯»å–ï¼Œè¿™æ ·å°±å¯ä»¥é˜²æ­¢å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚

æœ€åæˆ‘ä»¬å¯ä»¥æä¾›ä¸€ä¸ªå¯¹å¤–çš„æ¥å£æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¦‚æœä½ çš„ç¨‹åºæ˜¯ python å†™çš„é‚£ä¹ˆç›´æ¥è°ƒç”¨å³å¯ï¼Œä½†å¦‚æœä½ çš„ç¨‹åºæ˜¯å…¶ä»–è¯­è¨€å†™çš„ï¼Œå¯èƒ½éœ€è¦å…ˆå»ºç«‹ä¸€ä¸ª python æœåŠ¡å™¨æä¾› REST æœåŠ¡ï¼Œæˆ–è€…ä½¿ç”¨ TorchScript è¿›è¡Œè·¨è¯­è¨€äº¤äº’ï¼Œè¯¦ç»†å¯ä»¥å‚è€ƒå®˜æ–¹çš„[æ•™ç¨‹](https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html)ã€‚

æ€»ç»“èµ·æ¥æˆ‘ä»¬ä¼šæ‹†åˆ†ä»¥ä¸‹è¿‡ç¨‹ï¼š

- è¯»å–åŸå§‹æ•°æ®é›†å¹¶è½¬æ¢åˆ° tensor å¯¹è±¡
    - åˆ†æ‰¹æ¬¡ä¿å­˜ tensor å¯¹è±¡åˆ°ç¡¬ç›˜
- åˆ†æ‰¹æ¬¡ä»ç¡¬ç›˜è¯»å– tensor å¯¹è±¡å¹¶è¿›è¡Œè®­ç»ƒ
    - è®­ç»ƒæ—¶ä¿å­˜æ¨¡å‹çŠ¶æ€åˆ°ç¡¬ç›˜ (ä¸€èˆ¬é€‰æ‹©ä¿å­˜éªŒè¯é›†æ­£ç¡®ç‡æœ€é«˜æ—¶çš„æ¨¡å‹çŠ¶æ€)
- æä¾›æ¥å£ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

ä»¥ä¸‹æ˜¯æ”¹è¿›åçš„ç¤ºä¾‹ä»£ç ï¼š

``` python
import os
import sys
import pandas
import torch
import gzip
import itertools
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=100)
        self.layer2 = nn.Linear(in_features=100, out_features=50)
        self.layer3 = nn.Linear(in_features=50, out_features=1)

    def forward(self, x):
        hidden1 = nn.functional.relu(self.layer1(x))
        hidden2 = nn.functional.relu(self.layer2(hidden1))
        y = self.layer3(hidden2)
        return y

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # ä» csv è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡è¯»å– 2000 è¡Œ
    for batch, df in enumerate(pandas.read_csv('salary.csv', chunksize=2000)):
        dataset_tensor = torch.tensor(df.values, dtype=torch.float)

        # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
        random_indices = torch.randperm(dataset_tensor.shape[0])
        traning_indices = random_indices[:int(len(random_indices)*0.6)]
        validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
        testing_indices = random_indices[int(len(random_indices)*0.8):]
        training_set = dataset_tensor[traning_indices]
        validating_set = dataset_tensor[validating_indices]
        testing_set = dataset_tensor[testing_indices]

        # ä¿å­˜åˆ°ç¡¬ç›˜
        save_tensor(training_set, f"data/training_set.{batch}.pt")
        save_tensor(validating_set, f"data/validating_set.{batch}.pt")
        save_tensor(testing_set, f"data/testing_set.{batch}.pt")
        print(f"batch {batch} saved")

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel()

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.SGD(model.parameters(), lr=0.0000001)

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    traning_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return max(0, 1 - ((actual - predicted).abs() / actual.abs()).mean().item())

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        traning_accuracy_list = []
        for batch in read_batches("data/training_set"):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
             for index in range(0, batch.shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
                batch_x = batch[index:index+100,:-1]
                batch_y = batch[index:index+100,-1:]
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x)
                # è®¡ç®—æŸå¤±
                loss = loss_function(predicted, batch_y)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    traning_accuracy_list.append(calc_accuracy(batch_y, predicted))
        traning_accuracy = sum(traning_accuracy_list) / len(traning_accuracy_list)
        traning_accuracy_history.append(traning_accuracy)
        print(f"training accuracy: {traning_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            validating_accuracy_list.append(calc_accuracy(batch[:,-1:],  model(batch[:,:-1])))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 100:
            # åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 100 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        testing_accuracy_list.append(calc_accuracy(batch[:,-1:],  model(batch[:,:-1])))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(traning_accuracy_history, label="traning")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    parameters = [
        "Age",
        "Gender (0: Male, 1: Female)",
        "Years of work experience",
        "Java Skill (0 ~ 5)",
        "NET Skill (0 ~ 5)",
        "JS Skill (0 ~ 5)",
        "CSS Skill (0 ~ 5)",
        "HTML Skill (0 ~ 5)"
    ]

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel()
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    while True:
        try:
            x = torch.tensor([int(input(f"Your {p}: ")) for p in parameters], dtype=torch.float)
            # è½¬æ¢åˆ° 1 è¡Œ 1 åˆ—çš„çŸ©é˜µï¼Œè¿™é‡Œå…¶å®å¯ä»¥ä¸è½¬æ¢ä½†æ¨èè¿™ä¹ˆåšï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒéæ‰¹æ¬¡è¾“å…¥
            x = x.view(1, len(x))
            y = model(x)
            print("Your estimated salary:", y[0,0].item(), "\n")
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯èµ°ä¸€éå®Œæ•´çš„æµç¨‹ï¼Œå¦‚æœä½ éœ€è¦è°ƒæ•´æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥é‡æ–°è¿è¡Œ train é¿å… prepare çš„æ—¶é—´æ¶ˆè€—ï¼š

``` text
python3 example.py prepare
python3 example.py train
python3 example.py eval
```

æ³¨æ„ä»¥ä¸Šä»£ç åœ¨æ‰“ä¹±æ•°æ®é›†å’Œåˆ†æ‰¹çš„å¤„ç†ä¸Šä¸ä»¥å¾€çš„ä»£ç ä¸ä¸€æ ·ï¼Œä»¥ä¸Šçš„ä»£ç ä¼šåˆ†æ®µè¯»å– csv æ–‡ä»¶ï¼Œç„¶åå¯¹æ¯ä¸€æ®µæ‰“ä¹±å†åˆ‡åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œè¿™æ ·åšåŒæ ·å¯ä»¥ä¿è¯æ•°æ®åœ¨å„ä¸ªé›†åˆä¸­åˆ†å¸ƒå‡åŒ€ã€‚æœ€ç»ˆè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–å¦‚ä¸‹ï¼š

![05](./05.png)

## æ­£è§„åŒ–è¾“å…¥å’Œè¾“å‡ºå€¼

ç›®å‰ä¸ºæ­¢æˆ‘ä»¬åœ¨è®­ç»ƒçš„æ—¶å€™éƒ½æ˜¯ç›´æ¥ç»™æ¨¡å‹åŸå§‹çš„è¾“å…¥å€¼ï¼Œç„¶åç”¨åŸå§‹çš„è¾“å‡ºå€¼å»è°ƒæ•´å‚æ•°ï¼Œè¿™æ ·åšçš„é—®é¢˜æ˜¯ï¼Œå¦‚æœè¾“å…¥å€¼éå¸¸å¤§å¯¼å‡½æ•°å€¼ä¹Ÿä¼šéå¸¸å¤§ï¼Œå¦‚æœè¾“å‡ºå€¼éå¸¸å¤§éœ€è¦è°ƒæ•´å‚æ•°çš„æ¬¡æ•°ä¼šéå¸¸å¤šï¼Œè¿‡å»æˆ‘ä»¬ç”¨ä¸€ä¸ªéå¸¸éå¸¸å°çš„å­¦ä¹ æ¯”ç‡ (0.0000001) æ¥é¿å¼€è¿™ä¸ªé—®é¢˜ï¼Œä½†å…¶å®æœ‰æ›´å¥½çš„åŠæ³•ï¼Œé‚£å°±æ˜¯æ­£è§„åŒ–è¾“å…¥å’Œè¾“å‡ºå€¼ã€‚è¿™é‡Œçš„æ­£è§„åŒ–æŒ‡çš„æ˜¯è®©è¾“å…¥å€¼å’Œè¾“å‡ºå€¼æŒ‰ä¸€å®šæ¯”ä¾‹ç¼©æ”¾ï¼Œè®©å¤§éƒ¨åˆ†çš„å€¼éƒ½è½åœ¨ -1 ~ 1 çš„åŒºé—´ä¸­ã€‚åœ¨æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¹´é¾„å’Œå·¥ä½œç»éªŒå¹´æ•°ä¹˜ä»¥ 0.01 (èŒƒå›´ 0 ~ 100 å¹´)ï¼Œå„é¡¹æŠ€èƒ½ä¹˜ä»¥ 0.02 (èŒƒå›´ 0 ~ 5)ï¼Œå·¥èµ„ä¹˜ä»¥ 0.0001 (ä»¥ä¸‡ä¸ºå•ä½)ï¼Œå¯¹ `dataset_tensor` è¿›è¡Œä»¥ä¸‹æ“ä½œå³å¯å®ç°ï¼š

``` python
# å¯¹æ¯ä¸€è¡Œä¹˜ä»¥æŒ‡å®šçš„ç³»æ•°
dataset_tensor *= torch.tensor([0.01, 1, 0.01, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0001])
```

ç„¶åå†ä¿®æ”¹å­¦ä¹ æ¯”ç‡ä¸º 0.01:

``` python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

æ¯”è¾ƒè®­ç»ƒ 300 æ¬¡çš„æ­£ç¡®ç‡å˜åŒ–å¦‚ä¸‹ï¼š

æ­£è§„åŒ–è¾“å…¥å’Œè¾“å‡ºå€¼å‰

![06](./06.png)

æ­£è§„åŒ–è¾“å…¥å’Œè¾“å‡ºå€¼å

![07](./07.png)

å¯ä»¥çœ‹åˆ°æ•ˆæœç›¸å½“æƒŠäººğŸ˜ˆï¼Œæ­£è§„åŒ–è¾“å…¥å’Œè¾“å‡ºå€¼åè®­ç»ƒé€Ÿåº¦å˜å¿«äº†å¹¶ä¸”æ­£ç¡®ç‡çš„å˜åŒ–æ›²çº¿å¹³æ»‘äº†å¾ˆå¤šã€‚å®é™…ä¸Šè¿™æ˜¯å¿…é¡»åšçš„ï¼Œéƒ¨åˆ†æ•°æ®é›†å¦‚æœæ²¡æœ‰ç»è¿‡æ­£è§„åŒ–æ ¹æœ¬æ— æ³•å­¦ä¹ ï¼Œè®©æ¨¡å‹æ¥æ”¶å’Œè¾“å‡ºæ›´å°çš„å€¼ (-1 ~ 1 çš„åŒºé—´) å¯ä»¥é˜²æ­¢å¯¼å‡½æ•°å€¼çˆ†ç‚¸å’Œä½¿ç”¨æ›´é«˜çš„å­¦ä¹ æ¯”ç‡åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

æ­¤å¤–ï¼Œåˆ«å¿˜äº†åœ¨ä½¿ç”¨æ¨¡å‹çš„æ—¶å€™ç¼©æ”¾è¾“å…¥å’Œè¾“å‡ºå€¼ï¼š

``` python
x = torch.tensor([int(input(f"Your {p}: ")) for p in parameters], dtype=torch.float)
x *= torch.tensor([0.01, 1, 0.01, 0.2, 0.2, 0.2, 0.2, 0.2])
# è½¬æ¢åˆ° 1 è¡Œ 1 åˆ—çš„çŸ©é˜µï¼Œè¿™é‡Œå…¶å®å¯ä»¥ä¸è½¬æ¢ä½†æ¨èè¿™ä¹ˆåšï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒéæ‰¹æ¬¡è¾“å…¥
x = x.view(1, len(x))
y = model(x) * 10000
print("Your estimated salary:", y[0,0].item(), "\n")
```

## ä½¿ç”¨ Dropout å¸®åŠ©æ³›åŒ–æ¨¡å‹

åœ¨ä¹‹å‰çš„å†…å®¹ä¸­å·²ç»æåˆ°è¿‡ï¼Œå¦‚æœæ¨¡å‹èƒ½åŠ›è¿‡äºå¼ºå¤§æˆ–è€…æ•°æ®æ‚è´¨è¾ƒå¤šï¼Œåˆ™æ¨¡å‹æœ‰å¯èƒ½ä¼šé€‚åº”æ•°æ®ä¸­çš„æ‚è´¨ä»¥è¾¾åˆ°æ›´é«˜çš„æ­£ç¡®ç‡ (è¿‡æ‹Ÿåˆç°è±¡)ï¼Œè¿™æ—¶å€™è™½ç„¶è®­ç»ƒé›†çš„æ­£ç¡®ç‡ä¼šä¸Šå‡ï¼Œä½†éªŒè¯é›†çš„æ­£ç¡®ç‡ä¼šç»´æŒç”šè‡³ä¸‹é™ï¼Œæ¨¡å‹åº”å¯¹æœªçŸ¥æ•°æ®çš„èƒ½åŠ›ä¼šé™ä½ã€‚é˜²æ­¢è¿‡æ‹Ÿåˆç°è±¡ï¼Œå¢å¼ºæ¨¡å‹åº”å¯¹æœªçŸ¥æ•°æ®çš„èƒ½åŠ›åˆç§°æ³›åŒ–æ¨¡å‹ (Generalize Model)ï¼Œæ³›åŒ–æ¨¡å‹çš„æ‰‹æ®µä¹‹ä¸€æ˜¯ä½¿ç”¨ Dropoutï¼ŒDropout ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºå±è”½ä¸€éƒ¨åˆ†çš„ç¥ç»å…ƒï¼Œè®©è¿™äº›ç¥ç»å…ƒçš„è¾“å‡ºä¸º 0ï¼ŒåŒæ—¶å¢å¹…æ²¡æœ‰è¢«å±è”½çš„ç¥ç»å…ƒè¾“å‡ºè®©è¾“å‡ºå€¼åˆè®¡æ¥è¿‘åŸæœ‰çš„æ°´å¹³ï¼Œè¿™æ ·åšçš„å¥½å¤„æ˜¯æ¨¡å‹ä¼šå°è¯•æ‘¸ç´¢æ€æ ·åœ¨ä¸€éƒ¨åˆ†ç¥ç»å…ƒè¢«å±è”½åä»ç„¶å¯ä»¥æ­£ç¡®é¢„æµ‹ç»“æœ (å‡å¼±è·¨å±‚ç¥ç»å…ƒä¹‹é—´çš„å…³è”)ï¼Œæœ€ç»ˆå¯¼è‡´æ¨¡å‹æ›´å……åˆ†çš„æŒæ¡æ•°æ®çš„è§„å¾‹ã€‚

ä¸‹å›¾æ˜¯ä½¿ç”¨ Dropout ä»¥åçš„ç¥ç»å…ƒç½‘ç»œä¾‹å­ (3 è¾“å…¥ 2 è¾“å‡ºï¼Œ3 å±‚æ¯å±‚å„ 5 éšè—å€¼)ï¼š

![10](./10.png)

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹åœ¨ Pytorch ä¸­æ€ä¹ˆä½¿ç”¨ Dropoutï¼š

``` python
# å¼•ç”¨ pytorch ç±»åº“
>>> import torch

# åˆ›å»ºå±è”½ 20% çš„ Dropout å‡½æ•°
>>> dropout = torch.nn.Dropout(0.2)

# å®šä¹‰ä¸€ä¸ª tensor (å‡è®¾è¿™ä¸ª tensor æ˜¯æŸä¸ªç¥ç»å…ƒç½‘ç»œå±‚çš„è¾“å‡ºç»“æœ)
>>> a = torch.tensor(range(1, 11), dtype=torch.float)
>>> a
tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])

# åº”ç”¨ Dropout å‡½æ•°
# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ²¡æœ‰å±è”½çš„å€¼éƒ½ä¼šç›¸åº”çš„å¢åŠ  (é™¤ä»¥ 0.8) ä»¥è®©åˆè®¡å€¼ç»´æŒåŸæœ‰çš„æ°´å¹³
# æ­¤å¤–å±è”½çš„æ•°é‡ä¼šæ ¹æ®æ¦‚ç‡æµ®åŠ¨ï¼Œä¸ä¸€å®š 100% ç­‰äºæˆ‘ä»¬è®¾ç½®çš„æ¯”ä¾‹ (è¿™é‡Œæœ‰å±è”½ 1 ä¸ªå€¼çš„ä¹Ÿæœ‰å±è”½ 3 ä¸ªå€¼çš„)
>>> dropout(a)
tensor([ 0.0000,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  8.7500, 10.0000,
        11.2500, 12.5000])
>>> dropout(a)
tensor([ 1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  8.7500,  0.0000,
        11.2500,  0.0000])
>>> dropout(a)
tensor([ 1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  8.7500,  0.0000,
        11.2500, 12.5000])
>>> dropout(a)
tensor([ 1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  0.0000, 10.0000,
        11.2500,  0.0000])
>>> dropout(a)
tensor([ 1.2500,  2.5000,  3.7500,  5.0000,  0.0000,  7.5000,  8.7500, 10.0000,
        11.2500,  0.0000])
>>> dropout(a)
tensor([ 1.2500,  2.5000,  0.0000,  5.0000,  0.0000,  7.5000,  8.7500, 10.0000,
        11.2500, 12.5000])
>>> dropout(a)
tensor([ 0.0000,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  0.0000, 10.0000,
         0.0000,  0.0000])
```

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æ€æ ·åº”ç”¨ Dropout åˆ°æ¨¡å‹ä¸­ï¼Œé¦–å…ˆæˆ‘ä»¬é‡ç°ä¸€ä¸‹è¿‡æ‹Ÿåˆç°è±¡ï¼Œå¢åŠ æ¨¡å‹çš„ç¥ç»å…ƒæ•°é‡å¹¶ä¸”å‡å°‘è®­ç»ƒé›†çš„æ•°æ®é‡å³å¯ï¼š

æ¨¡å‹éƒ¨åˆ†çš„ä»£ç ï¼š

``` python
class MyModel(nn.Module):
    """æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=200)
        self.layer2 = nn.Linear(in_features=200, out_features=100)
        self.layer3 = nn.Linear(in_features=100, out_features=1)

    def forward(self, x):
        hidden1 = nn.functional.relu(self.layer1(x))
        hidden2 = nn.functional.relu(self.layer2(hidden1))
        y = self.layer3(hidden2)
        return y
```

è®­ç»ƒéƒ¨åˆ†çš„ä»£ç  (æ¯ä¸ªæ‰¹æ¬¡åªè®­ç»ƒå‰ 16 ä¸ªæ•°æ®)ï¼š

``` python
for batch in read_batches("data/training_set"):
    # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
     for index in range(0, batch.shape[0], 16):
        # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
        batch_x = batch[index:index+16,:-1]
        batch_y = batch[index:index+16,-1:]
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = model(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_function(predicted, batch_y)
        # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
        loss.backward()
        # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
        optimizer.step()
        # æ¸…ç©ºå¯¼å‡½æ•°å€¼
        optimizer.zero_grad()
        # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
        with torch.no_grad():
            traning_accuracy_list.append(calc_accuracy(batch_y, predicted))
        # åªè®­ç»ƒå‰ 16 ä¸ªæ•°æ®
        break
```

å›ºå®šè®­ç»ƒ 1000 æ¬¡ä»¥åçš„æ­£ç¡®ç‡ï¼š

``` text
training accuracy: 0.9706422178819776
validating accuracy: 0.8514168351888657
highest validating accuracy: 0.8607834208011628 from epoch 223
testing accuracy: 0.8603586450219154
```

ä»¥åŠæ­£ç¡®ç‡å˜åŒ–çš„è¶‹åŠ¿ï¼š

![08](./08.png)

è¯•ç€åœ¨æ¨¡å‹ä¸­åŠ å…¥ä¸¤ä¸ª Dropoutï¼Œåˆ†åˆ«å¯¹åº”ç¬¬ä¸€å±‚ä¸ç¬¬äºŒå±‚çš„è¾“å‡º (éšè—å€¼)ï¼š

``` python
class MyModel(nn.Module):
    """æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=200)
        self.layer2 = nn.Linear(in_features=200, out_features=100)
        self.layer3 = nn.Linear(in_features=100, out_features=1)
        self.dropout1 = nn.Dropout(0.2)
        self.dropout2 = nn.Dropout(0.2)

    def forward(self, x):
        hidden1 = self.dropout1(nn.functional.relu(self.layer1(x)))
        hidden2 = self.dropout2(nn.functional.relu(self.layer2(hidden1)))
        y = self.layer3(hidden2)
        return y
```

è¿™æ—¶å€™å†æ¥è®­ç»ƒä¼šå¾—å‡ºä»¥ä¸‹çš„æ­£ç¡®ç‡ï¼š

``` text
training accuracy: 0.9326518730819225
validating accuracy: 0.8692235469818115
highest validating accuracy: 0.8728838726878166 from epoch 867
testing accuracy: 0.8733032837510109
```

ä»¥åŠæ­£ç¡®ç‡å˜åŒ–çš„è¶‹åŠ¿ï¼š

![09](./09.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è®­ç»ƒé›†çš„æ­£ç¡®ç‡æ²¡æœ‰ç›²ç›®çš„ä¸Šå‡ï¼Œå¹¶ä¸”éªŒè¯é›†ä¸æµ‹è¯•é›†çš„æ­£ç¡®ç‡éƒ½å„ä¸Šå‡äº† 1% ä»¥ä¸Šï¼Œè¯´æ˜ Dropout æ˜¯æœ‰ä¸€å®šæ•ˆæœçš„ã€‚

ä½¿ç”¨ Dropout æ—¶åº”è¯¥æ³¨æ„ä»¥ä¸‹çš„å‡ ç‚¹ï¼š

- Dropout åº”è¯¥é’ˆå¯¹éšè—å€¼ä½¿ç”¨ï¼Œä¸èƒ½æ”¾åœ¨ç¬¬ä¸€å±‚çš„å‰é¢ (é’ˆå¯¹è¾“å…¥) æˆ–è€…æœ€åä¸€å±‚çš„åé¢ (é’ˆå¯¹è¾“å‡º)
- Dropout åº”è¯¥æ”¾åœ¨æ¿€æ´»å‡½æ•°åé¢ (å› ä¸ºæ¿€æ´»å‡½æ•°æ˜¯ç¥ç»å…ƒçš„ä¸€éƒ¨åˆ†)
- Dropout åªåº”è¯¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œè¯„ä»·æˆ–å®é™…ä½¿ç”¨æ¨¡å‹æ—¶åº”è¯¥è°ƒç”¨ `model.eval()` åˆ‡æ¢æ¨¡å‹åˆ°è¯„ä»·æ¨¡å¼ï¼Œä»¥ç¦æ­¢ Dropout
- Dropout å‡½æ•°åº”è¯¥å®šä¹‰ä¸ºæ¨¡å‹çš„æˆå‘˜ï¼Œè¿™æ ·è°ƒç”¨ `model.eval()` å¯ä»¥ç´¢å¼•åˆ°æ¨¡å‹å¯¹åº”çš„æ‰€æœ‰ Dropout å‡½æ•°
- Dropout çš„å±è”½æ¯”ä¾‹æ²¡æœ‰æœ€ä½³å€¼ï¼Œä½ å¯ä»¥é’ˆå¯¹å½“å‰çš„æ•°æ®å’Œæ¨¡å‹å¤šè¯•å‡ æ¬¡æ‰¾å‡ºæœ€å¥½çš„ç»“æœ

æå‡º Dropout æ‰‹æ³•çš„åŸå§‹è®ºæ–‡åœ¨[è¿™é‡Œ](https://arxiv.org/abs/1207.0580)ï¼Œå¦‚æœä½ æœ‰å…´è¶£å¯ä»¥æŸ¥çœ‹ã€‚

## ä½¿ç”¨ BatchNorm æ­£è§„åŒ–æ‰¹æ¬¡

BatchNorm æ˜¯å¦å¤–ä¸€ç§æå‡è®­ç»ƒæ•ˆæœçš„æ‰‹æ³•ï¼Œåœ¨ä¸€äº›åœºæ™¯ä¸‹å¯ä»¥æå‡è®­ç»ƒæ•ˆç‡å’ŒæŠ‘åˆ¶è¿‡æ‹Ÿåˆï¼ŒBatchNorm å’Œ Dropout ä¸€æ ·é’ˆå¯¹éšè—å€¼ä½¿ç”¨ï¼Œä¼šå¯¹æ¯ä¸ªæ‰¹æ¬¡çš„å„é¡¹å€¼ (æ¯ä¸€åˆ—) è¿›è¡Œæ­£è§„åŒ–ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

![11](./11.png)

æ€»ç»“æ¥è¯´å°±æ˜¯è®©æ¯ä¸€åˆ—ä¸­çš„å„ä¸ªå€¼å‡å»è¿™ä¸€åˆ—çš„å¹³å‡å€¼ï¼Œç„¶åé™¤ä»¥è¿™ä¸€åˆ—çš„æ ‡å‡†å·®ï¼Œå†æŒ‰ä¸€å®šæ¯”ä¾‹è°ƒæ•´ã€‚

åœ¨ python ä¸­ä½¿ç”¨ BatchNorm çš„ä¾‹å­å¦‚ä¸‹ï¼š

``` python
# åˆ›å»º batchnorm å‡½æ•°ï¼Œ3 ä»£è¡¨åˆ—æ•°
>>> batchnorm = torch.nn.BatchNorm1d(3)

# æŸ¥çœ‹ batchnorm å‡½æ•°å†…éƒ¨çš„æƒé‡ä¸åç§»
>>> list(batchnorm.parameters())
[Parameter containing:
tensor([1., 1., 1.], requires_grad=True), Parameter containing:
tensor([0., 0., 0.], requires_grad=True)]

# éšæœºåˆ›å»ºä¸€ä¸ª 10 è¡Œ 3 åˆ—çš„ tensor
>>> a = torch.rand((10, 3))
>>> a
tensor([[0.9643, 0.6933, 0.0039],
        [0.3967, 0.8239, 0.3490],
        [0.4011, 0.8903, 0.3053],
        [0.0666, 0.5766, 0.4976],
        [0.4928, 0.1403, 0.8900],
        [0.7317, 0.9461, 0.1816],
        [0.4461, 0.9987, 0.8324],
        [0.3714, 0.6550, 0.9961],
        [0.4852, 0.7415, 0.1779],
        [0.6876, 0.1538, 0.3429]])

# åº”ç”¨ batchnorm å‡½æ•°
>>> batchnorm(a)
tensor([[ 1.9935,  0.1096, -1.4156],
        [-0.4665,  0.5665, -0.3391],
        [-0.4477,  0.7985, -0.4754],
        [-1.8972, -0.2986,  0.1246],
        [-0.0501, -1.8245,  1.3486],
        [ 0.9855,  0.9939, -0.8611],
        [-0.2523,  1.1776,  1.1691],
        [-0.5761, -0.0243,  1.6798],
        [-0.0831,  0.2783, -0.8727],
        [ 0.7941, -1.7770, -0.3581]], grad_fn=<NativeBatchNormBackward>)

# æ‰‹åŠ¨é‡ç° batchnorm å¯¹ç¬¬ä¸€åˆ—çš„è®¡ç®—
>>> aa = a[:,:1]
>>> aa
tensor([[0.9643],
        [0.3967],
        [0.4011],
        [0.0666],
        [0.4928],
        [0.7317],
        [0.4461],
        [0.3714],
        [0.4852],
        [0.6876]])
>>> (aa - aa.mean()) / (((aa - aa.mean()) ** 2).mean() + 0.00001).sqrt()
tensor([[ 1.9935],
        [-0.4665],
        [-0.4477],
        [-1.8972],
        [-0.0501],
        [ 0.9855],
        [-0.2523],
        [-0.5761],
        [-0.0831],
        [ 0.7941]])
```

ä¿®æ”¹æ¨¡å‹ä½¿ç”¨ BatchNorm çš„ä»£ç å¦‚ä¸‹ï¼š

``` python
class MyModel(nn.Module):
    """æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=200)
        self.layer2 = nn.Linear(in_features=200, out_features=100)
        self.layer3 = nn.Linear(in_features=100, out_features=1)
        self.batchnorm1 = nn.BatchNorm1d(200)
        self.batchnorm2 = nn.BatchNorm1d(100)
        self.dropout1 = nn.Dropout(0.1)
        self.dropout2 = nn.Dropout(0.1)

    def forward(self, x):
        hidden1 = self.dropout1(self.batchnorm1(nn.functional.relu(self.layer1(x))))
        hidden2 = self.dropout2(self.batchnorm2(nn.functional.relu(self.layer2(hidden1))))
        y = self.layer3(hidden2)
        return y
```

éœ€è¦åŒæ—¶è°ƒæ•´å­¦ä¹ æ¯”ç‡ï¼š

``` text
# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD(model.parameters(), lr=0.05)
```

å›ºå®šè®­ç»ƒ 1000 æ¬¡çš„ç»“æœå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ BatchNorm æ²¡æœ‰å‘æŒ¥ä½œç”¨ğŸ¤•ï¼Œåè€Œå‡æ…¢äº†å­¦ä¹ é€Ÿåº¦å’Œå½±å“å¯è¾¾åˆ°çš„æœ€é«˜æ­£ç¡®ç‡ (ä½ å¯ä»¥è¯•è¯•å¢åŠ è®­ç»ƒæ¬¡æ•°)ï¼š

``` text
training accuracy: 0.9048486271500588
validating accuracy: 0.8341873311996459
highest validating accuracy: 0.8443503141403198 from epoch 946
testing accuracy: 0.8452585405111313
```

ä½¿ç”¨ BatchNorm æ—¶åº”è¯¥æ³¨æ„ä»¥ä¸‹çš„å‡ ç‚¹ï¼š

- BatchNorm åº”è¯¥é’ˆå¯¹éšè—å€¼ä½¿ç”¨ï¼Œå’Œ Dropout ä¸€æ ·
- BatchNorm éœ€è¦æŒ‡å®šéšè—å€¼æ•°é‡ï¼Œåº”è¯¥ä¸å¯¹åº”å±‚çš„è¾“å‡ºæ•°é‡åŒ¹é…
- BatchNorm åº”è¯¥æ”¾åœ¨ Dropout å‰é¢ï¼Œæœ‰éƒ¨åˆ†äººä¼šé€‰æ‹©æŠŠ BatchNorm æ”¾åœ¨æ¿€æ´»å‡½æ•°å‰ï¼Œä¹Ÿæœ‰éƒ¨åˆ†äººé€‰æ‹©æ”¾åœ¨æ¿€æ´»å‡½æ•°å
    - Linear => ReLU => BatchNorm => Dropout
    - Linear => BatchNorm => ReLU => Dropout
- BatchNorm åªåº”è¯¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œå’Œ Dropout ä¸€æ ·
- BatchNorm å‡½æ•°åº”è¯¥å®šä¹‰ä¸ºæ¨¡å‹çš„æˆå‘˜ï¼Œå’Œ Dropout ä¸€æ ·
- ä½¿ç”¨ BatchNorm çš„æ—¶å€™åº”è¯¥ç›¸åº”çš„å‡å°‘ Dropout çš„å±è”½æ¯”ä¾‹
- éƒ¨åˆ†åœºæ™¯å¯èƒ½ä¸é€‚ç”¨ BatchNorm (æ®è¯´æ›´é€‚ç”¨äºå¯¹è±¡è¯†åˆ«å’Œå›¾ç‰‡åˆ†ç±»)ï¼Œéœ€è¦å®è·µæ‰èƒ½å‡ºçœŸçŸ¥ â˜­

æå‡º BatchNorm æ‰‹æ³•çš„åŸå§‹è®ºæ–‡åœ¨[è¿™é‡Œ](https://arxiv.org/abs/1502.03167)ï¼Œå¦‚æœä½ æœ‰å…´è¶£å¯ä»¥æŸ¥çœ‹ã€‚

## ç†è§£æ¨¡å‹çš„ eval å’Œ train æ¨¡å¼

åœ¨å‰é¢çš„ä¾‹å­ä¸­æˆ‘ä»¬ä½¿ç”¨äº† `eval` å’Œ `train` å‡½æ•°åˆ‡æ¢æ¨¡å‹åˆ°è¯„ä»·æ¨¡å¼å’Œè®­ç»ƒæ¨¡å¼ï¼Œè¯„ä»·æ¨¡å¼ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼ŒDropout å’Œ BatchNormï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªæ¨¡å¼æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ

pytorch çš„æ¨¡å‹éƒ½åŸºäº `torch.nn.Module` è¿™ä¸ªç±»ï¼Œä¸ä»…æ˜¯æˆ‘ä»¬è‡ªå·±å®šä¹‰çš„æ¨¡å‹ï¼Œ`nn.Sequential`, `nn.Linear`, `nn.ReLU`, `nn.Dropout`, `nn.BatchNorm1d` ç­‰ç­‰çš„ç±»å‹éƒ½ä¼šåŸºäº `torch.nn.Module`ï¼Œ`torch.nn.Module` æœ‰ä¸€ä¸ª `training` æˆå‘˜ä»£è¡¨æ¨¡å‹æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œè€Œ `eval` å‡½æ•°ç”¨äºé€’å½’è®¾ç½®æ‰€æœ‰ `Module` çš„ `training` ä¸º `False`ï¼Œ`train` å‡½æ•°ç”¨äºé€’å½’è®¾ç½®æ‰€æœ‰ `Module` çš„ `training` ä¸º Trueã€‚æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è®¾ç½®è¿™ä¸ªæˆå‘˜çœ‹çœ‹æ˜¯å¦èƒ½èµ·åˆ°ç›¸åŒæ•ˆæœï¼š

``` python
>>> a = torch.tensor(range(1, 11), dtype=torch.float)
>>> dropout = torch.nn.Dropout(0.2)

>>> dropout.training = False
>>> dropout(a)
tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])

>>> dropout.training = True
>>> dropout(a)
tensor([ 1.2500,  2.5000,  3.7500,  0.0000,  0.0000,  7.5000,  8.7500, 10.0000,
         0.0000, 12.5000])
```

ç†è§£è¿™ä¸€ç‚¹åï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ä¸­æ·»åŠ åªåœ¨è®­ç»ƒæˆ–è€…è¯„ä»·çš„æ—¶å€™æ‰§è¡Œçš„ä»£ç ï¼Œæ ¹æ® `self.training` åˆ¤æ–­å³å¯ã€‚

## æœ€ç»ˆä»£ç 

æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„æœ€ç»ˆä»£ç å¦‚ä¸‹ï¼š

``` python
import os
import sys
import pandas
import torch
import gzip
import itertools
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®ç å†œæ¡ä»¶é¢„æµ‹å·¥èµ„çš„æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=200)
        self.layer2 = nn.Linear(in_features=200, out_features=100)
        self.layer3 = nn.Linear(in_features=100, out_features=1)
        self.batchnorm1 = nn.BatchNorm1d(200)
        self.batchnorm2 = nn.BatchNorm1d(100)
        self.dropout1 = nn.Dropout(0.1)
        self.dropout2 = nn.Dropout(0.1)

    def forward(self, x):
        hidden1 = self.dropout1(self.batchnorm1(nn.functional.relu(self.layer1(x))))
        hidden2 = self.dropout2(self.batchnorm2(nn.functional.relu(self.layer2(hidden1))))
        y = self.layer3(hidden2)
        return y

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # ä» csv è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡è¯»å– 2000 è¡Œ
    for batch, df in enumerate(pandas.read_csv('salary.csv', chunksize=2000)):
        dataset_tensor = torch.tensor(df.values, dtype=torch.float)

        # æ­£è§„åŒ–è¾“å…¥å’Œè¾“å‡º
        dataset_tensor *= torch.tensor([0.01, 1, 0.01, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0001])

        # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
        random_indices = torch.randperm(dataset_tensor.shape[0])
        traning_indices = random_indices[:int(len(random_indices)*0.6)]
        validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
        testing_indices = random_indices[int(len(random_indices)*0.8):]
        training_set = dataset_tensor[traning_indices]
        validating_set = dataset_tensor[validating_indices]
        testing_set = dataset_tensor[testing_indices]

        # ä¿å­˜åˆ°ç¡¬ç›˜
        save_tensor(training_set, f"data/training_set.{batch}.pt")
        save_tensor(validating_set, f"data/validating_set.{batch}.pt")
        save_tensor(testing_set, f"data/testing_set.{batch}.pt")
        print(f"batch {batch} saved")

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel()

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.SGD(model.parameters(), lr=0.05)

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    traning_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return max(0, 1 - ((actual - predicted).abs() / actual.abs()).mean().item())

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        traning_accuracy_list = []
        for batch in read_batches("data/training_set"):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
             for index in range(0, batch.shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
                batch_x = batch[index:index+100,:-1]
                batch_y = batch[index:index+100,-1:]
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x)
                # è®¡ç®—æŸå¤±
                loss = loss_function(predicted, batch_y)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    traning_accuracy_list.append(calc_accuracy(batch_y, predicted))
        traning_accuracy = sum(traning_accuracy_list) / len(traning_accuracy_list)
        traning_accuracy_history.append(traning_accuracy)
        print(f"training accuracy: {traning_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            validating_accuracy_list.append(calc_accuracy(batch[:,-1:],  model(batch[:,:-1])))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 100:
            # åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 100 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        testing_accuracy_list.append(calc_accuracy(batch[:,-1:],  model(batch[:,:-1])))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(traning_accuracy_history, label="traning")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    parameters = [
        "Age",
        "Gender (0: Male, 1: Female)",
        "Years of work experience",
        "Java Skill (0 ~ 5)",
        "NET Skill (0 ~ 5)",
        "JS Skill (0 ~ 5)",
        "CSS Skill (0 ~ 5)",
        "HTML Skill (0 ~ 5)"
    ]

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel()
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    while True:
        try:
            x = torch.tensor([int(input(f"Your {p}: ")) for p in parameters], dtype=torch.float)
            # æ­£è§„åŒ–è¾“å…¥
            x *= torch.tensor([0.01, 1, 0.01, 0.2, 0.2, 0.2, 0.2, 0.2])
            # è½¬æ¢åˆ° 1 è¡Œ 1 åˆ—çš„çŸ©é˜µï¼Œè¿™é‡Œå…¶å®å¯ä»¥ä¸è½¬æ¢ä½†æ¨èè¿™ä¹ˆåšï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒéæ‰¹æ¬¡è¾“å…¥
            x = x.view(1, len(x))
            # é¢„æµ‹è¾“å‡º
            y = model(x)
            # åæ­£è§„åŒ–è¾“å‡º
            y *= 10000
            print("Your estimated salary:", y[0,0].item(), "\n")except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æœ€ç»ˆè®­ç»ƒç»“æœå¦‚ä¸‹ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†æ­£ç¡®ç‡è¾¾åˆ°äº† 94.3% (å‰ä¸€ç¯‡åˆ†åˆ«æ˜¯ 93.3% å’Œ 93.1%)ï¼š

``` text
epoch: 848
training accuracy: 0.929181088420252
validating accuracy: 0.9417830203473568
stop training because highest validating accuracy not updated in 100 epoches
highest validating accuracy: 0.9437697219848633 from epoch 747
testing accuracy: 0.9438129015266895
```

æ­£ç¡®ç‡å˜åŒ–å¦‚ä¸‹ï¼š

![12](./12.png)

ç®—æ˜¯åœ†æ»¡æˆåŠŸäº†å­ğŸ¥³ã€‚

## å†™åœ¨æœ€å

åœ¨è¿™ä¸€ç¯‡æˆ‘ä»¬çœ‹åˆ°äº†å„ç§æ”¹è¿›è®­ç»ƒè¿‡ç¨‹å’Œæ”¹å–„è®­ç»ƒæ•ˆæœçš„æ‰‹æ³•ï¼Œé¢„æµ‹äº†å„ç§å„æ ·ç å†œçš„å·¥èµ„ğŸ™€ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥è¯•ç€åšä¸€äº›ä¸åŒçš„äº‹æƒ…äº†ã€‚ä¸‹ä¸€ç¯‡ä¼šä»‹ç»é€’å½’æ¨¡å‹ RNNï¼ŒLSTM ä¸ GRUï¼Œå®ƒä»¬å¯ä»¥ç”¨äºå¤„ç†ä¸å®šé•¿åº¦çš„æ•°æ®ï¼Œå®ç°æ ¹æ®ä¸Šä¸‹æ–‡åˆ†ç±»ï¼Œé¢„æµ‹è¶‹åŠ¿ï¼Œè‡ªåŠ¨è¡¥å…¨ç­‰åŠŸèƒ½ã€‚
