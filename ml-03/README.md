# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (ä¸‰) - çº¿æ€§æ¨¡å‹ï¼Œæ¿€æ´»å‡½æ•°ä¸å¤šå±‚çº¿æ€§æ¨¡å‹

## ç”Ÿç‰©ç¥ç»å…ƒä¸äººå·¥ç¥ç»å…ƒ

åœ¨äº†è§£ç¥ç»å…ƒç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç®€å•çš„çœ‹çœ‹ç”Ÿç‰©å­¦ä¸Šçš„ç¥ç»å…ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Œä¸‹å›¾æ‘˜è‡ªç»´åŸºç™¾ç§‘ï¼š

ï¼ˆå› ä¸ºæˆ‘ä¸æ˜¯ä¸“å®¶ï¼Œè¿™é‡Œçš„è§£é‡Šåªç”¨äºç†è§£äººå·¥ç¥ç»å…ƒæ¨¡æ‹Ÿäº†ç”Ÿç‰©ç¥ç»å…ƒçš„ä»€ä¹ˆåœ°æ–¹ï¼Œä¸ä¸€å®šå®Œå…¨å‡†ç¡®ï¼‰

![wikipeda-neuron](./wikipeda-neuron.png)

ç¥ç»å…ƒä¸»è¦ç”±ç»†èƒä½“å’Œç»†èƒçªç»„æˆï¼Œè€Œç»†èƒçªåˆ†ä¸ºæ ‘çª (Dendrites) å’Œè½´çª (Axon)ï¼Œæ ‘çªè´Ÿè´£æ¥æ”¶å…¶ä»–ç¥ç»å…ƒè¾“å…¥çš„ç”µæµï¼Œè€Œè½´çªè´Ÿè´£æŠŠç”µæµè¾“å‡ºç»™å…¶ä»–ç¥ç»å…ƒã€‚ä¸€ä¸ªç¥ç»å…ƒå¯ä»¥é€šè¿‡æ ‘çªä»å¤šä¸ªç¥ç»å…ƒæ¥æ”¶ç”µæµï¼Œå¦‚æœç”µæµæ²¡æœ‰è¾¾åˆ°æŸä¸ªé˜ˆå€¼åˆ™ç¥ç»å…ƒä¸ä¼šæŠŠç”µæµè¾“å‡ºï¼Œå¦‚æœç”µæµè¾¾åˆ°äº†æŸä¸ªé˜ˆå€¼åˆ™ç¥ç»å…ƒä¼šé€šè¿‡è½´çªçš„çªè§¦æŠŠç”µæµè¾“å‡ºç»™å…¶ä»–ç¥ç»å…ƒï¼Œè¿™æ ·çš„è§„åˆ™è¢«ç§°ä¸ºå…¨æœ‰å…¨æ— å¾‹ã€‚è¾“å…¥ç”µæµè¾¾åˆ°é˜ˆå€¼ä»¥åè¾“å‡ºç”µæµçš„çŠ¶æ€åˆç§°ä¸ºåˆ°è¾¾åŠ¨ä½œç”µä½ï¼ŒåŠ¨ä½œç”µä½ä¼šæŒç»­ 1 ~ 2 æ¯«ç§’ï¼Œä¹‹åä¼šè¿›å…¥çº¦ 0.5 æ¯«ç§’çš„ç»å¯¹ä¸åº”æœŸï¼Œæ— è®ºè¾“å…¥å¤šå¤§çš„ç”µæµéƒ½ä¸ä¼šè¾“å‡ºï¼Œç„¶åå†è¿›å…¥çº¦ 3.5 æ¯«ç§’çš„ç›¸å¯¹ä¸åº”æœŸï¼Œéœ€è¦ç”µæµè¾¾åˆ°æ›´å¤§çš„é˜ˆå€¼æ‰ä¼šè¾“å‡ºï¼Œæœ€åè¿”å›é™æ¯ç”µä½ã€‚ç¥ç»å…ƒä¹‹é—´è¿æ¥èµ·æ¥çš„ç½‘ç»œç§°ä¸ºç¥ç»å…ƒç½‘ç»œï¼Œäººçš„å¤§è„‘ä¸­å¤§çº¦æœ‰ 860 äº¿ä¸ªç¥ç»å…ƒï¼Œå› ä¸º 860 äº¿ä¸ªç¥ç»å…ƒå¯ä»¥åŒæ—¶å·¥ä½œï¼Œæ‰€ä»¥ç›®å‰çš„è®¡ç®—æœºæ— æ³•æ¨¡æ‹Ÿè¿™ç§å·¥ä½œæ–¹å¼ (é™¤éå¼€å‘ä¸“ç”¨çš„èŠ¯ç‰‡)ï¼Œåªèƒ½æ¨¡æ‹Ÿä¸€éƒ¨åˆ†çš„å·¥ä½œæ–¹å¼å’Œä½¿ç”¨æ›´å°è§„æ¨¡çš„ç½‘ç»œã€‚

è®¡ç®—æœºæ¨¡æ‹Ÿç¥ç»å…ƒç½‘ç»œä½¿ç”¨çš„æ˜¯äººå·¥ç¥ç»å…ƒï¼Œå•ä¸ªäººå·¥ç¥ç»å…ƒå¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼è¡¨è¾¾ï¼š

![01](./01.png)

å…¶ä¸­ n ä»£è¡¨è¾“å…¥çš„ä¸ªæ•°ï¼Œä½ å¯ä»¥æŠŠ n çœ‹ä½œè¿™ä¸ªç¥ç»å…ƒæ‹¥æœ‰çš„æ ‘çªä¸ªæ•°ï¼Œx çœ‹ä½œæ¯ä¸ªæ ‘çªè¾“å…¥ç”µæµçš„å€¼ï¼›è€Œ w (weight) ä»£è¡¨å„ä¸ªè¾“å…¥çš„æƒé‡ï¼Œä¹Ÿå°±æ˜¯å„ä¸ªæ ‘çªå¯¹ç”µæµå¤§å°çš„è°ƒæ•´ï¼›è€Œ b (bias) ç”¨äºè°ƒæ•´å„ä¸ªè¾“å…¥ä¹˜æƒé‡ç›¸åŠ åçš„å€¼ï¼Œä½¿å¾—è¿™ä¸ªå€¼å¯ä»¥é…åˆæŸä¸ªé˜ˆå€¼å·¥ä½œï¼›è€Œ g åˆ™æ˜¯æ¿€æ´»å‡½æ•°ï¼Œç”¨äºåˆ¤æ–­å€¼æ˜¯å¦è¾¾åˆ°é˜ˆå€¼å¹¶è¾“å‡ºå’Œè¾“å‡ºå¤šå°‘ï¼Œé€šå¸¸ä¼šä½¿ç”¨éçº¿æ€§å‡½æ•°ï¼›è€Œ y åˆ™æ˜¯è¾“å‡ºçš„å€¼ï¼Œå¯ä»¥æŠŠå®ƒçœ‹ä½œè½´çªè¾“å‡ºçš„ç”µæµï¼Œè¿æ¥è¿™ä¸ª y åˆ°å…¶ä»–ç¥ç»å…ƒå°±å¯ä»¥ç»„å»ºç¥ç»å…ƒç½‘ç»œã€‚

æˆ‘ä»¬åœ¨å‰ä¸¤ç¯‡çœ‹åˆ°çš„å…¶å®å°±æ˜¯åªæœ‰ä¸€ä¸ªè¾“å…¥å¹¶ä¸”æ²¡æœ‰æ¿€æ´»å‡½æ•°çš„å•ä¸ªäººå·¥ç¥ç»å…ƒï¼ŒæŠŠåŒæ ·çš„è¾“å…¥ä¼ ç»™å¤šä¸ªç¥ç»å…ƒ (ç¬¬ä¸€å±‚)ï¼Œç„¶åå†ä¼ ç»™å…¶ä»–ç¥ç»å…ƒ (ç¬¬äºŒå±‚)ï¼Œç„¶åå†ä¼ ç»™å…¶ä»–ç¥ç»å…ƒ (ç¬¬ä¸‰å±‚) å°±å¯ä»¥ç»„å»ºäººå·¥ç¥ç»å…ƒç½‘ç»œäº†ï¼ŒåŒä¸€å±‚çš„ç¥ç»å…ƒä¸ªæ•°è¶Šå¤šï¼Œç¥ç»å…ƒçš„å±‚æ•°è¶Šå¤šï¼Œç½‘ç»œå°±è¶Šå¼ºå¤§ï¼Œä½†éœ€è¦æ›´å¤šçš„è¿ç®—æ—¶é—´å¹¶ä¸”æ›´æœ‰å¯èƒ½å‘ç”Ÿç¬¬ä¸€ç¯‡æ–‡ç« è®²è¿‡çš„è¿‡æ‹Ÿåˆ (Overfitting) ç°è±¡ã€‚

ä¸‹å›¾æ˜¯äººå·¥ç¥ç»å…ƒç½‘ç»œçš„ä¾‹å­ï¼Œæœ‰ 3 è¾“å…¥ 1 ä¸ªè¾“å‡ºï¼Œç»è¿‡ 3 å±‚å¤„ç†ï¼Œç¬¬ 1 å±‚å’Œç¬¬ 2 å±‚å„æœ‰ä¸¤ä¸ªç¥ç»å…ƒå¯¹åº”éšè—å€¼ (ä¸­é—´å€¼)ï¼Œç¬¬ 3 å±‚æœ‰ä¸€ä¸ªç¥ç»å…ƒå¯¹åº”è¾“å‡ºå€¼ï¼š

![02](./02.png)

ç¥ç»å…ƒä¸­åŒ…å«çš„ w å’Œ b å°±æ˜¯æˆ‘ä»¬éœ€è¦é€šè¿‡æœºå™¨å­¦ä¹ è°ƒæ•´çš„å‚æ•°å€¼ã€‚

å¦‚æœä½ è§‰å¾—å›¾ç‰‡æœ‰ç‚¹éš¾ä»¥ç†è§£ï¼Œå¯ä»¥çœ‹è½¬æ¢åçš„ä»£ç ï¼š

``` python
h11 = g(x1 * w111 + x2 * w112 + x3 * w113 + b11)
h12 = g(x1 * w121 + x2 * w122 + x3 * w123 + b12)
h21 = g(h11 * w211 + h12 * w212 + b21)
h22 = g(h11 * w221 + h12 * w222 + b22)
y = g(h21 * w311 + h22 * w312 + b31)
```

å¾ˆå¤šç—´è¿·äººå·¥ç¥ç»å…ƒç½‘ç»œçš„å­¦è€…å£°ç§°äººå·¥ç¥ç»å…ƒç½‘ç»œå¯ä»¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ï¼Œåšåˆ°æŸäº›é¢†åŸŸä¸Šè¶…è¿‡äººè„‘çš„åˆ¤æ–­ï¼Œä½†å®é™…ä¸Šè¿™è¿˜æœ‰å¾ˆå¤§çš„äº‰è®®ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°äººå·¥ç¥ç»å…ƒçš„è¿æ¥æ–¹å¼åªä¼šæŒ‰å›ºå®šçš„æ¨¡å¼ï¼Œåˆ¤æ–­æ˜¯å¦è¾¾åˆ°é˜ˆå€¼å¹¶è¾“å‡ºçš„é€»è¾‘ä¹Ÿæ— æ³•åšåˆ°å’Œç”Ÿç‰©ç¥ç»å…ƒä¸€æ ·ï¼ˆç›®å‰è¿˜æ²¡æœ‰è§£æ˜ï¼‰ï¼Œå¹¶ä¸”ä¹Ÿæ²¡æœ‰ç”Ÿç‰©ç¥ç»å…ƒçš„ä¸åº”æœŸï¼Œæ‰€ä»¥ä¹Ÿæœ‰å­¦è€…å£°ç§°äººå·¥ç¥ç»å…ƒä¸è¿‡åªæ˜¯åšäº†å¤æ‚çš„æ•°å­¦è¿ç®—æ¥æ¨¡æ‹Ÿé€»è¾‘åˆ¤æ–­ï¼Œéœ€è¦æ ¹æ®ä¸åŒçš„åœºæ™¯åˆ‡æ¢ä¸åŒçš„è®¡ç®—æ–¹æ³•ï¼Œä½¿ç”¨è¿™ç§æ–¹å¼å¹¶ä¸èƒ½è¾¾åˆ°äººè„‘çš„æ°´å¹³ã€‚

## å•å±‚çº¿æ€§æ¨¡å‹

åœ¨å‰ä¸€ç¯‡æ–‡ç« æˆ‘ä»¬å·²ç»ç¨å¾®äº†è§£è¿‡æœºå™¨å­¦ä¹ æ¡†æ¶ pytorchï¼Œç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹æ€ä¹ˆä½¿ç”¨ pytorch å°è£…çš„çº¿æ€§æ¨¡å‹ï¼Œä»¥ä¸‹ä»£ç è¿è¡Œåœ¨ python çš„ REPL ä¸­ï¼š

``` python
# å¯¼å…¥ pytorch ç±»åº“
>>> import torch

# åˆ›å»º pytorch å°è£…çš„çº¿æ€§æ¨¡å‹ï¼Œè®¾ç½®è¾“å…¥æœ‰ 3 ä¸ªè¾“å‡ºæœ‰ 1 ä¸ª
>>> model = torch.nn.Linear(in_features=3, out_features=1)

# æŸ¥çœ‹çº¿æ€§æ¨¡å‹å†…éƒ¨åŒ…å«çš„å‚æ•°åˆ—è¡¨
# è¿™é‡Œä¸€å…±åŒ…å«ä¸¤ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ 1 è¡Œ 3 åˆ—çš„çŸ©é˜µåˆ†åˆ«è¡¨ç¤º 3 ä¸ªè¾“å…¥å¯¹åº”çš„ w å€¼ (æƒé‡)ï¼Œç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤º b å€¼ (åç§»)
# åˆå§‹å€¼ä¼šéšæœºç”Ÿæˆ (ä½¿ç”¨ kaiming_uniform ç”Ÿæˆæ­£æ€åˆ†å¸ƒ)
>>> list(model.parameters())
[Parameter containing:
tensor([[0.0599, 0.1324, 0.0099]], requires_grad=True), Parameter containing:
tensor([-0.2772], requires_grad=True)]

# å®šä¹‰è¾“å…¥å’Œè¾“å‡º
>>> x = torch.tensor([1, 2, 3], dtype=torch.float)
>>> y = torch.tensor([6], dtype=torch.float)

# æŠŠè¾“å…¥ä¼ ç»™æ¨¡å‹
>>> p = model(x)

# æŸ¥çœ‹é¢„æµ‹è¾“å‡ºå€¼
# 1 * 0.0599 + 2 * 0.1324 + 3 * 0.0099 - 0.2772 = 0.0772
>>> p
tensor([0.0772], grad_fn=<AddBackward0>)

# è®¡ç®—è¯¯å·®å¹¶è‡ªåŠ¨å¾®åˆ†
>>> l = (p - y).abs()
>>> l
tensor([5.9228], grad_fn=<AbsBackward>)
>>> l.backward()

# æŸ¥çœ‹å„ä¸ªå‚æ•°å¯¹åº”çš„å¯¼å‡½æ•°å€¼
>>> list(model.parameters())[0].grad
tensor([[-1., -2., -3.]])
>>> list(model.parameters())[1].grad
tensor([-1.])
```

ä»¥ä¸Šå¯ä»¥çœ‹ä½œ 1 å±‚ 1 ä¸ªç¥ç»å…ƒï¼Œå¾ˆå¥½ç†è§£å§ï¼Ÿæˆ‘ä»¬æ¥çœ‹çœ‹ 1 å±‚ 2 ä¸ªç¥ç»å…ƒï¼š

``` python
# å¯¼å…¥ pytorch ç±»åº“
>>> import torch

# åˆ›å»º pytorch å°è£…çš„çº¿æ€§æ¨¡å‹ï¼Œè®¾ç½®è¾“å…¥æœ‰ 3 ä¸ªè¾“å‡ºæœ‰ 2 ä¸ª
>>> model = torch.nn.Linear(in_features=3, out_features=2)

# æŸ¥çœ‹çº¿æ€§æ¨¡å‹å†…éƒ¨åŒ…å«çš„å‚æ•°åˆ—è¡¨
# è¿™é‡Œä¸€å…±åŒ…å«ä¸¤ä¸ªå‚æ•°
# ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ 2 è¡Œ 3 åˆ—çš„çŸ©é˜µåˆ†åˆ«è¡¨ç¤º 2 ä¸ªè¾“å‡ºå’Œ 3 ä¸ªè¾“å…¥å¯¹åº”çš„ w å€¼ (æƒé‡)
# ç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤º 2 ä¸ªè¾“å‡ºå¯¹åº”çš„ b å€¼ (åç§»)
>>> list(model.parameters())
[Parameter containing:
tensor([[0.1393, 0.5165, 0.2910],
        [0.2276, 0.1579, 0.1958]], requires_grad=True), Parameter containing:
tensor([0.2566, 0.1701], requires_grad=True)]

# å®šä¹‰è¾“å…¥å’Œè¾“å‡º
>>> x = torch.tensor([1, 2, 3], dtype=torch.float)
>>> y = torch.tensor([6, -6], dtype=torch.float)

# æŠŠè¾“å…¥ä¼ ç»™æ¨¡å‹
>>> p = model(x)

# æŸ¥çœ‹é¢„æµ‹è¾“å‡ºå€¼
# 1 * 0.1393 + 2 * 0.5165 + 3 * 0.2910 + 0.2566 = 2.3019
# 1 * 0.2276 + 2 * 0.1579 + 3 * 0.1958 + 0.1701 = 1.3009
>>> p
tensor([2.3019, 1.3009], grad_fn=<AddBackward0>)

# è®¡ç®—è¯¯å·®å¹¶è‡ªåŠ¨å¾®åˆ†
# (abs(2.3019 - 6) + abs(1.3009 - -6)) / 2 = 5.4995
>>> l = (p - y).abs().mean()
>>> l
tensor(5.4995, grad_fn=<MeanBackward0>)
>>> l.backward()

# æŸ¥çœ‹å„ä¸ªå‚æ•°å¯¹åº”çš„å¯¼å‡½æ•°å€¼
# å› ä¸ºè¯¯å·®å–äº† 2 ä¸ªå€¼çš„å¹³å‡ï¼Œæ‰€ä»¥æ±‚å¯¼å‡½æ•°å€¼çš„æ—¶å€™ä¼šé™¤ä»¥ 2
>>> list(model.parameters())[0].grad
tensor([[-0.5000, -1.0000, -1.5000],
        [ 0.5000,  1.0000,  1.5000]])
>>> list(model.parameters())[1].grad
tensor([-0.5000,  0.5000])
```

ç°åœ¨æˆ‘ä»¬æ¥è¯•è¯•ç”¨çº¿æ€§æ¨¡å‹æ¥å­¦ä¹ ç¬¦åˆ `x_1 * 1 + x_2 * 2 + x_3 * 3 + 8 = y` çš„æ•°æ®ï¼Œè¾“å…¥å’Œè¾“å‡ºä¼šä½¿ç”¨çŸ©é˜µå®šä¹‰ï¼š

``` python
# å¼•ç”¨ pytorch
import torch

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
# è¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
torch.random.manual_seed(0)

# åˆ›å»ºçº¿æ€§æ¨¡å‹ï¼Œè®¾ç½®æœ‰ 3 ä¸ªè¾“å…¥ 1 ä¸ªè¾“å‡º
model = torch.nn.Linear(in_features=3, out_features=1)

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# éšæœºç”ŸæˆåŸå§‹æ•°æ®é›†ï¼Œä¸€å…± 20 ç»„æ•°æ®ï¼Œæ¯æ¡æ•°æ®æœ‰ 3 ä¸ªè¾“å…¥
dataset_x = torch.randn((20, 3))
dataset_y = dataset_x.mm(torch.tensor([[1], [2], [3]], dtype=torch.float)) + 8
print(f"dataset_x: {dataset_x}")
print(f"dataset_y: {dataset_y}")

# åˆ‡åˆ†è®­ç»ƒé›† (12 ç»„)ï¼ŒéªŒè¯é›† (4 ç»„) å’Œæµ‹è¯•é›† (4 ç»„)
random_indices = torch.randperm(dataset_x.shape[0])
training_indices = random_indices[:int(len(random_indices)*0.6)]
validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
testing_indices = random_indices[int(len(random_indices)*0.8):]
training_set_x = dataset_x[training_indices]
training_set_y = dataset_y[training_indices]
validating_set_x = dataset_x[validating_indices]
validating_set_y = dataset_y[validating_indices]
testing_set_x = dataset_x[testing_indices]
testing_set_y = dataset_y[testing_indices]

# å¼€å§‹è®­ç»ƒè¿‡ç¨‹
for epoch in range(1, 10000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.train()

    # è®¡ç®—é¢„æµ‹å€¼
    # 20 è¡Œ 3 åˆ—çš„çŸ©é˜µä¹˜ä»¥ 3 è¡Œ 1 åˆ—çš„çŸ©é˜µ (ç”± weight è½¬ç½®å¾—åˆ°) ç­‰äº 20 è¡Œ 1 åˆ—çš„çŸ©é˜µ
    predicted = model(training_set_x)
    # è®¡ç®—æŸå¤±
    loss = loss_function(predicted, training_set_y)
    # æ‰“å°é™¤é”™ä¿¡æ¯
    print(f"loss: {loss}, weight: {model.weight}, bias: {model.bias}")
    # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
    loss.backward()
    # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
    optimizer.step()
    # æ¸…ç©ºå¯¼å‡½æ•°å€¼
    optimizer.zero_grad()

    # æ£€æŸ¥éªŒè¯é›†
    # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.eval()
    predicted = model(validating_set_x)
    validating_accuracy = 1 - ((validating_set_y - predicted).abs() / validating_set_y).abs().mean()
    print(f"validating x: {validating_set_x}, y: {validating_set_y}, predicted: {predicted}")

    # å¦‚æœéªŒè¯é›†æ­£ç¡®ç‡å¤§äº 99 %ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    print(f"validating accuracy: {validating_accuracy}")
    if validating_accuracy > 0.99:
        break

# æ£€æŸ¥æµ‹è¯•é›†
predicted = model(testing_set_x)
testing_accuracy = 1 - ((testing_set_y - predicted).abs() / testing_set_y).abs().mean()
print(f"testing x: {testing_set_x}, y: {testing_set_y}, predicted: {predicted}")
print(f"testing accuracy: {testing_accuracy}")
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

``` text
dataset_x: tensor([[ 0.8487,  0.6920, -0.3160],
        [-2.1152, -0.3561,  0.4372],
        [ 0.4913, -0.2041,  0.1198],
        [ 1.2377,  1.1168, -0.2473],
        [-1.0438, -1.3453,  0.7854],
        [ 0.9928,  0.5988, -1.5551],
        [-0.3414,  1.8530,  0.4681],
        [-0.1577,  1.4437,  0.2660],
        [ 1.3894,  1.5863,  0.9463],
        [-0.8437,  0.9318,  1.2590],
        [ 2.0050,  0.0537,  0.4397],
        [ 0.1124,  0.6408,  0.4412],
        [-0.2159, -0.7425,  0.5627],
        [ 0.2596,  0.5229,  2.3022],
        [-1.4689, -1.5867, -0.5692],
        [ 0.9200,  1.1108,  1.2899],
        [-1.4782,  2.5672, -0.4731],
        [ 0.3356, -1.6293, -0.5497],
        [-0.4798, -0.4997, -1.0670],
        [ 1.1149, -0.1407,  0.8058]])
dataset_y: tensor([[ 9.2847],
        [ 6.4842],
        [ 8.4426],
        [10.7294],
        [ 6.6217],
        [ 5.5252],
        [12.7689],
        [11.5278],
        [15.4009],
        [12.7970],
        [11.4315],
        [10.7175],
        [ 7.9872],
        [16.2120],
        [ 1.6500],
        [15.0112],
        [10.2369],
        [ 3.4277],
        [ 3.3199],
        [11.2509]])
epoch: 1
loss: 142.77590942382812, weight: Parameter containing:
tensor([[-0.0043,  0.3097, -0.4752]], requires_grad=True), bias: Parameter containing:
tensor([-0.4249], requires_grad=True)
validating x: tensor([[-0.4798, -0.4997, -1.0670],
        [ 0.8487,  0.6920, -0.3160],
        [ 0.1124,  0.6408,  0.4412],
        [-1.0438, -1.3453,  0.7854]]), y: tensor([[ 3.3199],
        [ 9.2847],
        [10.7175],
        [ 6.6217]]), predicted: tensor([[-0.1385],
        [ 0.3020],
        [-0.0126],
        [-1.1801]], grad_fn=<AddmmBackward>)
validating accuracy: -0.04714548587799072
epoch: 2
loss: 131.40403747558594, weight: Parameter containing:
tensor([[ 0.0675,  0.4937, -0.3163]], requires_grad=True), bias: Parameter containing:
tensor([-0.1970], requires_grad=True)
validating x: tensor([[-0.4798, -0.4997, -1.0670],
        [ 0.8487,  0.6920, -0.3160],
        [ 0.1124,  0.6408,  0.4412],
        [-1.0438, -1.3453,  0.7854]]), y: tensor([[ 3.3199],
        [ 9.2847],
        [10.7175],
        [ 6.6217]]), predicted: tensor([[-0.2023],
        [ 0.6518],
        [ 0.3935],
        [-1.1479]], grad_fn=<AddmmBackward>)
validating accuracy: -0.03184401988983154
epoch: 3
loss: 120.98343658447266, weight: Parameter containing:
tensor([[ 0.1357,  0.6687, -0.1639]], requires_grad=True), bias: Parameter containing:
tensor([0.0221], requires_grad=True)
validating x: tensor([[-0.4798, -0.4997, -1.0670],
        [ 0.8487,  0.6920, -0.3160],
        [ 0.1124,  0.6408,  0.4412],
        [-1.0438, -1.3453,  0.7854]]), y: tensor([[ 3.3199],
        [ 9.2847],
        [10.7175],
        [ 6.6217]]), predicted: tensor([[-0.2622],
        [ 0.9860],
        [ 0.7824],
        [-1.1138]], grad_fn=<AddmmBackward>)
validating accuracy: -0.016991496086120605

çœç•¥é€”ä¸­è¾“å‡º

epoch: 637
loss: 0.001102567883208394, weight: Parameter containing:
tensor([[1.0044, 2.0283, 3.0183]], requires_grad=True), bias: Parameter containing:
tensor([7.9550], requires_grad=True)
validating x: tensor([[-0.4798, -0.4997, -1.0670],
        [ 0.8487,  0.6920, -0.3160],
        [ 0.1124,  0.6408,  0.4412],
        [-1.0438, -1.3453,  0.7854]]), y: tensor([[ 3.3199],
        [ 9.2847],
        [10.7175],
        [ 6.6217]]), predicted: tensor([[ 3.2395],
        [ 9.2574],
        [10.6993],
        [ 6.5488]], grad_fn=<AddmmBackward>)
validating accuracy: 0.9900396466255188
testing x: tensor([[-0.3414,  1.8530,  0.4681],
        [-1.4689, -1.5867, -0.5692],
        [ 1.1149, -0.1407,  0.8058],
        [ 0.3356, -1.6293, -0.5497]]), y: tensor([[12.7689],
        [ 1.6500],
        [11.2509],
        [ 3.4277]]), predicted: tensor([[12.7834],
        [ 1.5438],
        [11.2217],
        [ 3.3285]], grad_fn=<AddmmBackward>)
testing accuracy: 0.9757462739944458
```

å¯ä»¥çœ‹åˆ°æœ€ç»ˆ weight æ¥è¿‘ 1, 2, 3ï¼Œbias æ¥è¿‘ 8ã€‚å’Œå‰ä¸€ç¯‡æ–‡ç« æœ€åçš„ä¾‹å­æ¯”è¾ƒè¿˜å¯ä»¥å‘ç°ä»£ç é™¤äº†å®šä¹‰æ¨¡å‹çš„éƒ¨åˆ†ä»¥å¤–å‡ ä¹ä¸€æ¨¡ä¸€æ · (åé¢çš„ä»£ç åŸºæœ¬ä¸Šéƒ½æ˜¯ç›¸åŒçš„ç»“æ„ï¼Œè¿™ä¸ªç³»åˆ—æ˜¯å…ˆå­¦å¥—è·¯åœ¨å­¦ç»†èŠ‚ğŸ˜ˆ) ã€‚

çœ‹åˆ°è¿™é‡Œä½ å¯èƒ½ä¼šè§‰å¾—ï¼Œæ€ä¹ˆæˆ‘ä»¬ä¸€ç›´éƒ½åœ¨å­¦ä¹ ä¸€æ¬¡æ–¹ç¨‹å¼ï¼Œä¸èƒ½åšæ›´å¤æ‚çš„äº‹æƒ…å—ğŸ˜¡ï¼Ÿ

å¦‚æˆ‘ä»¬çœ‹åˆ°çš„ï¼Œçº¿æ€§æ¨¡å‹åªèƒ½è®¡ç®—ä¸€æ¬¡æ–¹ç¨‹å¼ï¼Œå¦‚æœæˆ‘ä»¬ç»™çš„æ•°æ®ä¸æ»¡è¶³ä»»ä½•ä¸€æ¬¡æ–¹ç¨‹å¼ï¼Œè¿™ä¸ªæ¨¡å‹å°†æ— æ³•å­¦ä¹ æˆåŠŸï¼Œé‚£ä¹ˆå åŠ å¤šå±‚çº¿æ€§æ¨¡å‹å¯ä»¥å­¦ä¹ æ›´å¤æ‚çš„æ•°æ®å—ï¼Ÿ

ä»¥ä¸‹æ˜¯ä¸€å±‚å’Œä¸¤å±‚äººå·¥ç¥ç»å…ƒç½‘ç»œçš„å…¬å¼ä¾‹å­:

![03](./03.png)

å› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰å­¦åˆ°æ¿€æ´»å‡½æ•°ï¼Œå…ˆå»æ‰æ¿€æ´»å‡½æ•°ï¼Œç„¶åå±•å¼€æ²¡æœ‰æ¿€æ´»å‡½æ•°çš„ä¸¤å±‚äººå·¥ç¥ç»å…ƒç½‘ç»œçœ‹çœ‹æ˜¯ä»€ä¹ˆæ ·å­ï¼š

![04](./04.png)

ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œä¸¤å±‚äººå·¥ç¥ç»å…ƒç½‘ç»œå’Œä¸€å±‚ç¥ç»å…ƒç½‘ç»œæ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œå®é™…ä¸Šï¼Œå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ä¸ç®¡å åŠ å¤šå°‘å±‚éƒ½å’Œä¸€å±‚ä¸€æ ·ğŸ™€ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬è¦æ„å»ºå¤šå±‚ç½‘ç»œï¼Œå¿…é¡»æ·»åŠ æ¿€æ´»å‡½æ•°ã€‚

## æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°çš„ä½œç”¨æ˜¯è®©äººå·¥ç¥ç»å…ƒç½‘ç»œæ”¯æŒå­¦ä¹ éçº¿æ€§çš„æ•°æ®ï¼Œæ‰€è°“éçº¿æ€§çš„æ•°æ®å°±æ˜¯ä¸æ»¡è¶³ä»»ä½•ä¸€æ¬¡æ–¹ç¨‹å¼çš„æ•°æ®ï¼Œè¾“å‡ºå’Œè¾“å…¥ä¹‹é—´ä¸ä¼šæŒ‰ä¸€å®šæ¯”ä¾‹å˜åŒ–ã€‚ä¸¾ä¸ªå¾ˆç®€å•çš„ä¾‹å­ï¼Œå¦‚æœéœ€è¦æŒ‰ç å†œçš„æ•°é‡è®¡ç®—æŸä¸ªé¡¹ç›®æ‰€éœ€çš„å®Œå·¥æ—¶é—´ï¼Œä¸€ä¸ªç å†œéœ€è¦ä¸€ä¸ªæœˆï¼Œä¸¤ä¸ªç å†œéœ€è¦åŠä¸ªæœˆï¼Œä¸‰ä¸ªç å†œéœ€è¦åå¤©ï¼Œå››ä¸ªç å†œéœ€è¦ä¸€ä¸ªæ˜ŸæœŸï¼Œä¹‹åæ— è®ºè¯·å¤šå°‘ä¸ªç å†œéƒ½éœ€è¦ä¸€ä¸ªæ˜ŸæœŸï¼Œå¤šå‡ºæ¥çš„ç å†œåªä¼šåƒé—²é¥­ï¼Œä½¿ç”¨å›¾è¡¨å¯ä»¥è¡¨ç°å¦‚ä¸‹ï¼š

![05](./05.png)

å¦‚æœä¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œåªç”¨çº¿æ€§æ¨¡å‹å¯ä»¥è°ƒæ•´ w åˆ° -5.4ï¼Œb åˆ° 30ï¼Œä½¿ç”¨å›¾è¡¨å¯¹æ¯” `-5.4 x + 30` å’Œå®é™…æ•°æ®å¦‚ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸ä»…é¢„æµ‹çš„è¯¯å·®è¾ƒå¤§ï¼Œéšç€ç å†œæ•°é‡çš„å¢é•¿é¢„æµ‹æ‰€éœ€çš„å®Œå·¥æ—¶é—´ä¼šå˜ä¸ºè´Ÿæ•°ğŸ˜±ï¼š

![06](./06.png)

é‚£ä¹ˆä½¿ç”¨æ¿€æ´»å‡½æ•°ä¼šæ€æ ·å‘¢ï¼Ÿæˆ‘ä»¬ä»¥æœ€ç®€å•ä¹Ÿæ˜¯æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•° ReLU (Rectified Linear Unit) ä¸ºä¾‹ï¼ŒReLU å‡½æ•°çš„å®šä¹‰å¦‚ä¸‹ï¼š

![07](./07.png)

æ„æ€æ˜¯ä¼ å…¥å€¼å¤§äº 0 æ—¶è¿”å›åŸå€¼ï¼Œå¦åˆ™è¿”å› 0ï¼Œç”¨ python ä»£ç å¯ä»¥è¡¨ç°å¦‚ä¸‹ï¼š

``` python
def relu(x):
    if x > 0:
        return x
    return 0
```

å†çœ‹çœ‹ä»¥ä¸‹ç»“åˆäº† ReLU æ¿€æ´»å‡½æ•°çš„ä¸¤å±‚äººå·¥ç¥ç»å…ƒç½‘ç»œ (æœ€åä¸€å±‚ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°)ï¼š

![08](./08.png)

è¯•è¯•è®¡ç®—ä¸Šé¢çš„ä¾‹å­ï¼š

``` python
def relu(x):
    if x > 0:
        return x
    return 0

for x in range(1, 11):
    h1 = relu(x * -1 + 2)
    h2 = relu(x * -1 + 3)
    h3 = relu(x * -1 + 4)
    y = h1 * 10 + h2 * 2 + h3 * 3 + 7
    print(x, y)
```

è¾“å…¥å¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°æ·»åŠ æ¿€æ´»å‡½æ•°åæ¨¡å‹èƒ½å¤Ÿæ”¯æŒè®¡ç®—ä¸Šé¢çš„éçº¿æ€§æ•°æ®ğŸ˜ï¼š

``` text
1 30
2 15
3 10
4 7
5 7
6 7
7 7
8 7
9 7
10 7
```

æ¿€æ´»å‡½æ•°é™¤äº†ä¸Šè¿°ä»‹ç»çš„ ReLU è¿˜æœ‰å¾ˆå¤šï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š

![09](./09.png)

å¦‚æœä½ æƒ³çœ‹å®ƒä»¬çš„æ›²çº¿å’Œå¯¼å‡½æ•°å¯ä»¥å‚è€ƒä»¥ä¸‹é“¾æ¥ï¼š

- https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity
- https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html

æ·»åŠ æ¿€æ´»å‡½æ•°ä»¥åæ±‚å¯¼å‡½æ•°å€¼åŒæ ·å¯ä»¥ä½¿ç”¨è¿é”å¾‹ï¼Œå¦‚æœç¬¬ä¸€å±‚è¿”å›çš„æ˜¯æ­£æ•°ï¼Œé‚£ä¹ˆå°±å’Œæ²¡æœ‰ ReLU æ—¶çš„è®¡ç®—ä¸€æ · (å¯¼å‡½æ•°ä¸º 1)ï¼š

``` python
>>> w1 = torch.tensor(5.0, requires_grad=True)
>>> b1 = torch.tensor(1.0, requires_grad=True)
>>> w2 = torch.tensor(6.0, requires_grad=True)
>>> b2 = torch.tensor(7.0, requires_grad=True)

>>> x = torch.tensor(2.0)
>>> y = torch.nn.functional.relu(x * w1 + b1) * w2 + b2

# å‡è®¾æˆ‘ä»¬è¦è°ƒæ•´ w1, b1, w2, b2 ä½¿å¾— y æ¥è¿‘ 0
>>> y
tensor(73., grad_fn=<AddBackward0>)
>>> y.abs().backward()

# w1 çš„å¯¼å‡½æ•°å€¼ä¸º x * w2
>>> w1.grad
tensor(12.)

# b1 çš„å¯¼å‡½æ•°å€¼ä¸º w2
>>> b1.grad
tensor(6.)

# w2 çš„å¯¼å‡½æ•°å€¼ä¸º x * w1 + b1
>>> w2.grad
tensor(11.)

# b1 çš„å¯¼å‡½æ•°å€¼ä¸º 1
>>> b2.grad
tensor(1.)
```

å‡è®¾ç¬¬ä¸€å±‚è¿”å›çš„æ˜¯è´Ÿæ•°ï¼Œé‚£ä¹ˆ ReLU ä¼šè®©ä¸Šä¸€å±‚çš„å¯¼å‡½æ•°å€¼ä¸º 0 (å¯¼å‡½æ•°ä¸º 0)ï¼š

``` python
>>> w1 = torch.tensor(5.0, requires_grad=True)
>>> b1 = torch.tensor(1.0, requires_grad=True)
>>> w2 = torch.tensor(6.0, requires_grad=True)
>>> b2 = torch.tensor(7.0, requires_grad=True)

>>> x = torch.tensor(-2.0)
>>> y = torch.nn.functional.relu(x * w1 + b1) * w2 + b2

>>> y
tensor(7., grad_fn=<AddBackward0>)
>>> y.backward()

>>> w1.grad
tensor(-0.)
>>> b1.grad
tensor(0.)
>>> w2.grad
tensor(0.)
>>> b2.grad
tensor(1.)
```

è™½ç„¶ ReLU å¯ä»¥é€‚åˆå¤§éƒ¨åˆ†åœºæ™¯ï¼Œä½†æœ‰æ—¶å€™æˆ‘ä»¬è¿˜æ˜¯éœ€è¦é€‰æ‹©å…¶ä»–æ¿€æ´»å‡½æ•°ï¼Œé€‰æ‹©æ¿€æ´»å‡½æ•°ä¸€èˆ¬ä¼šè€ƒè™‘ä»¥ä¸‹çš„å› ç´ ï¼š

- è®¡ç®—é‡
- æ˜¯å¦å­˜åœ¨æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient) é—®é¢˜
- æ˜¯å¦å­˜åœ¨åœæ­¢å­¦ä¹ é—®é¢˜

ä»ä¸Šå›¾æˆ‘ä»¬å¯ä»¥çœ‹åˆ° ReLU çš„è®¡ç®—é‡æ˜¯æœ€å°‘çš„ï¼ŒSigmoid å’Œ Tanh çš„è®¡ç®—é‡åˆ™å¾ˆå¤§ï¼Œä½¿ç”¨è®¡ç®—é‡å¤§çš„æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´å­¦ä¹ è¿‡ç¨‹æ›´æ…¢ã€‚

è€Œæ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient) é—®é¢˜åˆ™æ˜¯åœ¨äººå·¥ç¥ç»å…ƒç½‘ç»œå±‚æ•°å¢å¤šä»¥åå‡ºç°çš„é—®é¢˜ï¼Œå¦‚æœå±‚æ•°ä¸æ–­å¢å¤šï¼Œä½¿ç”¨è¿é”å¾‹æ±‚å¯¼å‡½æ•°çš„æ—¶å€™ä¼šä¸æ–­å åŠ æ¿€æ´»å‡½æ•°çš„å¯¼å‡½æ•°ï¼Œéƒ¨åˆ†æ¿€æ´»å‡½æ•°ä¾‹å¦‚ Sigmoid å’Œ Tanh çš„å¯¼å‡½æ•°ä¼šéšç€å åŠ æ¬¡æ•°å¢å¤šè€Œä¸æ–­çš„å‡å°‘å¯¼å‡½æ•°å€¼ã€‚ä¾‹å¦‚æœ‰ 3 å±‚çš„æ—¶å€™ï¼Œç¬¬ 3 å±‚å¯¼å‡½æ•°å€¼å¯èƒ½æ˜¯ `6, -2, 1`ï¼Œç¬¬ 2 å±‚çš„å¯¼å‡½æ•°å€¼å¯èƒ½æ˜¯ `0.07, 0.68, -0.002`ï¼Œç¬¬ 1 å±‚çš„å¯¼å‡½æ•°å€¼å¯èƒ½æ˜¯ `0.0004, -0.00016, -0.00003`ï¼Œä¹Ÿå°±æ˜¯å‰é¢çš„å±‚å‚æ•°åŸºæœ¬ä¸Šä¸ä¼šè°ƒæ•´ï¼Œåªæœ‰åé¢çš„å±‚å‚æ•°ä¸æ–­å˜åŒ–ï¼Œå¯¼è‡´æµªè´¹è®¡ç®—èµ„æºå’Œä¸èƒ½å®Œå…¨å‘æŒ¥æ¨¡å‹çš„èƒ½åŠ›ã€‚æ¿€æ´»å‡½æ•° ReLU åˆ™ä¸ä¼šå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå› ä¸ºä¸ç®¡å åŠ å¤šå°‘å±‚åªè¦ä¸­é—´ä¸å­˜åœ¨è´Ÿæ•°åˆ™å¯¼å‡½æ•°å€¼ä¼šä¸€ç›´ä¼ é€’ä¸Šå»ï¼Œè¿™ä¹Ÿæ˜¯ ReLU æµè¡Œçš„åŸå› ä¹‹ä¸€ã€‚

åœæ­¢å­¦ä¹ é—®é¢˜æ˜¯æ¨¡å‹è¾¾åˆ°æŸä¸ªçŠ¶æ€ (æœªå­¦ä¹ æˆåŠŸ) ä»¥åä¸ç®¡æ€ä¹ˆè°ƒæ•´å‚æ•°éƒ½ä¸ä¼šå˜åŒ–çš„é—®é¢˜ï¼Œä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯ä½¿ç”¨ ReLU æ—¶ï¼Œå¦‚æœç¬¬ä¸€å±‚çš„è¾“å‡ºåˆšå¥½å…¨éƒ¨éƒ½æ˜¯è´Ÿæ•°ï¼Œé‚£éšè—å€¼åˆ™å…¨éƒ¨ä¸º 0ï¼Œå¯¼å‡½æ•°å€¼ä¹Ÿä¸º 0ï¼Œä¸ç®¡å†æ€ä¹ˆè®­ç»ƒå‚æ•°éƒ½ä¸ä¼šå˜åŒ–ã€‚LeakyReLU ä¸ ELU åˆ™æ˜¯ä¸ºäº†è§£å†³åœæ­¢å­¦ä¹ é—®é¢˜äº§ç”Ÿçš„ï¼Œä½†å› ä¸ºå¢åŠ è®¡ç®—é‡å’Œå…è®¸è´Ÿæ•°å¯èƒ½ä¼šå¸¦æ¥å…¶ä»–å½±å“ï¼Œæˆ‘ä»¬ä¸€èˆ¬éƒ½ä¼šå…ˆä½¿ç”¨ ReLUï¼Œå‡ºç°åœæ­¢å­¦ä¹ é—®é¢˜å†è¯•è¯• ReLU çš„æ´¾ç”Ÿå‡½æ•°ã€‚

Sigmoid å’Œ Tanh è™½ç„¶æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½†æ˜¯å®ƒä»¬å¯ä»¥ç”¨äºåœ¨æŒ‡å®šåœºæ™¯ä¸‹è½¬æ¢æ•°å€¼åˆ° 0 ~ 1 å’Œ -1 ~ 1ã€‚ä¾‹å¦‚ Sigmoid å¯ä»¥ç”¨åœ¨æœ€åä¸€å±‚è¡¨ç°å¯èƒ½æ€§ï¼Œ100 è¡¨ç¤ºéå¸¸æœ‰å¯èƒ½ (è½¬æ¢åˆ° 1)ï¼Œ50 ä¹Ÿä»£è¡¨éå¸¸æœ‰å¯èƒ½ (è½¬æ¢åˆ° 1)ï¼Œ1 ä»£è¡¨æ¯”è¾ƒæœ‰å¯èƒ½ (è½¬æ¢åˆ° 0.7311)ï¼Œ0 ä»£è¡¨ä¸ç¡®å®š (è½¬æ¢åˆ° 0.5)ï¼Œ-1 ä»£è¡¨æ¯”è¾ƒä¸å¯èƒ½ (è½¬æ¢åˆ° 0.2689)ï¼Œ-100 ä»£è¡¨å¾ˆä¸å¯èƒ½ (è½¬æ¢åˆ° 0)ã€‚è€Œåé¢æ–‡ç« ä»‹ç»çš„ LSTM æ¨¡å‹ä¹Ÿä¼šä½¿ç”¨ Sigmoid å†³å®šéœ€è¦å¿˜è®°å“ªäº›å†…éƒ¨çŠ¶æ€ï¼ŒTanh å†³å®šåº”è¯¥æ€æ ·æ›´æ–°å†…éƒ¨çŠ¶æ€ã€‚æ­¤å¤–è¿˜æœ‰ Softmax ç­‰ä¸€èˆ¬åªç”¨åœ¨æœ€åä¸€å±‚çš„å‡½æ•°ï¼ŒSoftmax å¯ä»¥ç”¨äºåœ¨åˆ†ç±»çš„æ—¶å€™åˆ¤æ–­å“ªä¸ªç±»åˆ«å¯èƒ½æ€§æœ€å¤§ï¼Œä¾‹å¦‚è¯†åˆ«çŒ«ç‹—çŒªçš„æ—¶å€™æœ€åä¸€å±‚ç»™å‡º `6, 5, 8`ï¼Œæ•°å€¼è¶Šå¤§ä»£è¡¨å±äºè¯¥åˆ†ç±»çš„å¯èƒ½æ€§è¶Šé«˜ï¼Œç»è¿‡ Softmax è½¬æ¢ä»¥åå°±æ˜¯ `0.1142, 0.0420, 0.8438`ï¼Œä»£è¡¨æœ‰ 11.42% çš„å¯èƒ½æ€§æ˜¯çŒ«ï¼Œ4.2% çš„å¯èƒ½æ€§æ˜¯ç‹—ï¼Œ84.38% çš„å¯èƒ½æ€§æ˜¯çŒªã€‚

## å¤šå±‚çº¿æ€§æ¨¡å‹

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹æ€æ ·åœ¨ pytorch é‡Œé¢å®šä¹‰å¤šå±‚çº¿æ€§æ¨¡å‹ï¼Œä¸Šä¸€èŠ‚å·²ç»ä»‹ç»è¿‡ `torch.nn.Linear` æ˜¯ pytorch ä¸­å•å±‚çº¿æ€§æ¨¡å‹çš„å°è£…ï¼Œç»„åˆå¤šä¸ª `torch.nn.Linear` å°±å¯ä»¥å®ç°å¤šå±‚çº¿æ€§æ¨¡å‹ã€‚

ç»„åˆ `torch.nn.Linear` æœ‰ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„æ¨¡å‹ç±»ï¼Œå…³äºæ¨¡å‹ç±»åœ¨ä¸Šä¸€ç¯‡å·²ç»ä»‹ç»è¿‡ï¼š

``` python
# å¼•ç”¨ pytorchï¼Œnn ç­‰åŒäº torch.nn
import torch
from torch import nn

# å®šä¹‰æ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        # åˆå§‹åŒ–åŸºç±»
        super().__init__()
        # å®šä¹‰å‚æ•°
        # è¿™é‡Œä¸€å…±å®šä¹‰äº†ä¸‰å±‚
        #   ç¬¬ä¸€å±‚æ¥æ”¶ 2 ä¸ªè¾“å…¥ï¼Œè¿”å› 32 ä¸ªéšè—å€¼ (å†…éƒ¨ weight çŸ©é˜µä¸º 32 è¡Œ 2 åˆ—)
        #   ç¬¬äºŒå±‚æ¥æ”¶ 32 ä¸ªéšè—å€¼ï¼Œè¿”å› 64 ä¸ªéšè—å€¼ (å†…éƒ¨ weight çŸ©é˜µä¸º 64 è¡Œ 32 åˆ—)
        #   ç¬¬ä¸‰å±‚æ¥æ”¶ 64 ä¸ªéšè—å€¼ï¼Œè¿”å› 1 ä¸ªè¾“å‡º (å†…éƒ¨ weight çŸ©é˜µä¸º 1 è¡Œ 64 åˆ—)
        self.layer1 = nn.Linear(in_features=2, out_features=32)
        self.layer2 = nn.Linear(in_features=32, out_features=64)
        self.layer3 = nn.Linear(in_features=64, out_features=1)

    def forward(self, x):
        # x æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œè¡Œæ•°ä»£è¡¨æ‰¹æ¬¡ï¼Œåˆ—æ•°ä»£è¡¨è¾“å…¥ä¸ªæ•°ï¼Œä¾‹å¦‚æœ‰ 50 ä¸ªæ‰¹æ¬¡åˆ™ä¸º 50 è¡Œ 2 åˆ—

        # è®¡ç®—ç¬¬ä¸€å±‚è¿”å›çš„éšè—å€¼ï¼Œä¾‹å¦‚æœ‰ 50 ä¸ªæ‰¹æ¬¡åˆ™ hidden1 ä¸º 50 è¡Œ 32 åˆ—
        # è®¡ç®—çŸ©é˜µä¹˜æ³•æ—¶ä¼šè½¬ç½® layer1 å†…éƒ¨çš„ weight çŸ©é˜µï¼Œ50 è¡Œ 2 åˆ—ä¹˜ä»¥ 2 è¡Œ 32 åˆ—ç­‰äº 50 è¡Œ 32 åˆ—
        hidden1 = nn.functional.relu(self.layer1(x))

        # è®¡ç®—ç¬¬äºŒå±‚è¿”å›çš„éšè—å€¼ï¼Œä¾‹å¦‚æœ‰ 50 ä¸ªæ‰¹æ¬¡åˆ™ hidden2 ä¸º 50 è¡Œ 64 åˆ—
        hidden2 = nn.functional.relu(self.layer2(hidden1))

        # è®¡ç®—ç¬¬ä¸‰å±‚è¿”å›çš„è¾“å‡ºï¼Œä¾‹å¦‚æœ‰ 50 ä¸ªæ‰¹æ¬¡åˆ™ y ä¸º 50 è¡Œ 1 åˆ—
        y = self.layer3(hidden2)

        # è¿”å›è¾“å‡º
        return y

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModel()
```

æˆ‘ä»¬å¯ä»¥å®šä¹‰ä»»æ„æ•°é‡çš„å±‚ï¼Œä½†æ¯ä¸€å±‚çš„æ¥æ”¶å€¼ä¸ªæ•° (in_features) å¿…é¡»ç­‰äºä¸Šä¸€å±‚çš„è¿”å›å€¼ä¸ªæ•° (out_features)ï¼Œç¬¬ä¸€å±‚çš„æ¥æ”¶å€¼ä¸ªæ•°éœ€è¦ç­‰äºè¾“å…¥ä¸ªæ•°ï¼Œæœ€åä¸€å±‚çš„è¿”å›å€¼ä¸ªæ•°éœ€è¦ç­‰äºè¾“å‡ºä¸ªæ•°ã€‚

ç¬¬äºŒç§æ–¹æ³•æ˜¯ä½¿ç”¨ `torch.nn.Sequential`ï¼Œè¿™ç§æ–¹æ³•æ›´ç®€ä¾¿ï¼š

``` python3
# å¼•ç”¨ pytorchï¼Œnn ç­‰åŒäº torch.nn
import torch
from torch import nn

# åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼Œæ•ˆæœç­‰åŒäºç¬¬ä¸€ç§æ–¹æ³•
model = nn.Sequential(
    nn.Linear(in_features=2, out_features=32),
    nn.ReLU(),
    nn.Linear(in_features=32, out_features=64),
    nn.ReLU(),
    nn.Linear(in_features=64, out_features=1))

# æ³¨:
# nn.functional.relu(x) ç­‰äº nn.ReLU()(x)
```

å¦‚å‰é¢æ‰€è¯´çš„ï¼Œå±‚æ•°è¶Šå¤šéšè—å€¼æ•°é‡è¶Šå¤šæ¨¡å‹å°±è¶Šå¼ºå¤§ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´å¹¶ä¸”æ›´å®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®é™…æ“ä½œæ—¶æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå°çš„æ¨¡å‹ï¼Œå†æŒ‰éœ€è¦å¢åŠ å±‚æ•°å’Œéšè—å€¼ä¸ªæ•°ã€‚

ä¸‰å±‚çº¿æ€§æ¨¡å‹çš„è®¡ç®—å›¾å¯ä»¥è¡¨ç°å¦‚ä¸‹ï¼Œä»¥åè¿™ä¸ªç³»åˆ—åœ¨è®²è§£å…¶ä»–æ¨¡å‹çš„æ—¶å€™ä¹Ÿä¼šä½¿ç”¨ç›¸åŒå½¢å¼çš„å›¾è¡¨è¡¨ç¤ºæ¨¡å‹çš„è®¡ç®—è·¯å¾„ï¼š

![10](./10.png)

## å®ä¾‹ - æ ¹æ®ç å†œæ¡ä»¶æ±‚å·¥èµ„

æˆ‘ä»¬å·²ç»äº†è§£åˆ°å¦‚ä½•åˆ›å»ºå¤šå±‚çº¿æ€§æ¨¡å‹ï¼Œç°åœ¨å¯ä»¥è¯•è¯•è§£å†³æ¯”è¾ƒå®é™…çš„é—®é¢˜äº†ï¼Œå¯¹äºå¤§éƒ¨åˆ†ç å†œæ¥è¯´æœ€å®é™…çš„é—®é¢˜å°±æ˜¯æ¯ä¸ªæœˆèƒ½æ‹¿å¤šå°‘å·¥èµ„ğŸ˜¿ï¼Œé‚£å°±æ¥å»ºç«‹ä¸€ä¸ªæ ¹æ®ç å†œçš„æ¡ä»¶ï¼Œé¢„æµ‹å¯ä»¥æ‹¿åˆ°å¤šå°‘å·¥èµ„çš„æ¨¡å‹å§ã€‚

ä»¥ä¸‹æ˜¯ä»æŸä¸ªåœ°æ–¹ç§˜å¯†æ”¶é›†å›æ¥çš„ç å†œæ¡ä»¶å’Œå·¥èµ„æ•°æ®ï¼ˆå…¶å®æ˜¯æŒ‰æŸç§è§„å¾‹éšæœºç”Ÿæˆå‡ºæ¥çš„ï¼Œä¸æ˜¯å®é™…æ•°æ®ğŸ™€ï¼‰ï¼š

``` text
å¹´é¾„,æ€§åˆ«,å·¥ä½œç»éªŒ,Java,NET,JS,CSS,HTML,å·¥èµ„
29,0,0,1,2,2,1,4,12500
22,0,2,2,3,1,2,5,15500
24,0,4,1,2,1,1,2,16000
35,0,6,3,3,0,1,0,19500
45,0,18,0,5,2,0,5,17000
24,0,2,0,0,0,1,1,13500
23,1,2,2,3,1,1,0,10500
41,0,16,2,5,5,2,0,16500
50,0,18,0,5,0,5,2,16500
20,0,0,0,5,2,0,1,12500
26,0,6,1,5,5,1,1,27000
46,0,12,0,5,4,4,2,12500
26,0,6,1,5,3,1,1,23500
40,0,9,0,0,1,0,1,17500
41,0,20,3,5,3,3,5,20500
26,0,4,0,1,2,4,0,18500
42,0,18,5,0,0,2,5,18500
21,0,1,1,0,1,2,0,12000
26,0,1,0,0,0,0,2,12500
```

å®Œæ•´æ•°æ®æœ‰ 50000 æ¡ï¼Œå¯ä»¥ä» https://github.com/303248153/BlogArchive/tree/master/ml-03/salary.csv ä¸‹è½½ã€‚

æ¯ä¸ªç å†œæœ‰ä»¥ä¸‹æ¡ä»¶ï¼š

- å¹´é¾„
- æ€§åˆ« (0: ç”·æ€§, 1: å¥³æ€§)
- å·¥ä½œç»éªŒå¹´æ•° (ä»…é™äº’è”ç½‘è¡Œä¸š)
- Java ç¼–ç ç†Ÿç»ƒç¨‹åº¦ (0 ~ 5)
- NET ç¼–ç ç†Ÿç»ƒç¨‹åº¦ (0 ~ 5)
- JS ç¼–ç ç†Ÿç»ƒç¨‹åº¦ (0 ~ 5)
- CSS ç¼–ç ç†Ÿç»ƒç¨‹åº¦ (0 ~ 5)
- HTML ç¼–ç ç†Ÿç»ƒç¨‹åº¦ (0 ~ 5)

ä¹Ÿå°±æ˜¯æœ‰ 8 ä¸ªè¾“å…¥ï¼Œ1 ä¸ªè¾“å‡º (å·¥èµ„)ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸‰å±‚çº¿æ€§æ¨¡å‹ï¼š

- ç¬¬ä¸€å±‚æ¥æ”¶ 8 ä¸ªè¾“å…¥è¿”å› 100 ä¸ªéšè—å€¼
- ç¬¬äºŒå±‚æ¥æ”¶ 100 ä¸ªéšè—å€¼è¿”å› 50 ä¸ªéšè—å€¼
- ç¬¬ä¸‰å±‚æ¥æ”¶ 50 ä¸ªéšè—å€¼è¿”å› 1 ä¸ªè¾“å‡º

å†™æˆä»£ç å¦‚ä¸‹ (è¿™é‡Œä½¿ç”¨äº† pandas ç±»åº“è¯»å– csvï¼Œä½¿ç”¨ `pip3 install pandas` å³å¯å®‰è£…)ï¼š

``` python
# å¼•ç”¨ pytorch å’Œ pandas
import pandas
import torch
from torch import nn

# å®šä¹‰æ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(in_features=8, out_features=100)
        self.layer2 = nn.Linear(in_features=100, out_features=50)
        self.layer3 = nn.Linear(in_features=50, out_features=1)

    def forward(self, x):
        hidden1 = nn.functional.relu(self.layer1(x))
        hidden2 = nn.functional.relu(self.layer2(hidden1))
        y = self.layer3(hidden2)
        return y

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
# è¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
torch.random.manual_seed(0)

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModel()

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD(model.parameters(), lr=0.0000001)

# ä» csv è¯»å–åŸå§‹æ•°æ®é›†
df = pandas.read_csv('salary.csv')
dataset_tensor = torch.tensor(df.values, dtype=torch.float)

# åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
random_indices = torch.randperm(dataset_tensor.shape[0])
training_indices = random_indices[:int(len(random_indices)*0.6)]
validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
testing_indices = random_indices[int(len(random_indices)*0.8):]
training_set_x = dataset_tensor[training_indices][:,:-1]
training_set_y = dataset_tensor[training_indices][:,-1:]
validating_set_x = dataset_tensor[validating_indices][:,:-1]
validating_set_y = dataset_tensor[validating_indices][:,-1:]
testing_set_x = dataset_tensor[testing_indices][:,:-1]
testing_set_y = dataset_tensor[testing_indices][:,-1:]

# å¼€å§‹è®­ç»ƒè¿‡ç¨‹
for epoch in range(1, 1000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.train()

    for batch in range(0, training_set_x.shape[0], 100):
        # åˆ‡åˆ†æ‰¹æ¬¡ï¼Œä¸€æ¬¡åªè®¡ç®— 100 ç»„æ•°æ®
        batch_x = training_set_x[batch:batch+100]
        batch_y = training_set_y[batch:batch+100]
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = model(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_function(predicted, batch_y)
        # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
        loss.backward()
        # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
        optimizer.step()
        # æ¸…ç©ºå¯¼å‡½æ•°å€¼
        optimizer.zero_grad()

    # æ£€æŸ¥éªŒè¯é›†
    # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.eval()
    predicted = model(validating_set_x)
    validating_accuracy = 1 - ((validating_set_y - predicted).abs() / validating_set_y).mean()
    print(f"validating x: {validating_set_x}, y: {validating_set_y}, predicted: {predicted}")
    print(f"validating accuracy: {validating_accuracy}")

# æ£€æŸ¥æµ‹è¯•é›†
predicted = model(testing_set_x)
testing_accuracy = 1 - ((testing_set_y - predicted).abs() / testing_set_y).mean()
print(f"testing x: {testing_set_x}, y: {testing_set_y}, predicted: {predicted}")
print(f"testing accuracy: {testing_accuracy}")

# æ‰‹åŠ¨è¾“å…¥æ•°æ®é¢„æµ‹è¾“å‡º
while True:
    try:
        print("enter input:")
        r = list(map(float, input().split(",")))
        x = torch.tensor(r).view(1, len(r))
        print(model(x)[0,0].item())
    except Exception as e:
        print("error:", e)
```

è¾“å‡ºå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°æœ€åæ²¡æœ‰å‚ä¸è®­ç»ƒçš„éªŒè¯é›†çš„æ­£ç¡®åº¦è¾¾åˆ°äº† 93.3% æµ‹è¯•é›†çš„æ­£ç¡®åº¦è¾¾åˆ°äº† 93.1%ï¼Œæ¨¡å‹æˆåŠŸçš„æ‘¸ç´¢å‡ºäº†æŸç§è§„å¾‹ï¼š

``` text
epoch: 1
validating x: tensor([[42.,  0., 16.,  ...,  5.,  5.,  5.],
        [28.,  0.,  0.,  ...,  4.,  0.,  0.],
        [23.,  0.,  3.,  ...,  0.,  1.,  1.],
        ...,
        [44.,  1., 15.,  ...,  2.,  0.,  2.],
        [30.,  0.,  1.,  ...,  1.,  1.,  2.],
        [50.,  1., 18.,  ...,  5.,  5.,  2.]]), y: tensor([[24500.],
        [12500.],
        [17500.],
        ...,
        [10500.],
        [15000.],
        [16000.]]), predicted: tensor([[27604.2578],
        [15934.7607],
        [14536.8984],
        ...,
        [23678.5547],
        [18189.6953],
        [29968.8789]], grad_fn=<AddmmBackward>)
validating accuracy: 0.661293625831604
epoch: 2
validating x: tensor([[42.,  0., 16.,  ...,  5.,  5.,  5.],
        [28.,  0.,  0.,  ...,  4.,  0.,  0.],
        [23.,  0.,  3.,  ...,  0.,  1.,  1.],
        ...,
        [44.,  1., 15.,  ...,  2.,  0.,  2.],
        [30.,  0.,  1.,  ...,  1.,  1.,  2.],
        [50.,  1., 18.,  ...,  5.,  5.,  2.]]), y: tensor([[24500.],
        [12500.],
        [17500.],
        ...,
        [10500.],
        [15000.],
        [16000.]]), predicted: tensor([[29718.2441],
        [15790.3799],
        [15312.5791],
        ...,
        [23395.9668],
        [18672.0234],
        [31012.4062]], grad_fn=<AddmmBackward>)
validating accuracy: 0.6694601774215698

çœç•¥é€”ä¸­è¾“å‡º

epoch: 999
validating x: tensor([[42.,  0., 16.,  ...,  5.,  5.,  5.],
        [28.,  0.,  0.,  ...,  4.,  0.,  0.],
        [23.,  0.,  3.,  ...,  0.,  1.,  1.],
        ...,
        [44.,  1., 15.,  ...,  2.,  0.,  2.],
        [30.,  0.,  1.,  ...,  1.,  1.,  2.],
        [50.,  1., 18.,  ...,  5.,  5.,  2.]]), y: tensor([[24500.],
        [12500.],
        [17500.],
        ...,
        [10500.],
        [15000.],
        [16000.]]), predicted: tensor([[22978.7656],
        [13050.8018],
        [18396.5176],
        ...,
        [11449.5059],
        [14791.2969],
        [16635.2578]], grad_fn=<AddmmBackward>)
validating accuracy: 0.9311849474906921
testing x: tensor([[48.,  1., 18.,  ...,  5.,  0.,  5.],
        [22.,  1.,  2.,  ...,  2.,  1.,  2.],
        [24.,  0.,  1.,  ...,  3.,  2.,  0.],
        ...,
        [24.,  0.,  4.,  ...,  0.,  1.,  1.],
        [39.,  0.,  0.,  ...,  0.,  5.,  5.],
        [36.,  0.,  5.,  ...,  3.,  0.,  3.]]), y: tensor([[14000.],
        [10500.],
        [13000.],
        ...,
        [15500.],
        [12000.],
        [19000.]]), predicted: tensor([[15481.9062],
        [11011.7266],
        [12192.7949],
        ...,
        [16219.3027],
        [11074.0420],
        [20305.3516]], grad_fn=<AddmmBackward>)
testing accuracy: 0.9330180883407593
enter input:
```

æœ€åæˆ‘ä»¬æ‰‹åŠ¨è¾“å…¥ç å†œæ¡ä»¶å¯ä»¥å¾—å‡ºé¢„æµ‹è¾“å‡º (35 å²ç”· 10 å¹´ç»éªŒ Java 5 NET 2 JS 1 CSS 1 HTML 2 å¤§çº¦å¯æ‹¿ 26k ğŸ˜¤)ï¼š

``` text
enter input:
35,0,10,5,2,1,1,2
26790.982421875
```

è™½ç„¶è®­ç»ƒæˆåŠŸäº†ï¼Œä½†ä»¥ä¸Šä»£ç è¿˜æœ‰å‡ ä¸ªé—®é¢˜ï¼š

- å­¦ä¹ æ¯”ç‡è®¾ç½®çš„éå¸¸ä½ (0.0000001)ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰å¯¹æ•°æ®è¿›è¡Œæ­£è§„åŒ–å¤„ç†
- æ€»æ˜¯ä¼šå›ºå®šè®­ç»ƒ 1000 æ¬¡ï¼Œä¸èƒ½åƒç¬¬ä¸€ç¯‡æ–‡ç« æåˆ°è¿‡çš„é‚£æ ·è‡ªåŠ¨æ£€æµ‹å“ªä¸€æ¬¡çš„æ­£ç¡®åº¦æœ€é«˜ï¼Œæ²¡æœ‰è€ƒè™‘è¿‡æ‹Ÿåˆé—®é¢˜
- ä¸èƒ½æŠŠè®­ç»ƒç»“æœä¿å­˜åˆ°ç¡¬ç›˜ï¼Œæ¯æ¬¡è¿è¡Œéƒ½éœ€è¦ä»å¤´è®­ç»ƒ
- éœ€è¦æŠŠæ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§è¯»å–åˆ°å†…å­˜ä¸­ï¼Œæ•°æ®è¿‡å¤šæ—¶å†…å­˜å¯èƒ½ä¸å¤Ÿç”¨
- æ²¡æœ‰æä¾›ä¸€ä¸ªä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ¥å£

è¿™äº›é—®é¢˜éƒ½ä¼šåœ¨ä¸‹ç¯‡æ–‡ç« ä¸€ä¸ªä¸ªè§£å†³ã€‚

## å†™åœ¨æœ€å

åœ¨è¿™ä¸€ç¯‡æˆ‘ä»¬ç»ˆäºçœ‹åˆ°æ€æ ·åº”ç”¨æœºå™¨å­¦ä¹ åˆ°æ›´å¤æ‚çš„æ•°æ®ä¸­ï¼Œä½†è·¯è¿˜æœ‰å¾ˆé•¿ğŸƒã€‚

ä¸‹ä¸€ç¯‡æˆ‘ä»¬ä¼šåšä¸ªç¨æ¯ï¼Œä»‹ç»ä¸€äº›è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æŠ€å·§ (æœ¬æ¥æ‰“ç®—å†™åˆ°è¿™ç¯‡é‡Œé¢çš„ï¼Œä½†è¿™ç¯‡å·²ç»è¶³å¤Ÿé•¿äº†)ï¼Œå†ä¸‹ä¸€ç¯‡ä»‹ç»å¯ä»¥å¤„ç†ä¸å®šé•¿è¿ç»­æ•°æ®çš„é€’å½’æ¨¡å‹ã€‚
