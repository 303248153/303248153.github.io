# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (å è¡¥å……) - å¯¹è±¡è¯†åˆ« Faster-RCNN - æ”¹è¿›è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©çš„æ¨¡å‹

åœ¨[å‰ä¸€ç¯‡æ–‡ç« ](https://www.cnblogs.com/zkweb/p/14078501.html)ä¸­æˆ‘ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ Faster-RCNN æ¨¡å‹å®ç°è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©ï¼Œè¿™ä¸€ç¯‡æˆ‘å°†ä»‹ç»å¦‚ä½•æ”¹è¿›æ¨¡å‹çš„ç²¾åº¦ï¼Œå¹¶ä¸”ä»‹ç»å¦‚ä½•æ”¯æŒè§†é¢‘è¯†åˆ«ã€‚è¿‡å»çš„æ–‡ç« æˆ‘åŸºæœ¬ä¸Šéƒ½åªä»‹ç»æ¨¡å‹çš„å®ç°åŸç†ä¸ä½¿ç”¨ä¾‹å­ï¼Œæ²¡æœ‰è¿‡åº¦è¿½æ±‚ç²¾ç¡®ç‡ï¼Œè¿™æ˜¯ä¸ºäº†è®©è¯»è€…æŠ›å¼€ç»†èŠ‚ç†è§£ï¼Œä½†å®é™…åº”ç”¨æœºå™¨å­¦ä¹ çš„æ—¶å€™æˆ‘ä»¬è¿˜æ˜¯éœ€è¦å¯¹æ¨¡å‹ä½œå‡ºå„ç§ä¿®æ”¹ä»¥è¾¾åˆ°æ›´å¥½çš„ç»“æœã€‚æœ¬ç¯‡æ–‡ç« å°±æ˜¯æ”¹è¿›å¯¹è±¡è¯†åˆ«æ¨¡å‹çš„ä¾‹å­ï¼Œè¿™ä¸ªä¾‹å­ä½¿ç”¨çš„æ–¹æ³•ä¸ä¸€å®šé€‚ç”¨äºå…¶ä»–åœºæ™¯ï¼Œä½†åº”è¯¥å¯ä»¥ç»™ä½ å¸¦æ¥ä¸€äº›å¯å‘ğŸ¤—ã€‚

é¦–å…ˆå±•ç¤ºä¸‹æ”¹è¿›å‰åçš„æ•ˆæœï¼š

æ”¹è¿›å‰ (è§†é¢‘ 1)

![](wuhan.output_old.gif)

æ”¹è¿›å (è§†é¢‘ 1)

![](wuhan.output.gif)

æ”¹è¿›å‰ (è§†é¢‘ 2)

![](shanghai.output_old.gif)

æ”¹è¿›å (è§†é¢‘ 2)

![](shanghai.output.gif)

æ¥ä¸‹æ¥æˆ‘å°†ä¼šä»‹ç»æ”¹è¿›äº†å“ªäº›åœ°æ–¹ï¼Œå¹¶ä¸”æœ€åä¼šç»™å‡ºæ”¹è¿›åçš„å®Œæ•´ä»£ç ã€‚

## æ”¹è¿›å†…å®¹

### æ‰©å……æ•°æ®é›†

å†³å®šæœºå™¨å­¦ä¹ è®­ç»ƒæ•ˆæœæœ€å…³é”®çš„å› ç´ æ˜¯ä»€ä¹ˆï¼Œæ˜¯æ¨¡å‹å—ğŸ¥ºï¼Ÿå¹¶ä¸æ˜¯ï¼Œæ¯”æ¨¡å‹æ›´å…³é”®çš„æ˜¯æ•°æ®é›†çš„è´¨é‡ğŸ˜ ï¼Œå³ä½¿æ¨¡å‹å†å¼ºå¤§æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ä¸€æ ·è®­ç»ƒä¸å‡ºä»€ä¹ˆæˆæœã€‚æˆ‘ä»¬æ¥çœ‹çœ‹å‰ä¸€ç¯‡ä½¿ç”¨çš„æ•°æ®é›†ï¼š

https://www.kaggle.com/andrewmvd/face-mask-detection

è¿™ä¸ªæ•°æ®é›†åŒ…å«äº† 853 å¼ å›¾ç‰‡ (éƒ¨åˆ†å›¾ç‰‡æ²¡æœ‰ä½¿ç”¨)ï¼Œå…¶ä¸­å„ä¸ªåˆ†ç±»çš„æ•°é‡å¦‚ä¸‹ï¼š

- æˆ´å£ç½©çš„åŒºåŸŸ (with_mask): 3232 ä¸ª
- ä¸æˆ´å£ç½©çš„åŒºåŸŸ (without_mask): 717 ä¸ª
- å¸¦äº†å£ç½©ä½†å§¿åŠ¿ä¸æ­£ç¡®çš„åŒºåŸŸ (mask_weared_incorrect): 123 ä¸ª

æ˜¯ä¸æ˜¯æ„Ÿè§‰æ¯”è¾ƒå°‘ï¼Ÿå¦‚æœéœ€è¦è‡ªå·±é‡‡é›†æ•°æ®ï¼Œé‚£ä¹ˆå°±å¾—åŠ ç­åŠ ç‚¹å¤šé‡‡é›†ä¸€äº›ğŸ˜•ã€‚è€Œè¿™æ¬¡ç”¨çš„æ˜¯ç°æˆçš„æ•°æ®é›†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å»æ‰¾ä¸€æ‰¾æœ‰æ²¡æœ‰å…¶ä»–æ•°æ®é›†å¯ä»¥ä¸€èµ·ç”¨ï¼Œè¿˜è®°å¾—[ä»‹ç» Fast-RCNN çš„æ–‡ç« ](https://www.cnblogs.com/zkweb/p/14048685.html)å—ï¼Ÿè¿™ç¯‡æ–‡ç« ç”¨çš„æ•°æ®é›†åªåŒ…å«äº†äººè„¸åŒºåŸŸï¼Œæ²¡æœ‰åŒ…å«æ˜¯å¦æˆ´å£ç½©çš„æ ‡è®°ï¼Œä½†ä»”ç»†çœ‹æ•°æ®å†…å®¹ä¼šå‘ç°å›¾ç‰‡é‡Œé¢çš„äººè„¸éƒ½æ²¡æœ‰æˆ´å£ç½©ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›æ•°æ®å…¨éƒ¨å½“æˆä¸æˆ´å£ç½©çš„åŒºåŸŸï¼Œä¸€å…±æœ‰ 24533 ä¸ªï¼š

https://www.kaggle.com/vin1234/count-the-number-of-faces-present-in-an-image

åŠ åœ¨ä¸€èµ·ä»¥åï¼š

- æˆ´å£ç½©çš„åŒºåŸŸ (with_mask): 3232 ä¸ª
- ä¸æˆ´å£ç½©çš„åŒºåŸŸ (without_mask): 717+24533 = 25250 ä¸ª
- å¸¦äº†å£ç½©ä½†å§¿åŠ¿ä¸æ­£ç¡®çš„åŒºåŸŸ (mask_weared_incorrect): 123 ä¸ª

å†ä»”ç»†çœ‹ä¸€ä¸‹ï¼Œå¸¦äº†å£ç½©ä½†å§¿åŠ¿ä¸æ­£ç¡®çš„åŒºåŸŸçš„æ•°é‡æ˜æ˜¾å¤ªå°‘äº†ï¼Œä¸è¶³ä»¥åšå‡ºæ­£ç¡®çš„åˆ¤æ–­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›åŒºåŸŸå…¨éƒ¨å½’åˆ°æˆ´å£ç½©çš„åŒºåŸŸé‡Œé¢ï¼Œä¹Ÿå°±æ˜¯åªåˆ¤æ–­ä½ æˆ´å£ç½©ï¼Œä½ æˆ´çš„å§¿åŠ¿å¯¹ä¸å¯¹è€å­ç®¡ä¸ç€ğŸ¤¬ã€‚åŠ åœ¨ä¸€èµ·ä»¥åï¼š

- æˆ´å£ç½©çš„åŒºåŸŸ (with_mask): 3232+123=3355 ä¸ª
- ä¸æˆ´å£ç½©çš„åŒºåŸŸ (without_mask): 717+24533 = 25250 ä¸ª

å¥½äº†ï¼Œå†æƒ³æƒ³æœ‰æ²¡æœ‰åŠæ³•å¯ä»¥å¢åŠ æ•°æ®é‡ï¼Ÿå…¶å®æœ‰ä¸€ä¸ªéå¸¸ç®€å•çš„æ–¹æ³•ï¼ŒæŠŠå›¾ç‰‡å·¦å³ç¿»è½¬å°±å¯ä»¥è®©æ•°æ®é‡å˜æˆä¸¤å€ï¼š

![](./01.png)

é™¤äº†å·¦å³ç¿»è½¬ä»¥å¤–æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ—‹è½¬å›¾ç‰‡ï¼Œæ‰©å¤§ç¼©å°å›¾ç‰‡ï¼Œæ·»åŠ å™ªç‚¹ç­‰æ–¹å¼å¢åŠ æ•°æ®é‡ã€‚å·¦å³ç¿»è½¬ä»¥åçš„æœ€ç»ˆæ•°æ®é‡å¦‚ä¸‹ï¼Œæ€»æ•°æ®é‡å¤§æ¦‚æ˜¯åŸæ¥çš„ 14 å€ğŸ˜±ï¼š

- æˆ´å£ç½©çš„åŒºåŸŸ (with_mask): (3232+123)*2=6710 ä¸ª
- ä¸æˆ´å£ç½©çš„åŒºåŸŸ (without_mask): (717+24533)*2 = 50500 ä¸ª

è¯»å–ä¸¤ä¸ªæ•°æ®é›†çš„ä»£ç å¦‚ä¸‹ï¼ˆæœ€åä¼šç»™å‡ºå®Œæ•´ä»£ç ï¼‰ï¼š

``` python
# åŠ è½½å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
# { (è·¯å¾„, æ˜¯å¦å·¦å³ç¿»è½¬): [ åŒºåŸŸä¸åˆ†ç±», åŒºåŸŸä¸åˆ†ç±», .. ] }
# åŒä¸€å¼ å›¾ç‰‡å·¦å³ç¿»è½¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ–°çš„æ•°æ®ï¼Œè®©æ•°æ®é‡ç¿»å€
box_map = defaultdict(lambda: [])
for filename in os.listdir(DATASET_1_IMAGE_DIR):
    # ä»ç¬¬ä¸€ä¸ªæ•°æ®é›†åŠ è½½
    xml_path = os.path.join(DATASET_1_ANNOTATION_DIR, filename.split(".")[0] + ".xml")
    if not os.path.isfile(xml_path):
        continue
    tree = ET.ElementTree(file=xml_path)
    objects = tree.findall("object")
    path = os.path.join(DATASET_1_IMAGE_DIR, filename)
    for obj in objects:
        class_name = obj.find("name").text
        x1 = int(obj.find("bndbox/xmin").text)
        x2 = int(obj.find("bndbox/xmax").text)
        y1 = int(obj.find("bndbox/ymin").text)
        y2 = int(obj.find("bndbox/ymax").text)
        if class_name == "mask_weared_incorrect":
            # ä½©æˆ´å£ç½©ä¸æ­£ç¡®çš„æ ·æœ¬æ•°é‡å¤ªå°‘ (åªæœ‰ 123)ï¼Œæ¨¡å‹æ— æ³•å­¦ä¹ ï¼Œè¿™é‡Œå…¨åˆå¹¶åˆ°æˆ´å£ç½©çš„æ ·æœ¬
            class_name = "with_mask"
        box_map[(path, False)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))
        box_map[(path, True)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))
df = pandas.read_csv(DATASET_2_BOX_CSV_PATH)
for row in df.values:
    # ä»ç¬¬äºŒä¸ªæ•°æ®é›†åŠ è½½ï¼Œè¿™ä¸ªæ•°æ®é›†åªåŒ…å«æ²¡æœ‰æˆ´å£ç½©çš„å›¾ç‰‡
    filename, width, height, x1, y1, x2, y2 = row[:7]
    path = os.path.join(DATASET_2_IMAGE_DIR, filename)
    box_map[(path, False)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING["without_mask"]))
    box_map[(path, True)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING["without_mask"]))
# æ‰“ä¹±æ•°æ®é›† (å› ä¸ºç¬¬äºŒä¸ªæ•°æ®é›†åªæœ‰ä¸æˆ´å£ç½©çš„å›¾ç‰‡)
box_list = list(box_map.items())
random.shuffle(box_list)
print(f"found {len(box_list)} images")
```

ç¿»è½¬å›¾ç‰‡çš„ä»£ç å¦‚ä¸‹ï¼ŒåŒæ—¶ä¼šç¿»è½¬åŒºåŸŸçš„ x åæ ‡ (å›¾ç‰‡å®½åº¦ - åŸ x åæ ‡ - åŒºåŸŸå®½åº¦)ï¼š

``` python
for (image_path, flip), original_boxes_labels in box_list:
    with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
        sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
        if flip:
            img = resize_image(img_original.transpose(Image.FLIP_LEFT_RIGHT)) # ç¿»è½¬ç„¶åç¼©æ”¾å›¾ç‰‡
        else:
            img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
        image_index = len(image_tensors) # å›¾ç‰‡åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•å€¼
        image_tensors.append(image_to_tensor(img)) # æ·»åŠ å›¾ç‰‡åˆ°åˆ—è¡¨
        true_boxes_labels = [] # å›¾ç‰‡å¯¹åº”çš„çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
    # æ·»åŠ çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
    for box_label in original_boxes_labels:
        x, y, w, h, label = box_label
        if flip: # ç¿»è½¬åæ ‡
            x = sw - x - w
```

æ•°æ®é‡å˜å¤šä»¥åä¼šéœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´ï¼Œå‰ä¸€ç¯‡æ–‡ç« åœ¨ GTX1650 æ˜¾å¡ä¸Šè®­ç»ƒå¤§æ¦‚éœ€è¦ 3 å°æ—¶ï¼Œè€Œè¿™ä¸€ç¯‡åˆ™éœ€è¦ 15 å°æ—¶å·¦å³ğŸã€‚

### è°ƒæ•´ç”Ÿæˆé”šç‚¹çš„å‚æ•°

æˆ‘ä»¬å¯ä»¥è®©æ¨¡å‹æ›´è´´åˆæ•°æ®ä»¥æ”¹è¿›è®­ç»ƒæ•ˆæœã€‚åœ¨å‰ä¸€ç¯‡æ–‡ç« æˆ‘ä»‹ç»äº† Faster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œä¼šæ ¹æ®é”šç‚¹ (Anchor) åˆ¤æ–­å›¾ç‰‡ä¸­çš„å„ä¸ªéƒ¨åˆ†æ˜¯å¦åŒ…å«å¯¹è±¡ï¼š

![](./02.png)

å› ä¸º CNN æ¨¡å‹è¾“å‡ºçŸ©é˜µçš„å¤§å°æ˜¯ `é€šé“æ•°é‡,å›¾ç‰‡é•¿åº¦/8,å›¾ç‰‡å®½åº¦/8`ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªé”šç‚¹å¯¹åº” 8x8 åƒç´ çš„åŒºåŸŸï¼ŒåŒºåŸŸç”Ÿæˆç½‘ç»œéœ€è¦æ ¹æ® 8x8 åƒç´ çš„åŒºåŸŸåˆ¤æ–­è¿™ä¸ªåŒºåŸŸæ˜¯å¦æœ‰å¯èƒ½åŒ…å«å¯¹è±¡ã€‚è¿™ç¯‡ä½¿ç”¨çš„ä»£ç åœ¨å¤„ç†å›¾ç‰‡ä¹‹å‰ä¼šå…ˆæŠŠå›¾ç‰‡ç¼©æ”¾åˆ° 256x192ï¼Œ8x8 çš„åŒºåŸŸç›¸å¯¹èµ·æ¥ä¼¼ä¹è¿‡å°äº†ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠé”šç‚¹åŒºåŸŸæ‰©å¤§åˆ° 16x16ï¼Œä½¿å¾—åŒºåŸŸç”Ÿæˆç½‘ç»œåˆ¤æ–­èµ·æ¥æœ‰æ›´å……åˆ†çš„ä¾æ®ã€‚æ‰©å¤§é”šç‚¹åŒºåŸŸåŒæ—¶éœ€è¦ä¿®æ”¹ CNN æ¨¡å‹ï¼Œä½¿å¾—è¾“å‡ºçŸ©é˜µå¤§å°ä¸º `é€šé“æ•°é‡,å›¾ç‰‡é•¿åº¦/16,å›¾ç‰‡å®½åº¦/16`ï¼Œè¿™ä¸ªä¿®æ”¹å°†ä¼šåœ¨åé¢ä»‹ç»ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯æ‰©å¤§é”šç‚¹åŒºåŸŸä»¥åä¼šå‡å¼±æ£€æµ‹å°å¯¹è±¡çš„èƒ½åŠ›ï¼Œä½†è¿™ç¯‡çš„å›¾ç‰‡ä¸­çš„äººè„¸åŒºåŸŸåŸºæœ¬ä¸Šéƒ½åœ¨ 16x16 ä»¥ä¸Šï¼Œæ‰€ä»¥ä¸ä¼šå—åˆ°å½±å“ã€‚

æ­¤å¤–ï¼Œå‰ä¸€ç¯‡è¿˜ä»‹ç»äº†æ¯ä¸ªé”šç‚¹éƒ½ä¼šå¯¹åº”å¤šä¸ªå½¢çŠ¶ï¼š

![](./03.png)

é€šè¿‡è§‚å¯Ÿæ•°æ®æˆ‘ä»¬å¯ä»¥å‘ç°äººè„¸çš„é•¿å®½æ¯”ä¾‹æ¥è¿‘ 1:1ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¸éœ€è¦æ£€æµ‹äººè„¸ä»¥å¤–çš„ä¸œè¥¿ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åˆ æ‰é•¿å®½æ¯”ä¾‹ 1:2 ä¸ 2:1 çš„å½¢çŠ¶ï¼Œå‡å°‘æ¨¡å‹çš„è®¡ç®—é‡ã€‚

æ€»ç»“èµ·æ¥æˆ‘ä»¬å¯ä»¥è¿™æ ·ä¿®æ”¹ç”Ÿæˆé”šç‚¹çš„å‚æ•°ï¼š

ä¿®æ”¹å‰

``` python
AnchorSpan = 8 # é”šç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œåº”è¯¥ç­‰äºåŸæœ‰é•¿å®½ / resnet è¾“å‡ºé•¿å®½
AnchorScales = (0.5, 1, 2, 3, 4, 5, 6) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨
AnchorAspects = ((1, 2), (1, 1), (2, 1)) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
```

ä¿®æ”¹å

``` python
AnchorSpan = 16 # é”šç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œåº”è¯¥ç­‰äºåŸæœ‰é•¿å®½ / resnet è¾“å‡ºé•¿å®½
AnchorScales = (1, 2, 4, 6, 8) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨
AnchorAspects = ((1, 1),) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
```

åœ¨è¿™é‡Œæˆ‘ä»¬å­¦åˆ°äº†åº”è¯¥æ ¹æ®æ•°æ®å’Œæ£€æµ‹åœºæ™¯æ¥å†³å®šé”šç‚¹åŒºåŸŸå¤§å°å’Œé•¿å®½æ¯”ä¾‹ï¼Œå¦‚æœéœ€è¦æ£€æµ‹çš„ç‰©ä½“ç›¸å¯¹å›¾ç‰‡éƒ½æ¯”è¾ƒå¤§ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç›¸åº”çš„å¢åŠ é”šç‚¹åŒºåŸŸå¤§å°ï¼›å¦‚æœéœ€è¦æ£€æµ‹çš„ç‰©ä½“å½¢çŠ¶æ¯”è¾ƒå›ºå®šï¼Œé‚£ä¹ˆå°±å¯ä»¥ç›¸åº”è°ƒæ•´é•¿å®½æ¯”ä¾‹ï¼Œä¾‹å¦‚æ£€æµ‹è½¦è¾†å¯ä»¥ç”¨ 1:2ï¼Œæ£€æµ‹è¡Œäººå¯ä»¥ç”¨ 3:1ï¼Œæ£€æµ‹è½¦ç‰Œå¯ä»¥ç”¨ 1:3 ç­‰ç­‰ã€‚

### ä¿®æ”¹æ¨¡å‹

å› ä¸ºä¸Šé¢ä¿®æ”¹äº†é”šç‚¹ä¹‹é—´çš„è·ç¦»ä» 8x8 åˆ° 16x16ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ CNN æ¨¡å‹è¾“å‡ºçš„çŸ©é˜µå¤§å°ä» `é€šé“æ•°é‡,å›¾ç‰‡é•¿åº¦/8,å›¾ç‰‡å®½åº¦/8` ä¿®æ”¹åˆ° `é€šé“æ•°é‡,å›¾ç‰‡é•¿åº¦/16,å›¾ç‰‡å®½åº¦/16`ï¼Œè¿™ä¸ªä¿®æ”¹éå¸¸çš„ç®€å•ï¼Œå†åŠ ä¸€å±‚å·ç§¯å±‚å³å¯ã€‚å› ä¸ºè¿™ç¯‡ä½¿ç”¨çš„æ˜¯ Resnet æ¨¡å‹ï¼Œè¿™é‡Œä¼šåœ¨åé¢å¤šåŠ ä¸€ä¸ªå—ï¼Œä»£ç å¦‚ä¸‹ï¼š

ä¿®æ”¹å‰

``` python
self.rpn_resnet = nn.Sequential(
    nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
    nn.BatchNorm2d(self.previous_channels_out),
    nn.ReLU(inplace=True),
    self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=1),
    self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
    self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
    self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2))
```

ä¿®æ”¹å

``` python
self.rpn_resnet = nn.Sequential(
    nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
    nn.BatchNorm2d(self.previous_channels_out),
    nn.ReLU(inplace=True),
    self._make_layer(BasicBlock, channels_out=8, num_blocks=2, stride=1),
    self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=2),
    self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
    self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
    self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2))
```

`self.cls_resnet` ä¹Ÿéœ€è¦åšå‡ºåŒæ ·çš„ä¿®æ”¹ã€‚

æ­¤å¤–ä¸ºäº†é€‚åº”æ›´å¤šçš„æ•°æ®é‡ï¼Œè¿™é‡Œè¿˜å¢åŠ äº†æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾åç¼©æ”¾åˆ°çš„å¤§å°ï¼š

``` python
# æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾åç¼©æ”¾åˆ°çš„å¤§å°
self.pooling_size = 16
```

è¿™æ ·åˆ¤æ–­åˆ†ç±»çš„æ—¶å€™ä¼šä½¿ç”¨ `é€šé“æ•°é‡x16x16`ï¼Œå³ `128x16x16` çš„æ•°æ®ã€‚éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¹ˆåšä¸ä¸€å®šæœ‰å¥½å¤„ï¼Œåˆ¤æ–­åˆ†ç±»ä½¿ç”¨çš„æ•°æ®è¶Šå¤§å°±è¶Šæœ‰å¯èƒ½å‘ç”Ÿè¿‡æ‹Ÿåˆç°è±¡ (è®­ç»ƒé›†æ­£ç¡®ç‡å¾ˆé«˜ä½†éªŒè¯é›†æ­£ç¡®ç‡å´ä¸è¡Œï¼Œä¸èƒ½ç”¨äºè¯†åˆ«æœªçŸ¥æ•°æ®)ï¼Œå®é™…éœ€è¦æ ¹æ®è®­ç»ƒç»“æœåšå‡ºè°ƒæ•´ã€‚

### è¾“å‡ºåˆ†æ•°

æˆ‘ä»¬çŸ¥é“åŒºåŸŸç”Ÿæˆç½‘ç»œä¼šé’ˆå¯¹å„ä¸ªé”šç‚¹çš„å„ä¸ªå½¢çŠ¶è¾“å‡ºæ˜¯å¦å¯èƒ½åŒ…å«å¯¹è±¡ï¼Œè¾“å‡ºå€¼è¶Šæ¥è¿‘ 1 é‚£ä¹ˆå°±è¶Šå¯èƒ½åŒ…å«å¯¹è±¡ï¼Œè¶Šæ¥è¿‘ 0 é‚£ä¹ˆå°±è¶Šä¸å¯èƒ½åŒ…å«å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªè¾“å‡ºå€¼å½“ä½œåˆ†æ•°ï¼Œåˆ†æ•°è¶Šé«˜ä»£è¡¨åŒºåŸŸè¶Šæœ‰å¯èƒ½åŒ…å«å¯¹è±¡ã€‚æ¥ä¸‹æ¥æ ‡ç­¾åˆ†ç±»ç½‘ç»œä¼šé’ˆå¯¹åŒºåŸŸç”Ÿæˆç½‘ç»œç»™å‡ºçš„åŒºåŸŸè¿›è¡Œè¯†åˆ«ï¼Œæ¯ä¸ªåŒºåŸŸçš„æ¯ä¸ªåˆ†ç±»éƒ½ä¼šè¾“å‡ºä¸€ä¸ªå€¼ï¼Œç»è¿‡ softmax è®¡ç®—ä»¥åå¾—å‡ºå„ä¸ªåˆ†ç±»çš„æ¦‚ç‡ (åŠ èµ·æ¥ä¼šç­‰äº 1)ï¼Œè¿™ä¸ªæ¦‚ç‡ä¹Ÿå¯ä»¥æ‹¿æ¥ä½œä¸ºåˆ†æ•°ä½¿ç”¨ã€‚

æœ€ç»ˆæˆ‘ä»¬å¯ä»¥ç»™ Faster-RCNN è¾“å‡ºçš„å„ä¸ªåŒ…å«å¯¹è±¡çš„åŒºåŸŸèµ‹äºˆä¸€ä¸ªåˆ†æ•°ï¼š

åˆ†æ•° = åŒºåŸŸç”Ÿæˆç½‘ç»œè¾“å‡ºå€¼ * æœ€å¤§å€¼(softmax(æ ‡ç­¾åˆ†ç±»ç½‘ç»œå„ä¸ªåˆ†ç±»è¾“å‡ºå€¼))

åˆ†æ•°å°†ä¼šä»‹äº 0 ~ 1 ä¹‹é—´ã€‚

åŸåˆ™ä¸Šåˆ†æ•°è¶Šé«˜ä»£è¡¨æ¨¡å‹å¯¹è¿™ä¸ªåŒºåŸŸè¶Šæœ‰æŠŠæ¡ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªåˆ†æ•°å¯ä»¥ç”¨æ¥è°ƒæ•´é˜ˆå€¼ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è¿™ä¸ªåˆ†æ•°æ¥æ›´é«˜åˆå¹¶é¢„æµ‹ç»“æœåŒºåŸŸçš„ç®—æ³•ã€‚ä½†å®é™…ä¸Šä½ å¯èƒ½ä¼šçœ‹åˆ°åˆ†æ•°ä¸º 1 ä½†ç»“æœæ˜¯é”™è¯¯çš„åŒºåŸŸï¼Œæ‰€ä»¥åªèƒ½è¯´åŸåˆ™ä¸Šã€‚

è¿”å›åˆ†æ•°çš„ä»£ç è¯·å‚è€ƒåé¢å®Œæ•´ä»£ç çš„ `MyModel.forward` å‡½æ•°ä¸­å…³äº `rpn_score` ä¸ `cls_score` çš„éƒ¨åˆ†ã€‚

### æ›´æ”¹åˆå¹¶é¢„æµ‹ç»“æœåŒºåŸŸçš„ç®—æ³•

è¿˜è®°å¾—[ä»‹ç» Fast-RCNN çš„æ–‡ç« ](https://www.cnblogs.com/zkweb/p/14048685.html)é‡Œé¢ï¼Œæˆ‘æåˆ°äº†åˆå¹¶ç»“æœåŒºåŸŸçš„å‡ ä¸ªæ–¹æ³•ï¼š

- ä½¿ç”¨æœ€å·¦ï¼Œæœ€å³ï¼Œæœ€ä¸Šï¼Œæˆ–è€…æœ€ä¸‹çš„åŒºåŸŸ
- ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒºåŸŸ (åŒºåŸŸé€‰å–ç®—æ³•ä¼šæŒ‰å‡ºç°å¯¹è±¡çš„å¯èƒ½æ€§æ’åº)
- ç»“åˆæ‰€æœ‰é‡åˆçš„åŒºåŸŸ (å¦‚æœåŒºåŸŸè°ƒæ•´æ•ˆæœä¸è¡Œï¼Œåˆ™å¯èƒ½å‡ºç°ç»“æœåŒºåŸŸæ¯”çœŸå®åŒºåŸŸå¤§å¾ˆå¤šçš„é—®é¢˜)

å‰ä¸€ç¯‡æ–‡ç« çš„ Faster-RCNN æ¨¡å‹ä½¿ç”¨äº†ç¬¬ä¸‰ä¸ªæ–¹æ³•ï¼Œä½†ä¸Šé¢æˆ‘ä»¬è¾“å‡ºåˆ†æ•°ä»¥åå¯ä»¥é€‰æ‹©ç¬¬äºŒä¸ªæ–¹æ³•ï¼Œå³å…ˆæŒ‰åˆ†æ•°å¯¹åŒºåŸŸè¿›è¡Œæ’åºï¼Œç„¶åé€‰æ‹©é‡åˆçš„åŒºåŸŸä¸­åˆ†æ•°æœ€é«˜çš„åŒºåŸŸä½œä¸ºç»“æœï¼Œå¹¶å»é™¤å…¶ä»–é‡åˆçš„åŒºåŸŸã€‚è¿™ä¸ªæ–¹æ³•ä¹Ÿç§°ä½œ NMS (Non Max Suppression) æ³•ï¼š

![](./04.png)

ä½¿ç”¨è¿™ç§æ–¹æ³•çš„å¥½å¤„æ˜¯è¾“å‡ºçš„åŒºåŸŸå°†ä¼šæ›´å°ï¼Œçœ‹èµ·æ¥æ›´ç²¾ç¡®ï¼Œä½†å¦‚æœåœºæ™¯æ˜¯æ£€æµ‹éšœç¢ç‰©é‚£ä¹ˆæœ€å¥½è¿˜æ˜¯ä½¿ç”¨ç¬¬ä¸‰ç§æ–¹æ³•ğŸ¤•ã€‚

åˆå¹¶é¢„æµ‹ç»“æœåŒºåŸŸçš„ä»£ç å¦‚ä¸‹ï¼Œè¿™é‡Œæˆ‘æŠŠå‡½æ•°å†™åˆ° `MyModel` ç±»é‡Œé¢äº†ï¼š

``` python
# åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶é‡å åŒºåŸŸçš„é‡å ç‡é˜ˆå€¼
IOU_MERGE_THRESHOLD = 0.30
# æ˜¯å¦ä½¿ç”¨ NMS ç®—æ³•åˆå¹¶åŒºåŸŸ
USE_NMS_ALGORITHM = True

@staticmethod
def merge_predicted_result(cls_result):
    """åˆå¹¶é¢„æµ‹ç»“æœåŒºåŸŸ"""
    # è®°å½•é‡å çš„ç»“æœåŒºåŸŸ, ç»“æœæ˜¯ [ [(æ ‡ç­¾, åŒºåŸŸ, RPN åˆ†æ•°, æ ‡ç­¾è¯†åˆ«åˆ†æ•°)], ... ]
    final_result = []
    for label, box, rpn_score, cls_score in cls_result:
        for index in range(len(final_result)):
            exists_results = final_result[index]
            if any(calc_iou(box, r[1]) > IOU_MERGE_THRESHOLD for r in exists_results):
                exists_results.append((label, box, rpn_score, cls_score))
                break
        else:
            final_result.append([(label, box, rpn_score, cls_score)])
    # åˆå¹¶é‡å çš„ç»“æœåŒºåŸŸ
    # ä½¿ç”¨ NMS ç®—æ³•: RPN åˆ†æ•° * æ ‡ç­¾è¯†åˆ«åˆ†æ•° æœ€é«˜çš„åŒºåŸŸä¸ºç»“æœåŒºåŸŸ
    # ä¸ä½¿ç”¨ NMS ç®—æ³•: ä½¿ç”¨æ‰€æœ‰åŒºåŸŸçš„åˆå¹¶ï¼Œå¹¶ä¸”é€‰å–æ•°é‡æœ€å¤šçš„æ ‡ç­¾ (æŠ•ç¥¨å¼)
    for index in range(len(final_result)):
        exists_results = final_result[index]
        if USE_NMS_ALGORITHM:
            exists_results.sort(key=lambda r: r[2]*r[3])
            final_result[index] = exists_results[-1]
        else:
            cls_groups = defaultdict(lambda: [])
            for r in exists_results:
                cls_groups[r[0]].append(r)
            most_common = sorted(cls_groups.values(), key=len)[-1]
            label = most_common[0][0]
            box_merged = most_common[0][1]
            for _, box, _, _ in most_common[1:]:
                box_merged = merge_box(box_merged, box)
            rpn_score_mean = sum(x for _, _, x, _ in most_common) / len(most_common)
            cls_score_mean = sum(x for _, _, _, x in most_common) / len(most_common)
            final_result[index] = (label, box_merged, rpn_score_mean, cls_score_mean)
    return final_result
```

### åªæ ¹æ®æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åˆ¤æ–­æ˜¯å¦åœæ­¢è®­ç»ƒ

æœ€åæˆ‘ä»¬ä¿®æ”¹ä»¥ä¸‹åˆ¤æ–­æ˜¯å¦åœæ­¢è®­ç»ƒçš„é€»è¾‘ï¼Œä¹‹å‰çš„åˆ¤æ–­ä¾æ®æ˜¯ `éªŒè¯é›†çš„åŒºåŸŸç”Ÿæˆæ­£ç¡®ç‡æˆ–æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åœ¨ 20 æ¬¡è®­ç»ƒä»¥åæ²¡æœ‰æ›´æ–°` åˆ™åœæ­¢è®­ç»ƒï¼Œä½†è®¡ç®—æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡çš„æ—¶å€™ç”¨çš„æ˜¯ `é¢„æµ‹ç»“æœä¸­åŒºåŸŸèŒƒå›´ä¸å®é™…èŒƒå›´é‡å ç‡è¶…è¿‡é˜ˆå€¼å¹¶ä¸”åˆ†ç±»ä¸€è‡´çš„ç»“æœæ•°é‡ / å®é™…èŒƒå›´çš„æ€»æ•°é‡`ï¼Œä¹Ÿå°±æ˜¯æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡ä»£è¡¨äº†æ¨¡å‹å¯ä»¥æ‰¾å‡ºç™¾åˆ†ä¹‹å¤šå°‘çš„åŒºåŸŸå¹¶ä¸”æ­£ç¡®åˆ¤æ–­å®ƒä»¬çš„åˆ†ç±»ï¼Œå› ä¸ºæ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡ä¼šåŸºäºåŒºåŸŸç”Ÿæˆæ­£ç¡®ç‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åªä½¿ç”¨æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åˆ¤æ–­æ˜¯å¦åœæ­¢è®­ç»ƒã€‚ä¿®æ”¹ä»¥åçš„åˆ¤æ–­ä¾æ®ä¸º `éªŒè¯é›†çš„æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åœ¨ 20 æ¬¡è®­ç»ƒä»¥åæ²¡æœ‰æ›´æ–°` åˆ™åœæ­¢è®­ç»ƒã€‚

``` python
# è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
# åªä¾æ®æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åˆ¤æ–­ï¼Œå› ä¸ºæ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åŒæ—¶åŸºäº RPN æ­£ç¡®ç‡
if validating_cls_accuracy > validating_cls_accuracy_highest:
    validating_rpn_accuracy_highest = validating_rpn_accuracy
    validating_rpn_accuracy_highest_epoch = epoch
    validating_cls_accuracy_highest = validating_cls_accuracy
    validating_cls_accuracy_highest_epoch = epoch
    save_tensor(model.state_dict(), "model.pt")
    print("highest cls validating accuracy updated")
elif (epoch - validating_rpn_accuracy_highest_epoch > 20 and
    epoch - validating_cls_accuracy_highest_epoch > 20):
    # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
    print("stop training because highest validating accuracy not updated in 20 epoches")
    break
```

éœ€è¦æ³¨æ„çš„æ˜¯æˆ‘ç»™å‡ºçš„è®¡ç®—æ­£ç¡®ç‡çš„æ–¹æ³•æ˜¯æ¯”è¾ƒç®€å•çš„ï¼Œæ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯è®¡ç®— mAP (mean Average Precision)ï¼Œå…·ä½“å¯ä»¥å‚è€ƒ[è¿™ç¯‡æ–‡ç« ](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)ï¼Œæˆ‘ç»™å‡ºçš„æ–¹æ³•å®é™…åªç›¸å½“äºæ–‡ç« ä¸­çš„ `Recall`ã€‚

### æ”¯æŒè§†é¢‘è¯†åˆ«

ä¸Šä¸€ç¯‡æ–‡ç« ç»™å‡ºçš„ä»£ç åªèƒ½è¯†åˆ«å•å¼ å›¾ç‰‡ï¼Œè€Œå¯¹è±¡è¯†åˆ«çš„åº”ç”¨åœºæ™¯é€šå¸¸è¦æ±‚è¯†åˆ«è§†é¢‘ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘å†ç»™å‡ºæ”¯æŒè§†é¢‘è¯†åˆ«çš„ä»£ç ã€‚è¯»å–è§†é¢‘æ–‡ä»¶ (æˆ–è€…æ‘„åƒå¤´) ä½¿ç”¨çš„ç±»åº“æ˜¯ opencvï¼Œé’ˆå¯¹ä¸Šä¸€ç¯‡æ–‡ç« çš„è¯†åˆ«ä»£ç å¦‚ä¸‹ (è¿™ä¸€ç¯‡æ–‡ç« çš„è¯†åˆ«ä»£ç è¯·å‚è€ƒåé¢ç»™å‡ºçš„å®Œæ•´ä»£ç )ï¼š

``` python
def eval_video():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«è§†é¢‘"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è§†é¢‘è·¯å¾„ï¼Œç»™å¯èƒ½æ˜¯äººè„¸çš„åŒºåŸŸæ·»åŠ æ ‡è®°å¹¶ä¿å­˜æ–°è§†é¢‘
    import cv2
    from PIL import ImageFont
    font = ImageFont.truetype("FreeMonoBold.ttf", 20)
    while True:
        try:
            video_path = input("Video path: ")
            if not video_path:
                continue
            # è¯»å–è¾“å…¥è§†é¢‘
            video = cv2.VideoCapture(video_path)
            # è·å–æ¯ç§’çš„å¸§æ•°
            fps = int(video.get(cv2.CAP_PROP_FPS))
            # è·å–è§†é¢‘é•¿å®½
            size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))
            # åˆ›å»ºè¾“å‡ºè§†é¢‘
            video_output_path = os.path.join(
                os.path.dirname(video_path),
                os.path.splitext(os.path.basename(video_path))[0] + ".output.avi")
            result = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*"XVID"), fps, size)
            # é€å¸§å¤„ç†
            count = 0
            while(True):
                ret, frame = video.read()
                if not ret:
                    break
                # opencv ä½¿ç”¨çš„æ˜¯ BGR, Pillow ä½¿ç”¨çš„æ˜¯ RGB, éœ€è¦è½¬æ¢é€šé“é¡ºåº
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # æ„å»ºè¾“å…¥
                img_original = Image.fromarray(frame_rgb) # åŠ è½½åŸå§‹å›¾ç‰‡
                sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
                img_output = img_original.copy() # å¤åˆ¶å›¾ç‰‡ï¼Œç”¨äºåé¢æ·»åŠ æ ‡è®°
                tensor_in = image_to_tensor(img)
                # é¢„æµ‹è¾“å‡º
                cls_result = model(tensor_in.unsqueeze(0).to(device))[-1][0] or []
                # åˆå¹¶é‡å çš„ç»“æœåŒºåŸŸ, ç»“æœæ˜¯ [ [æ ‡ç­¾åˆ—è¡¨, åˆå¹¶åçš„åŒºåŸŸ], ... ]
                final_result = []
                for label, box in cls_result:
                    for index in range(len(final_result)):
                        exists_labels, exists_box = final_result[index]
                        if calc_iou(box, exists_box) > IOU_MERGE_THRESHOLD:
                            exists_labels.append(label)
                            final_result[index] = (exists_labels, merge_box(box, exists_box))
                            break
                    else:
                        final_result.append(([label], box))
                # åˆå¹¶æ ‡ç­¾ (é‡å åŒºåŸŸçš„æ ‡ç­¾ä¸­æ•°é‡æœ€å¤šçš„åˆ†ç±»ä¸ºæœ€ç»ˆåˆ†ç±»)
                for index in range(len(final_result)):
                    labels, box = final_result[index]
                    final_label = Counter(labels).most_common(1)[0][0]
                    final_result[index] = (final_label, box)
                # æ ‡è®°åœ¨å›¾ç‰‡ä¸Š
                draw = ImageDraw.Draw(img_output)
                for label, box  in final_result:
                    x, y, w, h = map_box_to_original_image(box, sw, sh)
                    color = "#00FF00" if CLASSES[label] == "with_mask" else "#FF0000"
                    draw.rectangle((x, y, x+w, y+h), outline=color, width=3)
                    draw.text((x, y-20), CLASSES[label], fill=color, font=font)
                # å†™å…¥å¸§åˆ°è¾“å‡ºè§†é¢‘
                frame_rgb_annotated = numpy.asarray(img_output)
                frame_bgr_annotated = cv2.cvtColor(frame_rgb_annotated, cv2.COLOR_RGB2BGR)
                result.write(frame_bgr_annotated)
                count += 1
                if count % fps == 0:
                    print(f"handled {count//fps}s")
            video.release()
            result.release()
            cv2.destroyAllWindows()
            print(f"saved to {video_output_path}")
            print()
        except Exception as e:
            raise
            print("error:", e)
```

æœ‰å‡ ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼š

- è¿™ä¸ªä¾‹å­æ˜¯è¯»å–ç°æœ‰çš„è§†é¢‘æ–‡ä»¶ï¼Œå¦‚æœä½ æƒ³ä»æ‘„åƒå¤´è¯»å–å¯ä»¥æŠŠ `video = cv2.VideoCapture(video_path)` æ”¹ä¸º ` video = cv2.VideoCapture(0)`ï¼Œ0 ä»£è¡¨ç¬¬ä¸€ä¸ªæ‘„åƒå¤´ï¼Œ1 ä»£è¡¨ç¬¬äºŒä¸ªæ‘„åƒå¤´ï¼Œä»¥æ­¤ç±»æ¨
- opencv è¯»å–å‡ºæ¥çš„é€šé“é¡ºåºæ˜¯ BGR (Blue, Green, Red)ï¼Œè€Œ Pillow ä½¿ç”¨çš„é€šé“é¡ºåºæ˜¯ RGB (Red, Blue, Green)ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨ `cv2.cvtColor` è¿›è¡Œè½¬æ¢
- è¾“å…¥è§†é¢‘ä¼šå®šä¹‰æ¯ç§’çš„å¸§æ•° (FPS)ï¼Œåˆ›å»ºè¾“å‡ºè§†é¢‘çš„æ—¶å€™éœ€è¦ä¿è¯ FPS ä¸€è‡´ï¼Œå¦åˆ™ä¼šå‡ºç°æ’­æ”¾é€Ÿåº¦ä¸ä¸€æ ·çš„é—®é¢˜
- è¿™é‡Œä¸ºäº†æ–¹ä¾¿çœ‹ï¼Œæˆ´å£ç½©çš„åŒºåŸŸä¼šä½¿ç”¨ç»¿è‰²æ ‡è®°ï¼Œè€Œä¸å¸¦å£ç½©çš„åŒºåŸŸä¼šä½¿ç”¨çº¢è‰²æ ‡è®°
- Pillow é»˜è®¤æ ‡è®°æ–‡æœ¬ä½¿ç”¨çš„å­—ä½“æ˜¯å›ºå®šå¤§å°çš„ï¼Œä¸æ”¯æŒç¼©æ”¾ï¼Œè¿™é‡Œæˆ‘ä½¿ç”¨äº† `FreeMonoBold.ttf` å­—ä½“å¹¶æŒ‡å®šå­—ä½“å¤§å°ä¸º 20ï¼Œå¦‚æœä½ çš„ç¯å¢ƒæ²¡æœ‰è¿™ä¸ªå­—ä½“åº”è¯¥æ¢ä¸€ä¸ªåç§° (Windows çš„è¯å¯ä»¥ç”¨ `arial.ttf`)

### å‡å°‘è§†é¢‘è¯†åˆ«ä¸­çš„è¯¯åˆ¤

è§†é¢‘è¯†åˆ«æœ‰ä¸€ä¸ªç‰¹æ€§æ˜¯å†…å®¹é€šå¸¸æ˜¯æœ‰è¿ç»­æ€§çš„ï¼Œè§†é¢‘ä¸­çš„ç‰©ä½“é€šå¸¸ä¼šå‡ºç°åœ¨è¿ç»­çš„å‡ å¸§é‡Œé¢ï¼Œåˆ©ç”¨è¿™ä¸ªç‰¹æ€§æˆ‘ä»¬å¯ä»¥å‡å°‘è§†é¢‘è¯†åˆ«ä¸­çš„è¯¯åˆ¤ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå¸§æ•°ï¼Œä¾‹å¦‚ 10 å¸§ï¼Œå¦‚æœç‰©ä½“å‡ºç°åœ¨è¿‡å» 10 å¸§çš„ 5 å¸§ä»¥ä¸Šé‚£ä¹ˆå°±åˆ¤æ–­ç‰©ä½“å­˜åœ¨ï¼Œè¿™æ ·åšå¯ä»¥æ’é™¤æ¨¡å‹é’ˆå¯¹æŸä¸€å¸§å¿½ç„¶å‡ºç°çš„è¯¯åˆ¤ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ç»Ÿè®¡è¿‡å» 10 å¸§é‡Œé¢è¯†åˆ«å‡ºæ¥çš„åˆ†ç±»ï¼Œç„¶åé€‰æ‹©å‡ºç°æ•°é‡æœ€å¤šçš„åˆ†ç±»ï¼ŒæŠ•ç¥¨å†³å®šç»“æœã€‚

å…·ä½“å®ç°ä»£ç å¦‚ä¸‹ï¼š

``` python
@staticmethod
def fix_predicted_result_from_history(cls_result, history_results):
    """æ ¹æ®å†å²ç»“æœå‡å°‘é¢„æµ‹ç»“æœä¸­çš„è¯¯åˆ¤ï¼Œé€‚ç”¨äºè§†é¢‘è¯†åˆ«ï¼Œhistory_results åº”ä¸ºæŒ‡å®šäº† maxlen çš„ deque"""
    # è¦æ±‚å†å²ç»“æœä¸­ 50% ä»¥ä¸Šå­˜åœ¨ç±»ä¼¼åŒºåŸŸï¼Œå¹¶ä¸”é€‰å–å†å²ç»“æœä¸­æœ€å¤šçš„åˆ†ç±»
    history_results.append(cls_result)
    final_result = []
    if len(history_results) < history_results.maxlen:
        # å†å²ç»“æœä¸è¶³ï¼Œä¸è¿”å›ä»»ä½•è¯†åˆ«ç»“æœ
        return final_result
    for label, box, rpn_score, cls_score in cls_result:
        # æŸ¥æ‰¾å†å²ä¸­çš„è¿‘ä¼¼åŒºåŸŸ
        similar_results = []
        for history_result in history_results:
            history_result = [(calc_iou(r[1], box), r) for r in history_result]
            history_result.sort(key = lambda r: r[0])
            if history_result and history_result[-1][0] > IOU_MERGE_THRESHOLD:
                similar_results.append(history_result[-1][1])
        # åˆ¤æ–­è¿‘ä¼¼åŒºåŸŸæ•°é‡æ˜¯å¦è¿‡åŠ
        if len(similar_results) < history_results.maxlen // 2:
            continue
        # é€‰å–å†å²ç»“æœä¸­æœ€å¤šçš„åˆ†ç±»
        cls_groups = defaultdict(lambda: [])
        for r in similar_results:
            cls_groups[r[0]].append(r)
        most_common = sorted(cls_groups.values(), key=len)[-1]
        # æ·»åŠ æœ€å¤šçš„åˆ†ç±»ä¸­çš„æœ€æ–°çš„ç»“æœ
        final_result.append(most_common[-1])
    return final_result
```

`history_results` æ˜¯ä¸€ä¸ªæŒ‡å®šäº†æœ€å¤§æ•°é‡çš„é˜Ÿåˆ—ç±»å‹ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹ä»£ç ç”Ÿæˆï¼š

``` python
from collections import deque

history_results = deque(maxlen = 10)
```

æ¯æ¬¡æ·»åŠ å…ƒç´ åˆ° `history_results` ä»¥åå¦‚æœæ•°é‡è¶…å‡ºæŒ‡å®šçš„æœ€å¤§æ•°é‡åˆ™å®ƒä¼šè‡ªåŠ¨å¼¹å‡ºæœ€æ—©æ·»åŠ çš„å…ƒç´ ã€‚

è¿™ä¸ªåšæ³•æé«˜äº†è§†é¢‘è¯†åˆ«çš„ç¨³å®šæ€§ï¼Œä½†åŒæ—¶ä¼šæŸå¤±ä¸€å®šçš„å®æ—¶æ€§å¹¶ä¸”å¸¦æ¥ä¸€äº›å‰¯ä½œç”¨ã€‚ä¾‹å¦‚ FPS ä¸º 30 çš„æ—¶å€™ï¼Œäººéœ€è¦åœ¨åŒä¸€ä¸ªä½ç½®åœç•™ 1/3 ç§’ä»¥åæ‰ä¼šè¢«è¯†åˆ«å‡ºæ¥ï¼Œå¦‚æœäººä¸€ç›´å¿«é€Ÿèµ°åŠ¨é‚£ä¹ˆå°±ä¸ä¼šè¢«è¯†åˆ«å‡ºæ¥ã€‚æ­¤å¤–å¦‚æœæˆ´å£ç½©çš„äººæŠŠå£ç½©è„±æ‰ï¼Œé‚£ä¹ˆè„±æ‰ä»¥åçš„ 1/6 ç§’æ¨¡å‹ä»ç„¶ä¼šè¯†åˆ«è¿™ä¸ªäººæˆ´ç€å£ç½©ã€‚æ˜¯ä½¿ç”¨è¿™ä¸ªåšæ³•éœ€è¦æ ¹æ®ä½¿ç”¨åœºæ™¯å†³å®šã€‚

## å®Œæ•´ä»£ç 

å¥½äº†ï¼Œæ”¹è¿›ä»¥åçš„å®Œæ•´ä»£ç å¦‚ä¸‹ğŸ˜¤ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import random
import numpy
import math
import pandas
import json
from PIL import Image
from PIL import ImageDraw
from PIL import ImageFont
from torch import nn
from matplotlib import pyplot
from collections import defaultdict
from collections import deque
import xml.etree.cElementTree as ET

# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (256, 192)
# è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†è·¯å¾„
DATASET_1_IMAGE_DIR = "./archive/images"
DATASET_1_ANNOTATION_DIR = "./archive/annotations"
DATASET_2_IMAGE_DIR = "./784145_1347673_bundle_archive/train/image_data"
DATASET_2_BOX_CSV_PATH = "./784145_1347673_bundle_archive/train/bbox_train.csv"
# åˆ†ç±»åˆ—è¡¨
CLASSES = [ "other", "with_mask", "without_mask" ]
CLASSES_MAPPING = { c: index for index, c in enumerate(CLASSES) }
# åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¯¹è±¡ä½¿ç”¨çš„åŒºåŸŸé‡å ç‡çš„é˜ˆå€¼
IOU_POSITIVE_THRESHOLD = 0.30
IOU_NEGATIVE_THRESHOLD = 0.10
# åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶é‡å åŒºåŸŸçš„é‡å ç‡é˜ˆå€¼
IOU_MERGE_THRESHOLD = 0.30
# æ˜¯å¦ä½¿ç”¨ NMS ç®—æ³•åˆå¹¶åŒºåŸŸ
USE_NMS_ALGORITHM = True

# ç”¨äºå¯ç”¨ GPU æ”¯æŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BasicBlock(nn.Module):
    """ResNet ä½¿ç”¨çš„åŸºç¡€å—"""
    expansion = 1 # å®šä¹‰è¿™ä¸ªå—çš„å®é™…å‡ºé€šé“æ˜¯ channels_out çš„å‡ å€ï¼Œè¿™é‡Œçš„å®ç°å›ºå®šæ˜¯ä¸€å€
    def __init__(self, channels_in, channels_out, stride):
        super().__init__()
        # ç”Ÿæˆ 3x3 çš„å·ç§¯å±‚
        # å¤„ç†é—´éš” stride = 1 æ—¶ï¼Œè¾“å‡ºçš„é•¿å®½ä¼šç­‰äºè¾“å…¥çš„é•¿å®½ï¼Œä¾‹å¦‚ (32-3+2)//1+1 == 32
        # å¤„ç†é—´éš” stride = 2 æ—¶ï¼Œè¾“å‡ºçš„é•¿å®½ä¼šç­‰äºè¾“å…¥çš„é•¿å®½çš„ä¸€åŠï¼Œä¾‹å¦‚ (32-3+2)//2+1 == 16
        # æ­¤å¤– resnet çš„ 3x3 å·ç§¯å±‚ä¸ä½¿ç”¨åç§»å€¼ bias
        self.conv1 = nn.Sequential(
            nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(channels_out))
        # å†å®šä¹‰ä¸€ä¸ªè®©è¾“å‡ºå’Œè¾“å…¥ç»´åº¦ç›¸åŒçš„ 3x3 å·ç§¯å±‚
        self.conv2 = nn.Sequential(
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(channels_out))
        # è®©åŸå§‹è¾“å…¥å’Œè¾“å‡ºç›¸åŠ çš„æ—¶å€™ï¼Œéœ€è¦ç»´åº¦ä¸€è‡´ï¼Œå¦‚æœç»´åº¦ä¸ä¸€è‡´åˆ™éœ€è¦æ•´åˆ
        self.identity = nn.Sequential()
        if stride != 1 or channels_in != channels_out * self.expansion:
            self.identity = nn.Sequential(
                nn.Conv2d(channels_in, channels_out * self.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(channels_out * self.expansion))

    def forward(self, x):
        # x => conv1 => relu => conv2 => + => relu
        # |                              ^
        # |==============================|
        tmp = self.conv1(x)
        tmp = nn.functional.relu(tmp, inplace=True)
        tmp = self.conv2(tmp)
        tmp += self.identity(x)
        y = nn.functional.relu(tmp, inplace=True)
        return y

class MyModel(nn.Module):
    """Faster-RCNN (åŸºäº ResNet çš„å˜ç§)"""
    Anchors = None # é”šç‚¹åˆ—è¡¨ï¼ŒåŒ…å« é”šç‚¹æ•°é‡ * å½¢çŠ¶æ•°é‡ çš„èŒƒå›´
    AnchorSpan = 16 # é”šç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œåº”è¯¥ç­‰äºåŸæœ‰é•¿å®½ / resnet è¾“å‡ºé•¿å®½
    AnchorScales = (1, 2, 4, 6, 8) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨
    AnchorAspects = ((1, 1),) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
    AnchorBoxes = len(AnchorScales) * len(AnchorAspects) # æ¯ä¸ªé”šç‚¹å¯¹åº”çš„å½¢çŠ¶æ•°é‡

    def __init__(self):
        super().__init__()
        # æŠ½å–å›¾ç‰‡å„ä¸ªåŒºåŸŸç‰¹å¾çš„ ResNet (é™¤å» AvgPool å’Œå…¨è¿æ¥å±‚)
        # å’Œ Fast-RCNN ä¾‹å­ä¸åŒçš„æ˜¯è¾“å‡ºçš„é•¿å®½ä¼šæ˜¯åŸæœ‰çš„ 1/16ï¼Œåé¢ä¼šæ ¹æ®é”šç‚¹ä¸ affine_grid æˆªå–åŒºåŸŸ
        # æ­¤å¤–ï¼Œä¸ºäº†å¯ä»¥è®©æ¨¡å‹è·‘åœ¨ 4GB æ˜¾å­˜ä¸Šï¼Œè¿™é‡Œå‡å°‘äº†æ¨¡å‹çš„é€šé“æ•°é‡
        # æ³¨æ„:
        # RPN ä½¿ç”¨çš„æ¨¡å‹å’Œæ ‡ç­¾åˆ†ç±»ä½¿ç”¨çš„æ¨¡å‹éœ€è¦åˆ†å¼€ï¼Œå¦åˆ™ä¼šå‡ºç°æ— æ³•å­¦ä¹  (RPN æ€»æ˜¯è¾“å‡ºè´Ÿ) çš„é—®é¢˜
        self.previous_channels_out = 4
        self.rpn_resnet = nn.Sequential(
            nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.previous_channels_out),
            nn.ReLU(inplace=True),
            self._make_layer(BasicBlock, channels_out=8, num_blocks=2, stride=1),
            self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2))
        self.previous_channels_out = 4
        self.cls_resnet = nn.Sequential(
            nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.previous_channels_out),
            nn.ReLU(inplace=True),
            self._make_layer(BasicBlock, channels_out=8, num_blocks=2, stride=1),
            self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2))
        self.features_channels = 128
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„å¯¹è±¡å¯èƒ½æ€§çš„æ¨¡å‹
        self.rpn_labels_model = nn.Sequential(
            nn.Linear(self.features_channels, self.features_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.features_channels, MyModel.AnchorBoxes*2))
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„åŒºåŸŸåç§»çš„æ¨¡å‹
        self.rpn_offsets_model = nn.Sequential(
            nn.Linear(self.features_channels, self.features_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.features_channels, MyModel.AnchorBoxes*4))
        # é€‰å–å¯èƒ½å‡ºç°å¯¹è±¡çš„åŒºåŸŸéœ€è¦çš„æœ€å°å¯èƒ½æ€§
        self.rpn_score_threshold = 0.9
        # æ¯å¼ å›¾ç‰‡æœ€å¤šé€‰å–çš„åŒºåŸŸåˆ—è¡¨
        self.rpn_max_candidates = 32
        # æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾åç¼©æ”¾åˆ°çš„å¤§å°
        self.pooling_size = 16
        # æ ¹æ®åŒºåŸŸç‰¹å¾åˆ¤æ–­åˆ†ç±»çš„æ¨¡å‹
        self.cls_labels_model = nn.Sequential(
            nn.Linear(self.features_channels * (self.pooling_size ** 2), self.features_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.features_channels, len(CLASSES)))
        # æ ¹æ®åŒºåŸŸç‰¹å¾å†æ¬¡ç”ŸæˆåŒºåŸŸåç§»çš„æ¨¡å‹ï¼Œæ³¨æ„åŒºåŸŸåç§»ä¼šé’ˆå¯¹å„ä¸ªåˆ†ç±»åˆ†åˆ«ç”Ÿæˆ
        self.cls_offsets_model = nn.Sequential(
            nn.Linear(self.features_channels * (self.pooling_size ** 2), self.features_channels*4),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.features_channels*4, len(CLASSES)*4))

    def _make_layer(self, block_type, channels_out, num_blocks, stride):
        """åˆ›å»º resnet ä½¿ç”¨çš„å±‚"""
        blocks = []
        # æ·»åŠ ç¬¬ä¸€ä¸ªå—
        blocks.append(block_type(self.previous_channels_out, channels_out, stride))
        self.previous_channels_out = channels_out * block_type.expansion
        # æ·»åŠ å‰©ä½™çš„å—ï¼Œå‰©ä½™çš„å—å›ºå®šå¤„ç†é—´éš”ä¸º 1ï¼Œä¸ä¼šæ”¹å˜é•¿å®½
        for _ in range(num_blocks-1):
            blocks.append(block_type(self.previous_channels_out, self.previous_channels_out, 1))
            self.previous_channels_out *= block_type.expansion
        return nn.Sequential(*blocks)

    @staticmethod
    def _generate_anchors():
        """æ ¹æ®é”šç‚¹å’Œå½¢çŠ¶ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨"""
        w, h = IMAGE_SIZE
        span = MyModel.AnchorSpan
        anchors = []
        for x in range(0, w, span):
            for y in range(0, h, span):
                xcenter, ycenter = x + span / 2, y + span / 2
                for scale in MyModel.AnchorScales:
                    for ratio in MyModel.AnchorAspects:
                        ww = span * scale * ratio[0]
                        hh = span * scale * ratio[1]
                        xx = xcenter - ww / 2
                        yy = ycenter - hh / 2
                        xx = max(int(xx), 0)
                        yy = max(int(yy), 0)
                        ww = min(int(ww), w - xx)
                        hh = min(int(hh), h - yy)
                        anchors.append((xx, yy, ww, hh))
        return anchors

    @staticmethod
    def _roi_crop(features, rois, pooling_size):
        """æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾ï¼Œæ¯æ¬¡åªèƒ½å¤„ç†å•å¼ å›¾ç‰‡"""
        width, height = IMAGE_SIZE
        theta = []
        results = []
        for roi in rois:
            x1, y1, w, h = roi
            x2, y2 = x1 + w, y1 + h
            theta = [[
                [
                    (y2 - y1) / height,
                    0,
                    (y2 + y1) / height - 1
                ],
                [
                    0,
                    (x2 - x1) / width,
                    (x2 + x1) / width - 1
                ]
            ]]
            theta_tensor = torch.tensor(theta)
            grid = nn.functional.affine_grid(
                theta_tensor,
                torch.Size((1, 1, pooling_size, pooling_size)),
                align_corners=False).to(device)
            result = nn.functional.grid_sample(
                features.unsqueeze(0), grid, align_corners=False)
            results.append(result)
        if not results:
            return None
        results = torch.cat(results, dim=0)
        return results

    def forward(self, x):
        # ***** æŠ½å–ç‰¹å¾éƒ¨åˆ† *****
        # åˆ†åˆ«æŠ½å– RPN å’Œæ ‡ç­¾åˆ†ç±»ä½¿ç”¨çš„ç‰¹å¾
        # ç»´åº¦æ˜¯ B,128,W/16,H/16
        rpn_features_original = self.rpn_resnet(x)
        # ç»´åº¦æ˜¯ B*W/16*H/16,128 (æŠŠé€šé“æ”¾åœ¨æœ€åï¼Œç”¨äºä¼ ç»™çº¿æ€§æ¨¡å‹)
        rpn_features = rpn_features_original.permute(0, 2, 3, 1).reshape(-1, self.features_channels)
        # ç»´åº¦æ˜¯ B,128,W/16,H/16
        cls_features = self.cls_resnet(x)

        # ***** é€‰å–åŒºåŸŸéƒ¨åˆ† *****
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„å¯¹è±¡å¯èƒ½æ€§
        # ç»´åº¦æ˜¯ B,W/16*H/16*AnchorBoxes,2
        rpn_labels = self.rpn_labels_model(rpn_features)
        rpn_labels = rpn_labels.reshape(
            rpn_features_original.shape[0],
            rpn_features_original.shape[2] * rpn_features_original.shape[3] * MyModel.AnchorBoxes,
            2)
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„åŒºåŸŸåç§»
        # ç»´åº¦æ˜¯ B,W/16*H/16*AnchorBoxes,4
        rpn_offsets = self.rpn_offsets_model(rpn_features)
        rpn_offsets = rpn_offsets.reshape(
            rpn_features_original.shape[0],
            rpn_features_original.shape[2] * rpn_features_original.shape[3] * MyModel.AnchorBoxes,
            4)
        # é€‰å–å¯èƒ½å‡ºç°å¯¹è±¡çš„åŒºåŸŸï¼Œå¹¶è°ƒæ•´åŒºåŸŸèŒƒå›´
        with torch.no_grad():
            rpn_scores = nn.functional.softmax(rpn_labels, dim=2)[:,:,1]
            # é€‰å–å¯èƒ½æ€§æœ€é«˜çš„éƒ¨åˆ†åŒºåŸŸ
            rpn_top_scores = torch.topk(rpn_scores, k=self.rpn_max_candidates, dim=1)
            rpn_candidates_batch = []
            for x in range(0, rpn_scores.shape[0]):
                rpn_candidates = []
                for score, index in zip(rpn_top_scores.values[x], rpn_top_scores.indices[x]):
                    # è¿‡æ»¤å¯èƒ½æ€§ä½äºæŒ‡å®šé˜ˆå€¼çš„åŒºåŸŸ
                    if score.item() < self.rpn_score_threshold:
                        continue
                    anchor_box = MyModel.Anchors[index.item()]
                    offset = rpn_offsets[x,index.item()].tolist()
                    # è°ƒæ•´åŒºåŸŸèŒƒå›´
                    candidate_box = adjust_box_by_offset(anchor_box, offset)
                    rpn_candidates.append((candidate_box, score.item()))
                rpn_candidates_batch.append(rpn_candidates)

        # ***** åˆ¤æ–­åˆ†ç±»éƒ¨åˆ† *****
        cls_output = []
        cls_result = []
        for index in range(0, cls_features.shape[0]):
            rois = [c[0] for c in rpn_candidates_batch[index]]
            pooled = MyModel._roi_crop(cls_features[index], rois, self.pooling_size)
            if pooled is None:
                # æ²¡æœ‰æ‰¾åˆ°å¯èƒ½åŒ…å«å¯¹è±¡çš„åŒºåŸŸ
                cls_output.append(None)
                cls_result.append(None)
                continue
            pooled = pooled.reshape(pooled.shape[0], -1)
            labels = self.cls_labels_model(pooled)
            offsets = self.cls_offsets_model(pooled)
            cls_output.append((labels, offsets))
            # ä½¿ç”¨ softmax åˆ¤æ–­å¯èƒ½æ€§æœ€å¤§çš„åˆ†ç±»
            labels_max = nn.functional.softmax(labels, dim=1).max(dim=1)
            classes = labels_max.indices
            classes_scores = labels_max.values
            # æ ¹æ®åˆ†ç±»å¯¹åº”çš„åç§»å†æ¬¡è°ƒæ•´åŒºåŸŸèŒƒå›´
            offsets_map = offsets.reshape(offsets.shape[0] * len(CLASSES), 4)
            result = []
            for box_index in range(0, classes.shape[0]):
                predicted_label = classes[box_index].item()
                if predicted_label == 0:
                    continue # 0 ä»£è¡¨ other, è¡¨ç¤ºéå¯¹è±¡
                candidate_box = rpn_candidates_batch[index][box_index][0]
                offset = offsets_map[box_index * len(CLASSES) + predicted_label].tolist()
                predicted_box = adjust_box_by_offset(candidate_box, offset)
                # æ·»åŠ åˆ†ç±»ä¸æœ€ç»ˆé¢„æµ‹åŒºåŸŸ
                rpn_score = rpn_candidates_batch[index][box_index][1]
                cls_score = classes_scores[box_index].item()
                result.append((predicted_label, predicted_box, rpn_score, cls_score))
            cls_result.append(result)

        # å‰é¢çš„é¡¹ç›®ç”¨äºå­¦ä¹ ï¼Œæœ€åä¸€é¡¹æ˜¯æœ€ç»ˆè¾“å‡ºç»“æœ
        return rpn_labels, rpn_offsets, rpn_candidates_batch, cls_output, cls_result

    @staticmethod
    def loss_function(predicted, actual):
        """Faster-RCNN ä½¿ç”¨çš„å¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨"""
        rpn_labels, rpn_offsets, rpn_candidates_batch, cls_output, _ = predicted
        rpn_labels_losses = []
        rpn_offsets_losses = []
        cls_labels_losses = []
        cls_offsets_losses = []
        for batch_index in range(len(actual)):
            # è®¡ç®— RPN çš„æŸå¤±
            (true_boxes_labels,
                actual_rpn_labels, actual_rpn_labels_mask,
                actual_rpn_offsets, actual_rpn_offsets_mask) = actual[batch_index]
            if actual_rpn_labels_mask.shape[0] > 0:
                rpn_labels_losses.append(nn.functional.cross_entropy(
                    rpn_labels[batch_index][actual_rpn_labels_mask],
                    actual_rpn_labels.to(device)))
            if actual_rpn_offsets_mask.shape[0] > 0:
                rpn_offsets_losses.append(nn.functional.smooth_l1_loss(
                    rpn_offsets[batch_index][actual_rpn_offsets_mask],
                    actual_rpn_offsets.to(device)))
            # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æŸå¤±
            if cls_output[batch_index] is None:
                continue
            cls_labels_mask = []
            cls_offsets_mask = []
            cls_actual_labels = []
            cls_actual_offsets = []
            cls_predicted_labels, cls_predicted_offsets = cls_output[batch_index]
            cls_predicted_offsets_map = cls_predicted_offsets.reshape(-1, 4)
            rpn_candidates = rpn_candidates_batch[batch_index]
            for box_index, (candidate_box, _) in enumerate(rpn_candidates):
                iou_list = [ calc_iou(candidate_box, true_box) for (_, true_box) in true_boxes_labels ]
                positive_index = next((index for index, iou in enumerate(iou_list) if iou > IOU_POSITIVE_THRESHOLD), None)
                is_negative = all(iou < IOU_NEGATIVE_THRESHOLD for iou in iou_list)
                if positive_index is not None:
                    true_label, true_box = true_boxes_labels[positive_index]
                    cls_actual_labels.append(true_label)
                    cls_labels_mask.append(box_index)
                    # å¦‚æœåŒºåŸŸæ­£ç¡®ï¼Œåˆ™å­¦ä¹ çœŸå®åˆ†ç±»å¯¹åº”çš„åŒºåŸŸåç§»
                    cls_actual_offsets.append(calc_box_offset(candidate_box, true_box))
                    cls_offsets_mask.append(box_index * len(CLASSES) + true_label)
                elif is_negative:
                    cls_actual_labels.append(0) # 0 ä»£è¡¨ other, è¡¨ç¤ºéå¯¹è±¡
                    cls_labels_mask.append(box_index)
                # å¦‚æœå€™é€‰åŒºåŸŸä¸çœŸå®åŒºåŸŸçš„é‡å ç‡ä»‹äºä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´ï¼Œåˆ™ä¸å‚ä¸å­¦ä¹ 
            if cls_labels_mask:
                cls_labels_losses.append(nn.functional.cross_entropy(
                    cls_predicted_labels[cls_labels_mask],
                    torch.tensor(cls_actual_labels).to(device)))
            if cls_offsets_mask:
                cls_offsets_losses.append(nn.functional.smooth_l1_loss(
                    cls_predicted_offsets_map[cls_offsets_mask],
                    torch.tensor(cls_actual_offsets).to(device)))
        # åˆå¹¶æŸå¤±å€¼
        # æ³¨æ„ loss ä¸å¯ä»¥ä½¿ç”¨ += åˆå¹¶
        loss = torch.tensor(.0, requires_grad=True)
        loss = loss + torch.mean(torch.stack(rpn_labels_losses))
        loss = loss + torch.mean(torch.stack(rpn_offsets_losses))
        if cls_labels_losses:
            loss = loss + torch.mean(torch.stack(cls_labels_losses))
        if cls_offsets_losses:
            loss = loss + torch.mean(torch.stack(cls_offsets_losses))
        return loss

    @staticmethod
    def calc_accuracy(actual, predicted):
        """Faster-RCNN ä½¿ç”¨çš„æ­£ç¡®ç‡è®¡ç®—å™¨ï¼Œè¿™é‡Œåªè®¡ç®— RPN ä¸æ ‡ç­¾åˆ†ç±»çš„æ­£ç¡®ç‡ï¼ŒåŒºåŸŸåç§»ä¸è®¡ç®—"""
        rpn_labels, rpn_offsets, rpn_candidates_batch, cls_output, cls_result = predicted
        rpn_acc = 0
        cls_acc = 0
        for batch_index in range(len(actual)):
            # è®¡ç®— RPN çš„æ­£ç¡®ç‡ï¼Œæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æ­£ç¡®ç‡åˆ†åˆ«è®¡ç®—å†å¹³å‡
            (true_boxes_labels,
                actual_rpn_labels, actual_rpn_labels_mask,
                actual_rpn_offsets, actual_rpn_offsets_mask) = actual[batch_index]
            a = actual_rpn_labels.to(device)
            p = torch.max(rpn_labels[batch_index][actual_rpn_labels_mask], 1).indices
            rpn_acc_positive = ((a == 0) & (p == 0)).sum().item() / ((a == 0).sum().item() + 0.00001)
            rpn_acc_negative = ((a == 1) & (p == 1)).sum().item() / ((a == 1).sum().item() + 0.00001)
            rpn_acc += (rpn_acc_positive + rpn_acc_negative) / 2
            # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æ­£ç¡®ç‡
            # æ­£ç¡®ç‡ = æœ‰å¯¹åº”é¢„æµ‹åŒºåŸŸå¹¶ä¸”é¢„æµ‹åˆ†ç±»æ­£ç¡®çš„çœŸå®åŒºåŸŸæ•°é‡ / æ€»çœŸå®åŒºåŸŸæ•°é‡
            cls_correct = 0
            for true_label, true_box in true_boxes_labels:
                if cls_result[batch_index] is None:
                    continue
                for predicted_label, predicted_box, _, _ in cls_result[batch_index]:
                    if calc_iou(predicted_box, true_box) > IOU_POSITIVE_THRESHOLD and predicted_label == true_label:
                        cls_correct += 1
                        break
            cls_acc += cls_correct / len(true_boxes_labels)
        rpn_acc /= len(actual)
        cls_acc /= len(actual)
        return rpn_acc, cls_acc

    @staticmethod
    def merge_predicted_result(cls_result):
        """åˆå¹¶é¢„æµ‹ç»“æœåŒºåŸŸ"""
        # è®°å½•é‡å çš„ç»“æœåŒºåŸŸ, ç»“æœæ˜¯ [ [(æ ‡ç­¾, åŒºåŸŸ, RPN åˆ†æ•°, æ ‡ç­¾è¯†åˆ«åˆ†æ•°)], ... ]
        final_result = []
        for label, box, rpn_score, cls_score in cls_result:
            for index in range(len(final_result)):
                exists_results = final_result[index]
                if any(calc_iou(box, r[1]) > IOU_MERGE_THRESHOLD for r in exists_results):
                    exists_results.append((label, box, rpn_score, cls_score))
                    break
            else:
                final_result.append([(label, box, rpn_score, cls_score)])
        # åˆå¹¶é‡å çš„ç»“æœåŒºåŸŸ
        # ä½¿ç”¨ NMS ç®—æ³•: RPN åˆ†æ•° * æ ‡ç­¾è¯†åˆ«åˆ†æ•° æœ€é«˜çš„åŒºåŸŸä¸ºç»“æœåŒºåŸŸ
        # ä¸ä½¿ç”¨ NMS ç®—æ³•: ä½¿ç”¨æ‰€æœ‰åŒºåŸŸçš„åˆå¹¶ï¼Œå¹¶ä¸”é€‰å–æ•°é‡æœ€å¤šçš„æ ‡ç­¾ (æŠ•ç¥¨å¼)
        for index in range(len(final_result)):
            exists_results = final_result[index]
            if USE_NMS_ALGORITHM:
                exists_results.sort(key=lambda r: r[2]*r[3])
                final_result[index] = exists_results[-1]
            else:
                cls_groups = defaultdict(lambda: [])
                for r in exists_results:
                    cls_groups[r[0]].append(r)
                most_common = sorted(cls_groups.values(), key=len)[-1]
                label = most_common[0][0]
                box_merged = most_common[0][1]
                for _, box, _, _ in most_common[1:]:
                    box_merged = merge_box(box_merged, box)
                rpn_score_mean = sum(x for _, _, x, _ in most_common) / len(most_common)
                cls_score_mean = sum(x for _, _, _, x in most_common) / len(most_common)
                final_result[index] = (label, box_merged, rpn_score_mean, cls_score_mean)
        return final_result

    @staticmethod
    def fix_predicted_result_from_history(cls_result, history_results):
        """æ ¹æ®å†å²ç»“æœå‡å°‘é¢„æµ‹ç»“æœä¸­çš„è¯¯åˆ¤ï¼Œé€‚ç”¨äºè§†é¢‘è¯†åˆ«ï¼Œhistory_results åº”ä¸ºæŒ‡å®šäº† maxlen çš„ deque"""
        # è¦æ±‚å†å²ç»“æœä¸­ 50% ä»¥ä¸Šå­˜åœ¨ç±»ä¼¼åŒºåŸŸï¼Œå¹¶ä¸”é€‰å–å†å²ç»“æœä¸­æœ€å¤šçš„åˆ†ç±»
        history_results.append(cls_result)
        final_result = []
        if len(history_results) < history_results.maxlen:
            # å†å²ç»“æœä¸è¶³ï¼Œä¸è¿”å›ä»»ä½•è¯†åˆ«ç»“æœ
            return final_result
        for label, box, rpn_score, cls_score in cls_result:
            # æŸ¥æ‰¾å†å²ä¸­çš„è¿‘ä¼¼åŒºåŸŸ
            similar_results = []
            for history_result in history_results:
                history_result = [(calc_iou(r[1], box), r) for r in history_result]
                history_result.sort(key = lambda r: r[0])
                if history_result and history_result[-1][0] > IOU_MERGE_THRESHOLD:
                    similar_results.append(history_result[-1][1])
            # åˆ¤æ–­è¿‘ä¼¼åŒºåŸŸæ•°é‡æ˜¯å¦è¿‡åŠ
            if len(similar_results) < history_results.maxlen // 2:
                continue
            # é€‰å–å†å²ç»“æœä¸­æœ€å¤šçš„åˆ†ç±»
            cls_groups = defaultdict(lambda: [])
            for r in similar_results:
                cls_groups[r[0]].append(r)
            most_common = sorted(cls_groups.values(), key=len)[-1]
            # æ·»åŠ æœ€å¤šçš„åˆ†ç±»ä¸­çš„æœ€æ–°çš„ç»“æœ
            final_result.append(most_common[-1])
        return final_result

MyModel.Anchors = MyModel._generate_anchors()

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def calc_resize_parameters(sw, sh):
    """è®¡ç®—ç¼©æ”¾å›¾ç‰‡çš„å‚æ•°"""
    sw_new, sh_new = sw, sh
    dw, dh = IMAGE_SIZE
    pad_w, pad_h = 0, 0
    if sw / sh < dw / dh:
        sw_new = int(dw / dh * sh)
        pad_w = (sw_new - sw) // 2 # å¡«å……å·¦å³
    else:
        sh_new = int(dh / dw * sw)
        pad_h = (sh_new - sh) // 2 # å¡«å……ä¸Šä¸‹
    return sw_new, sh_new, pad_w, pad_h

def resize_image(img):
    """ç¼©æ”¾å›¾ç‰‡ï¼Œæ¯”ä¾‹ä¸ä¸€è‡´æ—¶å¡«å……"""
    sw, sh = img.size
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    img_new = Image.new("RGB", (sw_new, sh_new))
    img_new.paste(img, (pad_w, pad_h))
    img_new = img_new.resize(IMAGE_SIZE)
    return img_new

def image_to_tensor(img):
    """è½¬æ¢å›¾ç‰‡å¯¹è±¡åˆ° tensor å¯¹è±¡"""
    arr = numpy.asarray(img)
    t = torch.from_numpy(arr)
    t = t.transpose(0, 2) # è½¬æ¢ç»´åº¦ H,W,C åˆ° C,W,H
    t = t / 255.0 # æ­£è§„åŒ–æ•°å€¼ä½¿å¾—èŒƒå›´åœ¨ 0 ~ 1
    return t

def map_box_to_resized_image(box, sw, sh):
    """æŠŠåŸå§‹åŒºåŸŸè½¬æ¢åˆ°ç¼©æ”¾åçš„å›¾ç‰‡å¯¹åº”çš„åŒºåŸŸ"""
    x, y, w, h = box
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int((x + pad_w) * scale)
    y = int((y + pad_h) * scale)
    w = int(w * scale)
    h = int(h * scale)
    if x + w > IMAGE_SIZE[0] or y + h > IMAGE_SIZE[1] or w == 0 or h == 0:
        return 0, 0, 0, 0
    return x, y, w, h

def map_box_to_original_image(box, sw, sh):
    """æŠŠç¼©æ”¾åå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸè½¬æ¢åˆ°ç¼©æ”¾å‰çš„åŸå§‹åŒºåŸŸ"""
    x, y, w, h = box
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int(x / scale - pad_w)
    y = int(y / scale - pad_h)
    w = int(w / scale)
    h = int(h / scale)
    if x + w > sw or y + h > sh or x < 0 or y < 0 or w == 0 or h == 0:
        return 0, 0, 0, 0
    return x, y, w, h

def calc_iou(rect1, rect2):
    """è®¡ç®—ä¸¤ä¸ªåŒºåŸŸé‡å éƒ¨åˆ† / åˆå¹¶éƒ¨åˆ†çš„æ¯”ç‡ (intersection over union)"""
    x1, y1, w1, h1 = rect1
    x2, y2, w2, h2 = rect2
    xi = max(x1, x2)
    yi = max(y1, y2)
    wi = min(x1+w1, x2+w2) - xi
    hi = min(y1+h1, y2+h2) - yi
    if wi > 0 and hi > 0: # æœ‰é‡å éƒ¨åˆ†
        area_overlap = wi*hi
        area_all = w1*h1 + w2*h2 - area_overlap
        iou = area_overlap / area_all
    else: # æ²¡æœ‰é‡å éƒ¨åˆ†
        iou = 0
    return iou

def calc_box_offset(candidate_box, true_box):
    """è®¡ç®—å€™é€‰åŒºåŸŸä¸å®é™…åŒºåŸŸçš„åç§»å€¼"""
    # è¿™é‡Œè®¡ç®—å‡ºæ¥çš„åç§»å€¼åŸºäºæ¯”ä¾‹ï¼Œè€Œä¸å—å…·ä½“ä½ç½®å’Œå¤§å°å½±å“
    # w h ä½¿ç”¨ log æ˜¯ä¸ºäº†å‡å°‘è¿‡å¤§çš„å€¼çš„å½±å“
    x1, y1, w1, h1 = candidate_box
    x2, y2, w2, h2 = true_box
    x_offset = (x2 - x1) / w1
    y_offset = (y2 - y1) / h1
    w_offset = math.log(w2 / w1)
    h_offset = math.log(h2 / h1)
    return (x_offset, y_offset, w_offset, h_offset)

def adjust_box_by_offset(candidate_box, offset):
    """æ ¹æ®åç§»å€¼è°ƒæ•´å€™é€‰åŒºåŸŸ"""
    # exp éœ€è¦é™åˆ¶å€¼å°äº log(16)ï¼Œå¦‚æœå€¼è¿‡å¤§å¯èƒ½ä¼šå¼•å‘ OverflowError
    x1, y1, w1, h1 = candidate_box
    x_offset, y_offset, w_offset, h_offset = offset
    x2 = min(IMAGE_SIZE[0]-1,  max(0, w1 * x_offset + x1))
    y2 = min(IMAGE_SIZE[1]-1,  max(0, h1 * y_offset + y1))
    w2 = min(IMAGE_SIZE[0]-x2, max(1, math.exp(min(w_offset, 2.78)) * w1))
    h2 = min(IMAGE_SIZE[1]-y2, max(1, math.exp(min(h_offset, 2.78)) * h1))
    return (x2, y2, w2, h2)

def merge_box(box_a, box_b):
    """åˆå¹¶ä¸¤ä¸ªåŒºåŸŸ"""
    x1, y1, w1, h1 = box_a
    x2, y2, w2, h2 = box_b
    x = min(x1, x2)
    y = min(y1, y2)
    w = max(x1 + w1, x2 + w2) - x
    h = max(y1 + h1, y2 + h2) - y
    return (x, y, w, h)

def prepare_save_batch(batch, image_tensors, image_boxes_labels):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æŒ‰ç´¢å¼•å€¼åˆ—è¡¨ç”Ÿæˆè¾“å…¥å’Œè¾“å‡º tensor å¯¹è±¡çš„å‡½æ•°
    def split_dataset(indices):
        image_in = []
        boxes_labels_out = {}
        for new_image_index, original_image_index in enumerate(indices.tolist()):
            image_in.append(image_tensors[original_image_index])
            boxes_labels_out[new_image_index] = image_boxes_labels[original_image_index]
        tensor_image_in = torch.stack(image_in) # ç»´åº¦: B,C,W,H
        return tensor_image_in, boxes_labels_out

    # åˆ‡åˆ†è®­ç»ƒé›† (80%)ï¼ŒéªŒè¯é›† (10%) å’Œæµ‹è¯•é›† (10%)
    random_indices = torch.randperm(len(image_tensors))
    training_indices = random_indices[:int(len(random_indices)*0.8)]
    validating_indices = random_indices[int(len(random_indices)*0.8):int(len(random_indices)*0.9):]
    testing_indices = random_indices[int(len(random_indices)*0.9):]
    training_set = split_dataset(training_indices)
    validating_set = split_dataset(validating_indices)
    testing_set = split_dataset(testing_indices)

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # åŠ è½½å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
    # { (è·¯å¾„, æ˜¯å¦å·¦å³ç¿»è½¬): [ åŒºåŸŸä¸åˆ†ç±», åŒºåŸŸä¸åˆ†ç±», .. ] }
    # åŒä¸€å¼ å›¾ç‰‡å·¦å³ç¿»è½¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ–°çš„æ•°æ®ï¼Œè®©æ•°æ®é‡ç¿»å€
    box_map = defaultdict(lambda: [])
    for filename in os.listdir(DATASET_1_IMAGE_DIR):
        # ä»ç¬¬ä¸€ä¸ªæ•°æ®é›†åŠ è½½
        xml_path = os.path.join(DATASET_1_ANNOTATION_DIR, filename.split(".")[0] + ".xml")
        if not os.path.isfile(xml_path):
            continue
        tree = ET.ElementTree(file=xml_path)
        objects = tree.findall("object")
        path = os.path.join(DATASET_1_IMAGE_DIR, filename)
        for obj in objects:
            class_name = obj.find("name").text
            x1 = int(obj.find("bndbox/xmin").text)
            x2 = int(obj.find("bndbox/xmax").text)
            y1 = int(obj.find("bndbox/ymin").text)
            y2 = int(obj.find("bndbox/ymax").text)
            if class_name == "mask_weared_incorrect":
                # ä½©æˆ´å£ç½©ä¸æ­£ç¡®çš„æ ·æœ¬æ•°é‡å¤ªå°‘ (åªæœ‰ 123)ï¼Œæ¨¡å‹æ— æ³•å­¦ä¹ ï¼Œè¿™é‡Œå…¨åˆå¹¶åˆ°æˆ´å£ç½©çš„æ ·æœ¬
                class_name = "with_mask"
            box_map[(path, False)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))
            box_map[(path, True)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))
    df = pandas.read_csv(DATASET_2_BOX_CSV_PATH)
    for row in df.values:
        # ä»ç¬¬äºŒä¸ªæ•°æ®é›†åŠ è½½ï¼Œè¿™ä¸ªæ•°æ®é›†åªåŒ…å«æ²¡æœ‰å¸¦å£ç½©çš„å›¾ç‰‡
        filename, width, height, x1, y1, x2, y2 = row[:7]
        path = os.path.join(DATASET_2_IMAGE_DIR, filename)
        box_map[(path, False)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING["without_mask"]))
        box_map[(path, True)].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING["without_mask"]))
    # æ‰“ä¹±æ•°æ®é›† (å› ä¸ºç¬¬äºŒä¸ªæ•°æ®é›†åªæœ‰ä¸æˆ´å£ç½©çš„å›¾ç‰‡)
    box_list = list(box_map.items())
    random.shuffle(box_list)
    print(f"found {len(box_list)} images")

    # ä¿å­˜å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åˆ†ç±»ä¸åŒºåŸŸåˆ—è¡¨
    batch_size = 20
    batch = 0
    image_tensors = [] # å›¾ç‰‡åˆ—è¡¨
    image_boxes_labels = {} # å›¾ç‰‡å¯¹åº”çš„çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨ï¼Œå’Œå€™é€‰åŒºåŸŸä¸åŒºåŸŸåç§»
    for (image_path, flip), original_boxes_labels in box_list:
        with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
            sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
            if flip:
                img = resize_image(img_original.transpose(Image.FLIP_LEFT_RIGHT)) # ç¿»è½¬ç„¶åç¼©æ”¾å›¾ç‰‡
            else:
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
            image_index = len(image_tensors) # å›¾ç‰‡åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•å€¼
            image_tensors.append(image_to_tensor(img)) # æ·»åŠ å›¾ç‰‡åˆ°åˆ—è¡¨
            true_boxes_labels = [] # å›¾ç‰‡å¯¹åº”çš„çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
        # æ·»åŠ çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
        for box_label in original_boxes_labels:
            x, y, w, h, label = box_label
            if flip: # ç¿»è½¬åæ ‡
                x = sw - x - w
            x, y, w, h = map_box_to_resized_image((x, y, w, h), sw, sh) # ç¼©æ”¾å®é™…åŒºåŸŸ
            if w < 20 or h < 20:
                continue # ç¼©æ”¾ååŒºåŸŸè¿‡å°
            # æ£€æŸ¥è®¡ç®—æ˜¯å¦æœ‰é—®é¢˜
            # child_img = img.copy().crop((x, y, x+w, y+h))
            # child_img.save(f"{os.path.basename(image_path)}_{x}_{y}_{w}_{h}_{label}.png")
            true_boxes_labels.append((label, (x, y, w, h)))
        # å¦‚æœå›¾ç‰‡ä¸­çš„æ‰€æœ‰åŒºåŸŸéƒ½è¿‡å°åˆ™è·³è¿‡
        if not true_boxes_labels:
            image_tensors.pop()
            image_index = len(image_tensors)
            continue
        # æ ¹æ®é”šç‚¹åˆ—è¡¨å¯»æ‰¾å€™é€‰åŒºåŸŸï¼Œå¹¶è®¡ç®—åŒºåŸŸåç§»
        actual_rpn_labels = []
        actual_rpn_labels_mask = []
        actual_rpn_offsets = []
        actual_rpn_offsets_mask = []
        positive_index_set = set()
        for index, anchor_box in enumerate(MyModel.Anchors):
            # å¦‚æœå€™é€‰åŒºåŸŸå’Œä»»æ„ä¸€ä¸ªå®é™…åŒºåŸŸé‡å ç‡å¤§äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯æ­£æ ·æœ¬
            # å¦‚æœå€™é€‰åŒºåŸŸå’Œæ‰€æœ‰å®é™…åŒºåŸŸé‡å ç‡éƒ½å°äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯è´Ÿæ ·æœ¬
            # é‡å ç‡ä»‹äºä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´çš„åŒºåŸŸä¸å‚ä¸å­¦ä¹ 
            iou_list = [ calc_iou(anchor_box, true_box) for (_, true_box) in true_boxes_labels ]
            positive_index = next((index for index, iou in enumerate(iou_list) if iou > IOU_POSITIVE_THRESHOLD), None)
            is_negative = all(iou < IOU_NEGATIVE_THRESHOLD for iou in iou_list)
            if positive_index is not None:
                positive_index_set.add(positive_index)
                actual_rpn_labels.append(1)
                actual_rpn_labels_mask.append(index)
                # åªæœ‰åŒ…å«å¯¹è±¡çš„åŒºåŸŸå‚éœ€è¦è°ƒæ•´åç§»
                true_box = true_boxes_labels[positive_index][1]
                actual_rpn_offsets.append(calc_box_offset(anchor_box, true_box))
                actual_rpn_offsets_mask.append(index)
            elif is_negative:
                actual_rpn_labels.append(0)
                actual_rpn_labels_mask.append(index)
        # è¾“å‡ºæ‰¾ä¸åˆ°å€™é€‰åŒºåŸŸçš„çœŸå®åŒºåŸŸï¼Œè°ƒæ•´é”šç‚¹ç”Ÿæˆå‚æ•°æ—¶ä½¿ç”¨
        for index in range(len(true_boxes_labels)):
           if index not in positive_index_set:
               print("no candidate box found for:", true_boxes_labels[index][1])
        # å¦‚æœä¸€ä¸ªå€™é€‰åŒºåŸŸéƒ½æ‰¾ä¸åˆ°åˆ™è·³è¿‡
        if not positive_index_set:
            image_tensors.pop()
            image_index = len(image_tensors)
            continue
        image_boxes_labels[image_index] = (
            true_boxes_labels,
            torch.tensor(actual_rpn_labels, dtype=torch.long),
            torch.tensor(actual_rpn_labels_mask, dtype=torch.long),
            torch.tensor(actual_rpn_offsets, dtype=torch.float),
            torch.tensor(actual_rpn_offsets_mask, dtype=torch.long))
        # ä¿å­˜æ‰¹æ¬¡
        if len(image_tensors) >= batch_size:
            prepare_save_batch(batch, image_tensors, image_boxes_labels)
            image_tensors.clear()
            image_boxes_labels.clear()
            batch += 1
    # ä¿å­˜å‰©ä½™çš„æ‰¹æ¬¡
    if len(image_tensors) > 10:
        prepare_save_batch(batch, image_tensors, image_boxes_labels)

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel().to(device)

    # åˆ›å»ºå¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨
    loss_function = MyModel.loss_function

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_rpn_accuracy_history = []
    training_cls_accuracy_history = []
    validating_rpn_accuracy_history = []
    validating_cls_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_rpn_accuracy_highest = -1
    validating_rpn_accuracy_highest_epoch = 0
    validating_cls_accuracy_highest = -1
    validating_cls_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            x, y = load_tensor(path)
            yield x.to(device), y

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    calc_accuracy = MyModel.calc_accuracy

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_rpn_accuracy_list = []
        training_cls_accuracy_list = []
        for batch_index, batch in enumerate(read_batches("data/training_set")):
            # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
            batch_x, batch_y = batch
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted, batch_y)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_batch_rpn_accuracy, training_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
            # è¾“å‡ºæ‰¹æ¬¡æ­£ç¡®ç‡
            training_rpn_accuracy_list.append(training_batch_rpn_accuracy)
            training_cls_accuracy_list.append(training_batch_cls_accuracy)
            print(f"epoch: {epoch}, batch: {batch_index}: " +
                f"batch rpn accuracy: {training_batch_rpn_accuracy}, cls accuracy: {training_batch_cls_accuracy}")
        training_rpn_accuracy = sum(training_rpn_accuracy_list) / len(training_rpn_accuracy_list)
        training_cls_accuracy = sum(training_cls_accuracy_list) / len(training_cls_accuracy_list)
        training_rpn_accuracy_history.append(training_rpn_accuracy)
        training_cls_accuracy_history.append(training_cls_accuracy)
        print(f"training rpn accuracy: {training_rpn_accuracy}, cls accuracy: {training_cls_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_rpn_accuracy_list = []
        validating_cls_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_y = batch
            predicted = model(batch_x)
            validating_batch_rpn_accuracy, validating_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
            validating_rpn_accuracy_list.append(validating_batch_rpn_accuracy)
            validating_cls_accuracy_list.append(validating_batch_cls_accuracy)
            # é‡Šæ”¾ predicted å ç”¨çš„æ˜¾å­˜é¿å…æ˜¾å­˜ä¸è¶³çš„é”™è¯¯
            predicted = None
        validating_rpn_accuracy = sum(validating_rpn_accuracy_list) / len(validating_rpn_accuracy_list)
        validating_cls_accuracy = sum(validating_cls_accuracy_list) / len(validating_cls_accuracy_list)
        validating_rpn_accuracy_history.append(validating_rpn_accuracy)
        validating_cls_accuracy_history.append(validating_cls_accuracy)
        print(f"validating rpn accuracy: {validating_rpn_accuracy}, cls accuracy: {validating_cls_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        # åªä¾æ®æ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åˆ¤æ–­ï¼Œå› ä¸ºæ ‡ç­¾åˆ†ç±»æ­£ç¡®ç‡åŒæ—¶åŸºäº RPN æ­£ç¡®ç‡
        if validating_cls_accuracy > validating_cls_accuracy_highest:
            validating_rpn_accuracy_highest = validating_rpn_accuracy
            validating_rpn_accuracy_highest_epoch = epoch
            validating_cls_accuracy_highest = validating_cls_accuracy
            validating_cls_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest cls validating accuracy updated")
        elif (epoch - validating_rpn_accuracy_highest_epoch > 20 and
            epoch - validating_cls_accuracy_highest_epoch > 20):
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest rpn validating accuracy: {validating_rpn_accuracy_highest}",
        f"from epoch {validating_rpn_accuracy_highest_epoch}")
    print(f"highest cls validating accuracy: {validating_cls_accuracy_highest}",
        f"from epoch {validating_cls_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_rpn_accuracy_list = []
    testing_cls_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_y = batch
        predicted = model(batch_x)
        testing_batch_rpn_accuracy, testing_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
        testing_rpn_accuracy_list.append(testing_batch_rpn_accuracy)
        testing_cls_accuracy_list.append(testing_batch_cls_accuracy)
    testing_rpn_accuracy = sum(testing_rpn_accuracy_list) / len(testing_rpn_accuracy_list)
    testing_cls_accuracy = sum(testing_cls_accuracy_list) / len(testing_cls_accuracy_list)
    print(f"testing rpn accuracy: {testing_rpn_accuracy}, cls accuracy: {testing_cls_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_rpn_accuracy_history, label="training_rpn_accuracy")
    pyplot.plot(training_cls_accuracy_history, label="training_cls_accuracy")
    pyplot.plot(validating_rpn_accuracy_history, label="validating_rpn_accuracy")
    pyplot.plot(validating_cls_accuracy_history, label="validating_cls_accuracy")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«å›¾ç‰‡"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®å›¾ç‰‡è·¯å¾„ï¼Œå¹¶æ˜¾ç¤ºæ‰€æœ‰å¯èƒ½æ˜¯äººè„¸çš„åŒºåŸŸ
    while True:
        try:
            image_path = input("Image path: ")
            if not image_path:
                continue
            # æ„å»ºè¾“å…¥
            with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
                sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
                img_output = img_original.copy() # å¤åˆ¶å›¾ç‰‡ï¼Œç”¨äºåé¢æ·»åŠ æ ‡è®°
                tensor_in = image_to_tensor(img)
            # é¢„æµ‹è¾“å‡º
            cls_result = model(tensor_in.unsqueeze(0).to(device))[-1][0]
            final_result = MyModel.merge_predicted_result(cls_result)
            # æ ‡è®°åœ¨å›¾ç‰‡ä¸Š
            draw = ImageDraw.Draw(img_output)
            for label, box, rpn_score, cls_score in final_result:
                x, y, w, h = map_box_to_original_image(box, sw, sh)
                score = rpn_score * cls_score
                color = "#00FF00" if CLASSES[label] == "with_mask" else "#FF0000"
                draw.rectangle((x, y, x+w, y+h), outline=color)
                draw.text((x, y-10), CLASSES[label], fill=color)
                draw.text((x, y+h), f"{score:.2f}", fill=color)
                print((x, y, w, h), CLASSES[label], rpn_score, cls_score)
            img_output.save("img_output.png")
            print("saved to img_output.png")
            print()
        except Exception as e:
            print("error:", e)

def eval_video():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯†åˆ«è§†é¢‘"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è§†é¢‘è·¯å¾„ï¼Œç»™å¯èƒ½æ˜¯äººè„¸çš„åŒºåŸŸæ·»åŠ æ ‡è®°å¹¶ä¿å­˜æ–°è§†é¢‘
    import cv2
    font = ImageFont.truetype("FreeMonoBold.ttf", 20)
    while True:
        try:
            video_path = input("Video path: ")
            if not video_path:
                continue
            # è¯»å–è¾“å…¥è§†é¢‘
            video = cv2.VideoCapture(video_path)
            # è·å–æ¯ç§’çš„å¸§æ•°
            fps = int(video.get(cv2.CAP_PROP_FPS))
            # è·å–è§†é¢‘é•¿å®½
            size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))
            # åˆ›å»ºè¾“å‡ºè§†é¢‘
            video_output_path = os.path.join(
                os.path.dirname(video_path),
                os.path.splitext(os.path.basename(video_path))[0] + ".output.avi")
            result = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*"XVID"), fps, size)
            # ç”¨äºå‡å°‘è¯¯åˆ¤çš„å†å²ç»“æœ
            history_results = deque(maxlen = fps // 2)
            # é€å¸§å¤„ç†
            count = 0
            while(True):
                ret, frame = video.read()
                if not ret:
                    break
                # opencv ä½¿ç”¨çš„æ˜¯ BGR, Pillow ä½¿ç”¨çš„æ˜¯ RGB, éœ€è¦è½¬æ¢é€šé“é¡ºåº
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # æ„å»ºè¾“å…¥
                img_original = Image.fromarray(frame_rgb) # åŠ è½½åŸå§‹å›¾ç‰‡
                sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
                img_output = img_original.copy() # å¤åˆ¶å›¾ç‰‡ï¼Œç”¨äºåé¢æ·»åŠ æ ‡è®°
                tensor_in = image_to_tensor(img)
                # é¢„æµ‹è¾“å‡º
                cls_result = model(tensor_in.unsqueeze(0).to(device))[-1][0] or []
                cls_result = MyModel.merge_predicted_result(cls_result)
                # æ ¹æ®å†å²ç»“æœå‡å°‘è¯¯åˆ¤
                final_result = MyModel.fix_predicted_result_from_history(cls_result, history_results)
                # æ ‡è®°åœ¨å›¾ç‰‡ä¸Š
                draw = ImageDraw.Draw(img_output)
                for label, box, rpn_score, cls_score in final_result:
                    x, y, w, h = map_box_to_original_image(box, sw, sh)
                    score = rpn_score * cls_score
                    color = "#00FF00" if CLASSES[label] == "with_mask" else "#FF0000"
                    draw.rectangle((x, y, x+w, y+h), outline=color, width=3)
                    draw.text((x, y-20), CLASSES[label], fill=color, font=font)
                    draw.text((x, y+h), f"{score:.2f}", fill=color, font=font)
                # å†™å…¥å¸§åˆ°è¾“å‡ºè§†é¢‘
                frame_rgb_annotated = numpy.asarray(img_output)
                frame_bgr_annotated = cv2.cvtColor(frame_rgb_annotated, cv2.COLOR_RGB2BGR)
                result.write(frame_bgr_annotated)
                count += 1
                if count % fps == 0:
                    print(f"handled {count//fps}s")
            video.release()
            result.release()
            cv2.destroyAllWindows()
            print(f"saved to {video_output_path}")
            print()
        except Exception as e:
            raise
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    elif operation == "eval-video":
        eval_video()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

è®­ç»ƒä»¥åä½¿ç”¨ `python3 example.py eval-video` å³å¯è¯†åˆ«è§†é¢‘ã€‚

å¦‚æœä½ æƒ³è¦ç°æˆè®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¸‹è½½[è¿™ä¸ªæ–‡ä»¶](https://github.com/303248153/BlogArchive/releases/download/202001/faster-rcnn-10.5-model.pt)ï¼Œæ”¹åä¸º `model.pt` ç„¶åæ”¾åœ¨ä»¥ä¸Šä»£ç çš„æ‰€åœ¨çš„ç›®å½•ä¸‹ã€‚

## è§†é¢‘è¯†åˆ«ç»“æœ

ä»¥ä¸‹æ˜¯è§†é¢‘è¯†åˆ«ç»“æœğŸ¤—ï¼š

[è§†é¢‘ 1](https://github.com/303248153/BlogArchive/releases/download/202001/wuhan.output.avi)

[è§†é¢‘ 2](https://github.com/303248153/BlogArchive/releases/download/202001/shanghai.output.avi)

## å†™åœ¨æœ€å

è¿™ç¯‡ä»‹ç»äº†å¦‚ä½•æ”¹è¿› Faster-RCNN æ¨¡å‹æ¥æ›´å‡†ç¡®çš„è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©ï¼Œä¸è¿‡ä¸­å›½ç›®å‰å·²ç»å¼€å§‹æ¥ç§ç–«è‹—äº†ï¼Œæˆ‘é™„è¿‘çš„é•‡åŒºä¹Ÿå‡ºç°ç–«è‹—æ¥ç§ç‚¹äº†ï¼ˆè¿˜æ˜¯å…è´¹çš„ï¼‰ï¼Œç›¸ä¿¡å¾ˆå¿«æ‰€æœ‰äººéƒ½ä¸å†éœ€è¦æˆ´å£ç½©ï¼Œå›½å®¶åº”å¯¹ç–«æƒ…çš„è¡¨ç°éå¸¸ä»¤äººéª„å‚²ï¼Œå–Šä¸€å¥ï¼šå‰å®³äº†æˆ‘çš„å›½ğŸ˜¤ï¼

ä¸‹ä¸€ç¯‡å°†ä¼šä»‹ç» YOLO æ¨¡å‹ï¼Œåœºæ™¯åŒæ ·æ˜¯è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©ï¼Œå†™å®Œå°±ä¼šç ”ç©¶å…¶ä»–ä¸œè¥¿å»äº†ğŸ¤’ã€‚
