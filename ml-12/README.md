# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (åäºŒ) - è„¸éƒ¨å…³é”®ç‚¹æ£€æµ‹

åœ¨å‰å‡ ç¯‡æ–‡ç« ä¸­æˆ‘ä»¬çœ‹åˆ°äº†æ€æ ·æ£€æµ‹å›¾ç‰‡ä¸Šçš„ç‰©ä½“ï¼Œä¾‹å¦‚äººè„¸ï¼Œé‚£ä¹ˆæŠŠå®ç°äººè„¸è¯†åˆ«çš„æ—¶å€™æ˜¯ä¸æ˜¯å¯ä»¥æŠŠå›¾ç‰‡ä¸­çš„äººè„¸æˆªå–å‡ºæ¥å†äº¤ç»™è¯†åˆ«äººè„¸çš„æ¨¡å‹å‘¢ï¼Ÿä¸‹é¢çš„æµç¨‹æ˜¯å¯è¡Œçš„ï¼Œä½†å› ä¸ºäººè„¸çš„èŒƒå›´ä¸å¤Ÿå‡†ç¡®ï¼Œæˆªå–å‡ºæ¥çš„äººè„¸å¹¶ä¸åœ¨å›¾ç‰‡çš„æ­£ä¸­å¿ƒï¼Œå¯¹äºè¯†åˆ«äººè„¸çš„æ¨¡å‹æ¥è¯´ï¼Œæ•°æ®è´¨é‡ä¸å¤Ÿå¥½å°±ä¼šå¯¼è‡´è¯†åˆ«çš„æ•ˆæœæ‰“æŠ˜ã€‚

![01](./01.png)

è¿™ä¸€ç¯‡æ–‡ç« ä¼šä»‹ç»å¦‚ä½•ä½¿ç”¨æœºå™¨å­¦ä¹ æ£€æµ‹è„¸éƒ¨å…³é”®ç‚¹ (çœ¼ç›é¼»å­å˜´å·´çš„ä½ç½®)ï¼Œæ£€æµ‹å›¾ç‰‡ä¸Šçš„äººè„¸ä»¥åï¼Œå†æ£€æµ‹è„¸éƒ¨å…³é”®ç‚¹ï¼Œç„¶ååŸºäºè„¸éƒ¨å…³é”®ç‚¹æ¥è°ƒæ•´äººè„¸èŒƒå›´ï¼Œå†æ ¹æ®è°ƒæ•´åçš„äººè„¸èŒƒå›´æˆªå–äººè„¸å¹¶äº¤ç»™åé¢çš„æ¨¡å‹ï¼Œå³å¯æå‡æ•°æ®è´¨é‡æ”¹å–„è¯†åˆ«æ•ˆæœã€‚

![02](./02.png)

## è„¸éƒ¨å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹

è„¸éƒ¨å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹å…¶å®å°±æ˜¯æ™®é€šçš„ CNN æ¨¡å‹ï¼Œåœ¨[ç¬¬å…«ç¯‡æ–‡ç« ](https://www.cnblogs.com/zkweb/p/13354826.html)ä¸­å·²ç»ä»‹ç»è¿‡ğŸ¤’ï¼Œç¬¬å…«ç¯‡æ–‡ç« ä¸­ï¼Œè¾“å…¥æ˜¯å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯åˆ†ç±» (ä¾‹å¦‚åŠ¨ç‰©çš„åˆ†ç±»ï¼Œæˆ–è€…éªŒè¯ç ä¸­çš„å­—æ¯åˆ†ç±»)ã€‚è€Œè¿™ä¸€ç¯‡æ–‡ç« è¾“å…¥åŒæ ·æ˜¯å›¾ç‰‡ï¼Œè¾“å‡ºåˆ™æ˜¯å„ä¸ªè„¸éƒ¨å…³é”®ç‚¹çš„åæ ‡ï¼š

![03](./03.png)

æˆ‘ä»¬ä¼šè®©æ¨¡å‹è¾“å‡ºäº”ä¸ªå…³é”®ç‚¹ (å·¦çœ¼ä¸­å¿ƒï¼Œå³çœ¼ä¸­å¿ƒï¼Œé¼»å°–ï¼Œå˜´å·´å·¦è¾¹è§’ï¼Œå˜´å·´å³è¾¹è§’) çš„ x åæ ‡ä¸ y åæ ‡ï¼Œåˆè®¡ä¸€å…± 10 ä¸ªè¾“å‡ºã€‚

æ¨¡å‹è¾“å‡ºçš„åæ ‡å€¼èŒƒå›´ä¼šè½åœ¨ -1 ~ 1 ä¹‹é—´ï¼Œè¿™æ˜¯æŠŠå›¾ç‰‡å·¦ä¸Šè§’è§†ä¸º -1,-1ï¼Œå³ä¸‹è§’è§†ä¸º 1,1 ä»¥åæ­£è§„åŒ–çš„åæ ‡å€¼ã€‚ä¸ä½¿ç”¨ç»å¯¹å€¼çš„åŸå› æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹å¹¶ä¸é€‚åˆå¤„ç†è¾ƒå¤§çš„å€¼ï¼Œå¹¶ä¸”ä½¿ç”¨ç›¸å¯¹åæ ‡å¯ä»¥è®©å¤„ç†ä¸åŒå¤§å°å›¾ç‰‡çš„é€»è¾‘æ›´åŠ ç®€å•ã€‚ä½ å¯èƒ½ä¼šé—®ä¸ºä»€ä¹ˆä¸åƒå‰ä¸€ç¯‡ä»‹ç»çš„ YOLO ä¸€æ ·ï¼Œè®©åæ ‡å€¼èŒƒå›´è½åœ¨ 0 ~ 1 ä¹‹é—´ï¼Œè¿™æ˜¯å› ä¸ºä¸‹é¢ä¼šä½¿ç”¨ä»¿å°„å˜æ¢æ¥å¢åŠ äººè„¸æ ·æœ¬ï¼Œè€Œä»¿å°„å˜æ¢è¦æ±‚ç›¸å¯¹åæ ‡åœ¨ -1 ~ 1 ä¹‹é—´ï¼Œè®©åæ ‡å€¼èŒƒå›´è½åœ¨ -1 ~ 1 ä¹‹é—´å¯ä»¥çœæ‰è½¬æ¢çš„æ­¥éª¤ã€‚

## è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†

å‡†å¤‡æ•°æ®é›†æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¤´ç–¼çš„éƒ¨åˆ†ï¼Œä¸€èˆ¬æ¥è¯´æˆ‘ä»¬éœ€è¦ä¸Šç™¾åº¦æœç´¢äººè„¸çš„å›¾ç‰‡ï¼Œç„¶åä¸€å¼ ä¸€å¼ çš„æˆªå–ï¼Œå†æ‰‹åŠ¨æ ‡è®°å„ä¸ªå™¨å®˜çš„ä½ç½®ï¼Œä½†è¿™æ ·å¤ªè‹¦ç´¯äº†ğŸ˜­ã€‚è¿™ç¯‡è¿˜æ˜¯åƒä¹‹å‰çš„æ–‡ç« ä¸€æ ·ï¼Œä»ç½‘ä¸Šæ‰¾ä¸€ä¸ªç°æˆçš„æ•°æ®é›†æ¥è®­ç»ƒï¼Œå·ä¸ªæ‡’ğŸ¤—ã€‚

ä½¿ç”¨çš„æ•°æ®é›†ï¼š

https://www.kaggle.com/drgilermo/face-images-with-marked-landmark-points

ä¸‹è½½å›æ¥ä»¥åå¯ä»¥çœ‹åˆ°ä»¥ä¸‹çš„æ–‡ä»¶ï¼š

``` text
face_images.npz
facial_keypoints.csv
```

`face_images.npz` æ˜¯ä½¿ç”¨ zip å‹ç¼©åçš„ numpy æ•°æ®è½¬å‚¨æ–‡ä»¶ï¼ŒæŠŠæ–‡ä»¶åæ”¹ä¸º `face_images.zip` ä»¥åå†è§£å‹ç¼©å³å¯å¾—åˆ° `face_images.npy` æ–‡ä»¶ã€‚

ä¹‹åå†æ‰§è¡Œ python å‘½ä»¤è¡Œï¼Œè¾“å…¥ä»¥ä¸‹ä»£ç åŠ è½½æ•°æ®å†…å®¹ï¼š

``` python
>>> import numpy
>>> data = numpy.load("face_images.npy")
>>> data.shape
(96, 96, 7049)
```

å¯ä»¥çœ‹åˆ°æ•°æ®åŒ…å«äº† 7049 å¼  96x96 çš„é»‘ç™½äººè„¸å›¾ç‰‡ã€‚

å†è¾“å…¥ä»¥ä¸‹ä»£ç ä¿å­˜ç¬¬ä¸€ç« å›¾ç‰‡ï¼š

``` python
>>> import torch
>>> data = torch.from_numpy(data).float()
>>> data.shape
torch.Size([96, 96, 7049])
# æŠŠé€šé“æ”¾åœ¨æœ€å‰é¢
>>> data = data.permute(2, 0, 1)
>>> data.shape
torch.Size([7049, 96, 96])
# æå–ç¬¬ä¸€å¼ å›¾ç‰‡çš„æ•°æ®å¹¶ä¿å­˜
>>> from PIL import Image
>>> img = Image.fromarray(data[0].numpy()).convert("RGB")
>>> img.save("1.png")
```

è¿™å°±æ˜¯æå–å‡ºæ¥çš„å›¾ç‰‡ï¼š

![face](./face.png)

å¯¹åº”ä»¥ä¸‹çš„åæ ‡ï¼Œåæ ‡çš„å€¼å¯ä»¥åœ¨ `facial_keypoints.csv` ä¸­æ‰¾åˆ°ï¼š

``` text
left_eye_center_x,left_eye_center_y,right_eye_center_x,right_eye_center_y,left_eye_inner_corner_x,left_eye_inner_corner_y,left_eye_outer_corner_x,left_eye_outer_corner_y,right_eye_inner_corner_x,right_eye_inner_corner_y,right_eye_outer_corner_x,right_eye_outer_corner_y,left_eyebrow_inner_end_x,left_eyebrow_inner_end_y,left_eyebrow_outer_end_x,left_eyebrow_outer_end_y,right_eyebrow_inner_end_x,right_eyebrow_inner_end_y,right_eyebrow_outer_end_x,right_eyebrow_outer_end_y,nose_tip_x,nose_tip_y,mouth_left_corner_x,mouth_left_corner_y,mouth_right_corner_x,mouth_right_corner_y,mouth_center_top_lip_x,mouth_center_top_lip_y,mouth_center_bottom_lip_x,mouth_center_bottom_lip_y
66.0335639098,39.0022736842,30.2270075188,36.4216781955,59.582075188,39.6474225564,73.1303458647,39.9699969925,36.3565714286,37.3894015038,23.4528721805,37.3894015038,56.9532631579,29.0336481203,80.2271278195,32.2281383459,40.2276090226,29.0023218045,16.3563789474,29.6474706767,44.4205714286,57.0668030075,61.1953082707,79.9701654135,28.6144962406,77.3889924812,43.3126015038,72.9354586466,43.1307067669,84.4857744361
```

å„ä¸ªåæ ‡å¯¹åº” csv ä¸­çš„å­—æ®µå¦‚ä¸‹ï¼š

- å·¦çœ¼ä¸­å¿ƒç‚¹çš„ x åæ ‡: left_eye_center_x
- å·¦çœ¼ä¸­å¿ƒç‚¹çš„ y åæ ‡: left_eye_center_y
- å³çœ¼ä¸­å¿ƒç‚¹çš„ x åæ ‡: right_eye_center_x
- å³çœ¼ä¸­å¿ƒç‚¹çš„ y åæ ‡: right_eye_center_y
- é¼»å°–çš„ x åæ ‡: nose_tip_x
- é¼»å°–çš„ y åæ ‡: nose_tip_y
- å˜´å·´å·¦è¾¹è§’çš„ x åæ ‡: mouth_left_corner_x
- å˜´å·´å·¦è¾¹è§’çš„ y åæ ‡: mouth_left_corner_y
- å˜´å·´å³è¾¹è§’çš„ x åæ ‡: mouth_right_corner_x
- å˜´å·´å³è¾¹è§’çš„ y åæ ‡: mouth_right_corner_y

csv ä¸­è¿˜æœ‰æ›´å¤šçš„åæ ‡ä½†æˆ‘ä»¬åªä½¿ç”¨è¿™äº›ğŸ¤’ã€‚

æ¥ä¸‹æ¥å®šä¹‰ä¸€ä¸ªåœ¨å›¾ç‰‡ä¸Šæ ‡è®°å…³é”®ç‚¹çš„å‡½æ•°ï¼š

``` python
from PIL import ImageDraw

DefaultPointColors = ["#FF0000", "#FF00FF", "#00FF00", "#00FFFF", "#FFFF00"]
def draw_points(img, points, colors = None, radius=1):
    """åœ¨å›¾ç‰‡ä¸Šæç”»å…³é”®ç‚¹"""
    draw = ImageDraw.Draw(img)
    colors = colors or DefaultPointColors
    for index, point in enumerate(points):
        x, y = point
        color = colors[index] if index < len(colors) else colors[0]
        draw.ellipse((x-radius, y-radius, x+radius, y+radius), fill=color, width=1)
```

å†ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ ‡è®°å›¾ç‰‡å³å¯å¾—åˆ°ï¼š

![face_marked](./face_marked.png)

## ä½¿ç”¨ä»¿å°„å˜æ¢å¢åŠ äººè„¸æ ·æœ¬

ä»”ç»†è§‚å¯Ÿ csv ä¸­çš„åæ ‡å€¼ï¼Œä½ å¯èƒ½ä¼šå‘ç°é‡Œé¢çš„åæ ‡å¤§å¤šéƒ½æ˜¯å¾ˆæ¥è¿‘çš„ï¼Œä¾‹å¦‚å·¦çœ¼ä¸­å¿ƒç‚¹çš„ x åæ ‡å¤§éƒ¨åˆ†éƒ½è½åœ¨ 65 ~ 75 ä¹‹é—´ã€‚è¿™æ˜¯å› ä¸ºæ•°æ®ä¸­çš„äººè„¸å›¾ç‰‡éƒ½æ˜¯ç»è¿‡å¤„ç†çš„ï¼Œå æ¯”å’Œä½ç½®æ¯”è¾ƒæ ‡å‡†ã€‚å¦‚æœæˆ‘ä»¬ç›´æ¥æ‹¿è¿™ä¸ªæ•°æ®é›†æ¥è®­ç»ƒï¼Œé‚£ä¹ˆæ¨¡å‹åªä¼šè¾“å‡ºå­¦ä¹ è¿‡çš„åŒºé—´çš„å€¼ï¼Œè¿™æ˜¯å†æ‹¿ä¸€å¼ å æ¯”å’Œä½ç½®ä¸æ ‡å‡†çš„äººè„¸å›¾ç‰‡ç»™æ¨¡å‹ï¼Œæ¨¡å‹å°±ä¼šè¾“å‡ºé”™è¯¯çš„åæ ‡ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜æˆ‘ä»¬å¯ä»¥éšæœºæ—‹è½¬ç§»åŠ¨ç¼©æ”¾äººè„¸ä»¥å¢åŠ æ•°æ®é‡ï¼Œåœ¨[ç¬¬åç¯‡æ–‡ç« ](https://www.cnblogs.com/zkweb/p/14078501.html)æˆ‘ä»¬å­¦åˆ°æ€æ ·ç”¨ä»¿å°„å˜æ¢æ¥æå–å›¾ç‰‡ä¸­çš„æŸä¸ªåŒºåŸŸå¹¶ç¼©æ”¾åˆ°å›ºå®šçš„å¤§å°ï¼Œä»¿å°„å˜æ¢è¿˜å¯ä»¥ç”¨æ¥å®ç°æ—‹è½¬ç§»åŠ¨å’Œç¼©æ”¾ï¼Œæ‰¹é‡è®¡ç®—æ—¶çš„æ•ˆç‡éå¸¸é«˜ã€‚

![04.png](./04.png)

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä»¥ä¸‹çš„å˜é‡ï¼š

- å¼§åº¦ï¼ŒèŒƒå›´æ˜¯ -Ï€ ~ Ï€ï¼Œå¯¹åº” -180Â°~ 180Â°
- ç¼©æ”¾æ¯”ä¾‹ï¼Œ1 ä»£è¡¨ 100%
- æ¨ªå‘ç§»åŠ¨é‡ï¼šèŒƒå›´æ˜¯ -1 ~ 1ï¼ŒæŠŠå›¾ç‰‡ä¸­å¿ƒè§†ä¸º 0ï¼Œå·¦è¾¹è§†ä¸º -1ï¼Œå³è¾¹è§†ä¸º 1
- çºµå‘ç§»åŠ¨é‡ï¼šèŒƒå›´æ˜¯ -1 ~ 1ï¼ŒæŠŠå›¾ç‰‡ä¸­å¿ƒè§†ä¸º 0ï¼Œå·¦è¾¹è§†ä¸º -1ï¼Œå³è¾¹è§†ä¸º 1

æ ¹æ®è¿™äº›å˜é‡ç”Ÿæˆä»¿å°„å˜æ¢å‚æ•°çš„å…¬å¼å¦‚ä¸‹ï¼š

![05](./05.png)

éœ€è¦æ³¨æ„çš„æ˜¯ä»¿å°„å˜æ¢å‚æ•°ç”¨äºè½¬æ¢ **ç›®æ ‡åæ ‡** åˆ° **æ¥æºåæ ‡**ï¼Œåœ¨å¤„ç†å›¾ç‰‡çš„æ—¶å€™å¯ä»¥æ ¹æ®ç›®æ ‡åƒç´ æ‰¾åˆ°æ¥æºåƒç´ ï¼Œç„¶åè®¾ç½®æ¥æºåƒç´ çš„å€¼åˆ°ç›®æ ‡åƒç´ çš„å€¼å®ç°å„ç§å˜å½¢æ“ä½œã€‚ä¸Šè¿°çš„å‚æ•°åªèƒ½ç”¨äºå¤„ç†å›¾ç‰‡ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è®¡ç®—å˜å½¢ä»¥åçš„å›¾ç‰‡å¯¹åº”çš„åæ ‡ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªè½¬æ¢ **æ¥æºåæ ‡** åˆ° **ç›®æ ‡åæ ‡** çš„ä»¿å°„å˜æ¢å‚æ•°ï¼Œè®¡ç®—ç›¸åçš„ä»¿å°„å˜æ¢å‚æ•°çš„å…¬å¼å¦‚ä¸‹ï¼š

![06](./06.png)

ç¿»è¯‘åˆ°ä»£ç å¦‚ä¸‹ï¼š

``` python
def generate_transform_theta(angle, scale, x_offset, y_offset, inverse=False):
    """
    è®¡ç®—å˜å½¢å‚æ•°
    angle: èŒƒå›´ -math.pi ~ math.pi
    scale: 1 ä»£è¡¨ 100%
    x_offset: èŒƒå›´ -1 ~ 1
    y_offset: èŒƒå›´ -1 ~ 1
    inverse: æ˜¯å¦è®¡ç®—ç›¸åçš„å˜å½¢å‚æ•° (é»˜è®¤è®¡ç®—æŠŠç›®æ ‡åæ ‡è½¬æ¢ä¸ºæ¥æºåæ ‡çš„å‚æ•°)
    """
    cos_a = math.cos(angle)
    sin_a = math.sin(angle)
    if inverse:
        return (
            ( cos_a * scale, sin_a * scale, -x_offset * cos_a * scale - y_offset * sin_a * scale),
            (-sin_a * scale, cos_a * scale, -y_offset * cos_a * scale + x_offset * sin_a * scale))
    else:
        return (
            (cos_a / scale, -sin_a / scale, x_offset),
            (sin_a / scale,  cos_a / scale, y_offset))
```

å˜å½¢åçš„äººè„¸æ ·æœ¬å¦‚ä¸‹ï¼ŒèƒŒæ™¯æ·»åŠ äº†éšæœºé¢œè‰²è®©æ¨¡å‹æ›´éš¾ä½œå¼Šï¼Œå…·ä½“ä»£ç å‚è€ƒåé¢çš„ `prepare` å‡½æ•°å§ğŸ˜°ï¼š

![07](./07.png)

## å®Œæ•´ä»£ç 

å®Œæ•´ä»£ç çš„æ—¶é—´åˆ°äº†ğŸ¥³ï¼Œç»“æ„è·Ÿå‰é¢çš„æ–‡ç« ä¸€æ ·ï¼Œåˆ†ä¸º `prepare`, `train`, `eval` ä¸‰æ­¥ã€‚

``` python
import os
import sys
import torch
import gzip
import itertools
import random
import numpy
import math
import json
import pandas
import torchvision
from PIL import Image, ImageDraw
from torch import nn
from matplotlib import pyplot
from collections import defaultdict

# å›¾ç‰‡å¤§å°
IMAGE_SIZE = (96, 96)
# è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†è·¯å¾„
DATASET_PATH = "./dataset/face-images-with-marked-landmark-points/face_images.npy"
DATASET_CSV_PATH = "./dataset/face-images-with-marked-landmark-points/facial_keypoints.csv"
# é’ˆå¯¹å„å¼ å›¾ç‰‡éšæœºå˜å½¢çš„æ•°é‡
RANDOM_TRANSFORM_SAMPLES = 10

# ç”¨äºå¯ç”¨ GPU æ”¯æŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FaceLandmarkModel(nn.Module):
    """
    æ£€æµ‹è„¸éƒ¨å…³é”®ç‚¹çš„æ¨¡å‹ (åŸºäº ResNet-18)
    é’ˆå¯¹å›¾ç‰‡è¾“å‡º:
    - å·¦çœ¼ä¸­å¿ƒç‚¹çš„ x åæ ‡
    - å·¦çœ¼ä¸­å¿ƒç‚¹çš„ y åæ ‡
    - å³çœ¼ä¸­å¿ƒç‚¹çš„ x åæ ‡
    - å³çœ¼ä¸­å¿ƒç‚¹çš„ y åæ ‡
    - é¼»å°–çš„ x åæ ‡
    - é¼»å°–çš„ y åæ ‡
    - å˜´å·´å·¦è¾¹è§’çš„ x åæ ‡
    - å˜´å·´å·¦è¾¹è§’çš„ y åæ ‡
    - å˜´å·´å³è¾¹è§’çš„ x åæ ‡
    - å˜´å·´å³è¾¹è§’çš„ y åæ ‡
    ä»¥ä¸Šåæ ‡å‡åœ¨ 0 ~ 1 çš„èŒƒå›´å†…ï¼Œè¡¨ç¤ºç›¸å¯¹å›¾ç‰‡é•¿å®½çš„ä½ç½®
    """

    def __init__(self):
        super().__init__()
        # Resnet çš„å®ç°
        self.resnet = torchvision.models.resnet18(num_classes=256)
        # æ”¯æŒé»‘ç™½å›¾ç‰‡
        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        # æœ€ç»ˆè¾“å‡ºå…³é”®ç‚¹çš„çº¿æ€§æ¨¡å‹
        # å› ä¸º torchvision çš„ resnet æœ€ç»ˆä¼šä½¿ç”¨ä¸€ä¸ª Linearï¼Œè¿™é‡Œçœç•¥æ‰ç¬¬ä¸€ä¸ª Lienar
        self.linear = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 10))

    def forward(self, x):
        tmp = self.resnet(x)
        y = self.linear(tmp)
        return y

    def detect_landmarks(self, images):
        """æ£€æµ‹ç»™å‡ºå›¾ç‰‡çš„å…³é”®ç‚¹"""
        tensor_in = torch.stack([ image_to_tensor(resize_image(img)) for img in images ])
        tensor_out = self.forward(tensor_in.to(device))
        tensor_out = tensor_out.reshape(len(images), -1, 2)
        # è½¬æ¢ -1 ~ 1 çš„åæ ‡å›ç»å¯¹åæ ‡
        size = torch.tensor(IMAGE_SIZE, dtype=torch.float).to(device)
        tensor_out = (tensor_out + 1) / 2 * size
        result = []
        for image, points in zip(images, tensor_out):
            points_mapped = []
            for point in points:
                points_mapped.append(map_point_to_original_image(point.tolist(), *image.size))
            result.append(points_mapped)
        return result

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def calc_resize_parameters(sw, sh):
    """è®¡ç®—ç¼©æ”¾å›¾ç‰‡çš„å‚æ•°"""
    sw_new, sh_new = sw, sh
    dw, dh = IMAGE_SIZE
    pad_w, pad_h = 0, 0
    if sw / sh < dw / dh:
        sw_new = int(dw / dh * sh)
        pad_w = (sw_new - sw) // 2 # å¡«å……å·¦å³
    else:
        sh_new = int(dh / dw * sw)
        pad_h = (sh_new - sh) // 2 # å¡«å……ä¸Šä¸‹
    return sw_new, sh_new, pad_w, pad_h

def resize_image(img):
    """ç¼©æ”¾å›¾ç‰‡ï¼Œæ¯”ä¾‹ä¸ä¸€è‡´æ—¶å¡«å……"""
    sw, sh = img.size
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    img_new = Image.new("RGB", (sw_new, sh_new))
    img_new.paste(img, (pad_w, pad_h))
    img_new = img_new.resize(IMAGE_SIZE)
    return img_new

def image_to_tensor(img):
    """ç¼©æ”¾å¹¶è½¬æ¢å›¾ç‰‡å¯¹è±¡åˆ° tensor å¯¹è±¡ (é»‘ç™½)"""
    img = img.convert("L") # è½¬æ¢åˆ°é»‘ç™½å›¾ç‰‡å¹¶ç¼©æ”¾
    arr = numpy.asarray(img)
    t = torch.from_numpy(arr)
    t = t.unsqueeze(0) # æ·»åŠ é€šé“
    t = t / 255.0 # æ­£è§„åŒ–æ•°å€¼ä½¿å¾—èŒƒå›´åœ¨ 0 ~ 1
    return t

def map_point_to_original_image(point, sw, sh):
    """æŠŠç¼©æ”¾åçš„åæ ‡è½¬æ¢åˆ°ç¼©æ”¾å‰çš„åæ ‡"""
    x, y = point
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int(x / scale - pad_w)
    y = int(y / scale - pad_h)
    x = min(max(0, x), sw - 1)
    y = min(max(0, y), sh - 1)
    return x, y

DefaultPointColors = ["#FF0000", "#FF00FF", "#00FF00", "#00FFFF", "#FFFF00"]
def draw_points(img, points, colors = None, radius=1):
    """åœ¨å›¾ç‰‡ä¸Šæç”»å…³é”®ç‚¹"""
    draw = ImageDraw.Draw(img)
    colors = colors or DefaultPointColors
    for index, point in enumerate(points):
        x, y = point
        color = colors[index] if index < len(colors) else colors[0]
        draw.ellipse((x-radius, y-radius, x+radius, y+radius), fill=color, width=1)

def generate_transform_theta(angle, scale, x_offset, y_offset, inverse=False):
    """
    è®¡ç®—å˜å½¢å‚æ•°
    angle: èŒƒå›´ -math.pi ~ math.pi
    scale: 1 ä»£è¡¨ 100%
    x_offset: èŒƒå›´ -1 ~ 1
    y_offset: èŒƒå›´ -1 ~ 1
    inverse: æ˜¯å¦è®¡ç®—ç›¸åçš„å˜å½¢å‚æ•° (é»˜è®¤è®¡ç®—æŠŠç›®æ ‡åæ ‡è½¬æ¢ä¸ºæ¥æºåæ ‡çš„å‚æ•°)
    """
    cos_a = math.cos(angle)
    sin_a = math.sin(angle)
    if inverse:
        return (
            ( cos_a * scale, sin_a * scale, -x_offset * cos_a * scale - y_offset * sin_a * scale),
            (-sin_a * scale, cos_a * scale, -y_offset * cos_a * scale + x_offset * sin_a * scale))
    else:
        return (
            (cos_a / scale, -sin_a / scale, x_offset),
            (sin_a / scale,  cos_a / scale, y_offset))

def prepare_save_batch(batch, image_tensors, point_tensors):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # è¿æ¥æ‰€æœ‰æ•°æ®
    # image_tensor çš„ç»´åº¦ä¼šå˜ä¸º æ•°é‡,1,W,H
    # point_tensor çš„ç»´åº¦ä¼šå˜ä¸º æ•°é‡,10 (10 = 5 ä¸ªå…³é”®ç‚¹çš„ x y åæ ‡)
    image_tensor = torch.cat(image_tensors, dim=0)
    image_tensor = image_tensor.unsqueeze(1)
    point_tensor = torch.cat(point_tensors, dim=0)
    point_tensor = point_tensor.reshape(point_tensor.shape[0], -1)

    # åˆ‡åˆ†è®­ç»ƒé›† (80%)ï¼ŒéªŒè¯é›† (10%) å’Œæµ‹è¯•é›† (10%)
    random_indices = torch.randperm(image_tensor.shape[0])
    training_indices = random_indices[:int(len(random_indices)*0.8)]
    validating_indices = random_indices[int(len(random_indices)*0.8):int(len(random_indices)*0.9):]
    testing_indices = random_indices[int(len(random_indices)*0.9):]
    training_set = (image_tensor[training_indices], point_tensor[training_indices])
    validating_set = (image_tensor[validating_indices], point_tensor[validating_indices])
    testing_set = (image_tensor[testing_indices], point_tensor[testing_indices])

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # åŠ è½½åŸå§‹æ•°æ®é›†
    images_data = torch.from_numpy(numpy.load(DATASET_PATH)).float()
    images_csv = pandas.read_csv(DATASET_CSV_PATH, usecols=(
        "left_eye_center_x",
        "left_eye_center_y",
        "right_eye_center_x",
        "right_eye_center_y",
        "nose_tip_x",
        "nose_tip_y",
        "mouth_left_corner_x",
        "mouth_left_corner_y",
        "mouth_right_corner_x",
        "mouth_right_corner_y"
    ))

    # åŸå§‹æ•°æ®çš„ç»´åº¦æ˜¯ (W, H, å›¾ç‰‡æ•°é‡)ï¼Œéœ€è¦è½¬æ¢ä¸º (å›¾ç‰‡æ•°é‡, W, H)
    images_data = images_data.permute(2, 0, 1)

    # å¤„ç†åŸå§‹æ•°æ®é›†
    batch = 0
    batch_size = 20 # å®é™…ä¼šæ·»åŠ  batch_size * (1 + RANDOM_TRANSFORM_SAMPLES) åˆ°æ¯ä¸ªæ‰¹æ¬¡
    image_tensors = []
    point_tensors = []
    for image_data, image_row in zip(images_data, images_csv.values):
        # è¯»å–å›¾ç‰‡å‚æ•°
        w = image_data.shape[0]
        h = image_data.shape[1]
        assert w == IMAGE_SIZE[0]
        assert h == IMAGE_SIZE[1]
        size = torch.tensor((w, h), dtype=torch.float)
        points = torch.tensor(image_row, dtype=torch.float).reshape(-1, 2)
        points_std = points / size * 2 - 1 # å·¦ä¸Šè§’ -1,-1 å³ä¸‹è§’ 1,1
        # æ­£è§„åŒ–å›¾ç‰‡æ•°æ®
        image_data = image_data / 255
        # å¯¹å›¾ç‰‡è¿›è¡Œéšæœºå˜å½¢
        thetas = []
        thetas_inverse = []
        for _ in range(RANDOM_TRANSFORM_SAMPLES):
            angle = math.pi * random.uniform(-0.1, 0.1)
            scale = random.uniform(0.30, 1.0)
            x_offset = random.uniform(-0.2, 0.2)
            y_offset = random.uniform(-0.2, 0.2)
            thetas.append(generate_transform_theta(angle, scale, x_offset, y_offset))
            thetas_inverse.append(generate_transform_theta(angle, scale, x_offset, y_offset, True))
        thetas_tensor = torch.tensor(thetas, dtype=torch.float)
        thetas_inverse_tensor = torch.tensor(thetas_inverse, dtype=torch.float)
        grid = nn.functional.affine_grid(
            thetas_tensor,
            torch.Size((RANDOM_TRANSFORM_SAMPLES, 1, w, h)),
            align_corners=False)
        images_transformed = nn.functional.grid_sample(
            image_data.repeat(RANDOM_TRANSFORM_SAMPLES, 1, 1)
                .reshape(RANDOM_TRANSFORM_SAMPLES, 1, w, h), grid)
        # æ›¿æ¢é»‘è‰²çš„éƒ¨åˆ†åˆ°éšæœºé¢œè‰²
        random_color = torch.rand(images_transformed.shape)
        zero_indices = images_transformed == 0
        images_transformed[zero_indices] = random_color[zero_indices]
        # è½¬æ¢å˜å½¢åçš„åæ ‡
        points_std_with_one = torch.cat((points_std, torch.ones(points_std.shape[0], 1)), dim=1)
        points_std_transformed = points_std_with_one.matmul(thetas_inverse_tensor.permute(0, 2, 1))
        # è¿æ¥åŸå›¾ç‰‡å’Œå˜å½¢åçš„å›¾ç‰‡ï¼ŒåŸåæ ‡å’Œå˜å½¢åçš„åæ ‡
        images_cat = torch.cat((image_data.unsqueeze(0), images_transformed.squeeze(1)), dim=0)
        points_std_cat = torch.cat((points_std.unsqueeze(0), points_std_transformed), dim=0)
        points_cat = (points_std_cat + 1) / 2 * size
        # æµ‹è¯•å˜å½¢åçš„å›¾ç‰‡ä¸å‚æ•°
        # for index, (img_data, points) in enumerate(zip(images_cat, points_cat)):
        #    img = Image.fromarray((img_data * 255).numpy()).convert("RGB")
        #    draw_points(img, points)
        #    img.save(f"test_{index}.png")
        # ä¿å­˜æ‰¹æ¬¡
        image_tensors.append(images_cat)
        point_tensors.append(points_std_cat)
        if len(image_tensors) > batch_size:
            prepare_save_batch(batch, image_tensors, point_tensors)
            image_tensors.clear()
            point_tensors.clear()
            batch += 1
    # ä¿å­˜å‰©ä½™çš„æ‰¹æ¬¡
    if len(image_tensors) > 10:
        prepare_save_batch(batch, image_tensors, point_tensors)

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = FaceLandmarkModel().to(device)

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    def loss_function(predicted, actual):
        predicted_flatten = predicted.view(-1)
        actual_flatten = actual.view(-1)
        mask = torch.isnan(actual_flatten).logical_not() # ç”¨äºè·³è¿‡ç¼ºæŸçš„å€¼
        return nn.functional.mse_loss(predicted_flatten[mask], actual_flatten[mask])

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = -1
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            images_tensor, points_tensor = load_tensor(path)
            yield images_tensor.to(device), points_tensor.to(device)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        predicted_flatten = predicted.view(-1)
        actual_flatten = actual.view(-1)
        mask = torch.isnan(actual_flatten).logical_not() # ç”¨äºè·³è¿‡ç¼ºæŸçš„å€¼
        diff = (predicted_flatten[mask] - actual_flatten[mask]).abs()
        return 1 - diff.mean().item()

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        for batch_index, batch in enumerate(read_batches("data/training_set")):
            # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
            batch_x, batch_y = batch
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted, batch_y)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_batch_accuracy = calc_accuracy(batch_y, predicted)
            training_accuracy_list.append(training_batch_accuracy)
            print(f"epoch: {epoch}, batch: {batch_index}: batch accuracy: {training_batch_accuracy}")
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_y = batch
            predicted = model(batch_x)
            validating_accuracy_list.append(calc_accuracy(batch_y, predicted))
            # é‡Šæ”¾ predicted å ç”¨çš„æ˜¾å­˜é¿å…æ˜¾å­˜ä¸è¶³çš„é”™è¯¯
            predicted = None
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 20:
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_y = batch
        predicted = model(batch_x)
        testing_accuracy_list.append(calc_accuracy(batch_y, predicted))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = FaceLandmarkModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®å›¾ç‰‡è·¯å¾„ï¼Œå¹¶æ ‡è®°å…³é”®ç‚¹
    while True:
        try:
            # æ‰“å¼€å›¾ç‰‡
            image_path = input("Image path: ")
            if not image_path:
                continue
            img = Image.open(image_path)
            # æ£€æµ‹å…³é”®ç‚¹
            points = model.detect_landmarks([img])[0]
            for point in points:
                print(point)
            # è¾“å‡ºå›¾ç‰‡ä¸å…³é”®ç‚¹
            draw_points(img, points)
            img.save("img_output.png")
            print("saved to img_output.png")
            print()
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æ–‡ç« ä¸­çš„ Resnet æ¨¡å‹ç›´æ¥å¼•ç”¨äº† torchvision ä¸­çš„å®ç°ï¼Œç»“æ„åœ¨[ç¬¬å…«ç¯‡æ–‡ç« ](https://www.cnblogs.com/zkweb/p/13354826.html)å·²ç»ä»‹ç»è¿‡ï¼Œä¸ºäº†æ”¯æŒé»‘ç™½å›¾ç‰‡ä¿®æ”¹äº† `conv1` ä½¿å¾—å…¥é€šé“æ•°å˜ä¸º 1ã€‚

å¦å¤–ä¸€ä¸ªç»†èŠ‚æ˜¯éƒ¨åˆ†å…³é”®ç‚¹çš„æ•°æ®æ˜¯ç¼ºæŸçš„ï¼Œä¾‹å¦‚åªæœ‰å·¦çœ¼å’Œå³çœ¼çš„åæ ‡ï¼Œä½†æ˜¯æ²¡æœ‰é¼»å­å’Œå˜´å·´çš„åæ ‡ï¼Œç¼ºæŸæ•°æ®åœ¨è¯»å–çš„æ—¶å€™ä¼šå˜ä¸º `nan`ï¼Œæ‰€ä»¥ä»£ç ä¸­è®¡ç®—æŸå¤±å’Œæ­£ç¡®ç‡çš„æ—¶å€™ä¼šæ’é™¤æ‰å€¼ä¸º `nan` çš„æ•°æ®ï¼Œè¿™æ ·å³ä½¿åŒä¸€æ•°æ®çš„éƒ¨åˆ†åæ ‡ç¼ºæŸæ¨¡å‹ä»ç„¶å¯ä»¥å­¦ä¹ æ²¡æœ‰ç¼ºæŸçš„åæ ‡ã€‚

æŠŠä»£ç ä¿å­˜åˆ° `face_landmark.py`ï¼Œç„¶åæŒ‰ä»¥ä¸‹çš„æ–‡ä»¶å¤¹ç»“æ„æ”¾ä»£ç å’Œæ•°æ®é›†ï¼š

- dataset
    - face-images-with-marked-landmark-points
        - face_images.npy
        - facial_keypoints.csv
- face_landmark.py

å†æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å¼€å§‹è®­ç»ƒï¼š

``` text
python3 face_landmark.py prepare
python3 face_landmark.py train
```

æœ€ç»ˆè®­ç»ƒç»“æœå¦‚ä¸‹ï¼š

``` text
epoch: 52, batch: 333: batch accuracy: 0.9883924638852477
epoch: 52, batch: 334: batch accuracy: 0.986928996630013
epoch: 52, batch: 335: batch accuracy: 0.9883735300973058
training accuracy: 0.9855450947513982
validating accuracy: 0.973902036840584
stop training because highest validating accuracy not updated in 20 epoches
highest validating accuracy: 0.976565406219812 from epoch 31
testing accuracy: 0.976561392748928
```

åæ ‡çš„åå·®å¤§çº¦æ˜¯ `(1 - 0.976565406219812) * 2` å³ç›¸å¯¹å›¾ç‰‡é•¿å®½çš„ 4.68% å·¦å³ğŸ¤”ã€‚

å†æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š

``` text
python3 face_landmark.py eval
```

ä»¥ä¸‹æ˜¯éƒ¨åˆ†è¯†åˆ«ç»“æœğŸ˜¤ï¼š

![face_landmark1](./face_landmark1.png)

![face_landmark2](./face_landmark2.png)

![face_landmark3](./face_landmark3.png)

æœ‰ä¸€å®šè¯¯å·®ï¼Œä½†æ˜¯ç”¨æ¥è°ƒæ•´è„¸éƒ¨èŒƒå›´æ˜¯è¶³å¤Ÿäº†ã€‚æ­¤å¤–ï¼Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ£€æµ‹å‡ºæ¥çš„é¼»å°–åæ ‡ä¼šç¨å¾®åä¸‹ï¼Œè¿™æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®ä¸­çš„å¤§éƒ¨åˆ†éƒ½æ˜¯é¼»å­æ¯”è¾ƒé«˜çš„ç™½äººğŸ˜¡ï¼Œæˆ‘ä»¬çœ‹åˆ°æ–°é—»é‡Œé¢è¯´äººè„¸è¯†åˆ«æ¨¡å‹å¯¹äºé»‘äººè¯†åˆ«ç‡å¾ˆä½ä¹Ÿæ˜¯å› ä¸ºæ ·æœ¬ä¸­ç»å¤§éƒ¨åˆ†éƒ½æ˜¯ç™½äººï¼Œæ•°æ®çš„åå‘ä¼šç›´æ¥å½±å“æ¨¡å‹çš„æ£€æµ‹ç»“æœğŸ¤’ã€‚

## å†™åœ¨æœ€å

è¿™ç¯‡æ¯”è¾ƒç®€å•ï¼Œä¸‹ä¸€ç¯‡å°†ä¼šä»‹ç»äººè„¸è¯†åˆ«æ¨¡å‹ï¼Œåˆ°æ—¶ä¼šåƒè¿™ç¯‡æœ€å¼€å§‹ç»™å‡ºçš„å›¾ç‰‡ä¸€æ ·ç»“åˆä¸‰ä¸ªæ¨¡å‹å®ç°ã€‚