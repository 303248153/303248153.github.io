# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (äºŒ) - pytorch ä¸çŸ©é˜µè®¡ç®—å…¥é—¨

## pytorch ç®€ä»‹

pytorch æ˜¯ç›®å‰ä¸–ç•Œä¸Šæœ€æµè¡Œçš„ä¸¤ä¸ªæœºå™¨å­¦ä¹ æ¡†æ¶çš„å…¶ä¸­ä¹‹ä¸€ï¼Œä¸ tensoflow å¹¶å³™åŒé›„ã€‚å®ƒæä¾›äº†å¾ˆå¤šæ–¹ä¾¿çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ ¹æ®æŸå¤±è‡ªåŠ¨å¾®åˆ†è®¡ç®—åº”è¯¥æ€æ ·è°ƒæ•´å‚æ•°ï¼Œæä¾›äº†ä¸€ç³»åˆ—çš„æ•°å­¦å‡½æ•°å°è£…ï¼Œè¿˜æä¾›äº†ä¸€ç³»åˆ—ç°æˆçš„æ¨¡å‹ï¼Œä»¥åŠæŠŠæ¨¡å‹ç»„åˆèµ·æ¥è¿›è¡Œè®­ç»ƒçš„æ¡†æ¶ã€‚pytorch çš„å‰èº«æ˜¯ torchï¼ŒåŸºäº luaï¼Œè€Œ pytorch åŸºäº pythonï¼Œè™½ç„¶å®ƒåŸºäº python ä½†åº•å±‚å®Œå…¨ç”± c++ ç¼–å†™ï¼Œæ”¯æŒè‡ªåŠ¨å¹¶åˆ—åŒ–è®¡ç®—å’Œä½¿ç”¨ GPU åŠ é€Ÿè¿ç®—ï¼Œæ‰€ä»¥å®ƒçš„æ€§èƒ½éå¸¸å¥½ã€‚

ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æœ‰çš„ä¼šåƒå‰ä¸€èŠ‚çš„ä¾‹å­ä¸­å…¨éƒ¨æ‰‹å†™ï¼Œæˆ–è€…åˆ©ç”¨ numpy ç±»åº“å‡å°‘ä¸€éƒ¨åˆ†å·¥ä½œé‡ï¼Œä¹Ÿæœ‰äººä¼šåˆ©ç”¨ scikit-learn (åŸºäº numpy) ç±»åº“å°è£…å¥½çš„å„ç§ç»å…¸ç®—æ³•ã€‚pytorch ä¸ tensorflow å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸ä¸€æ ·çš„æ˜¯ï¼Œå®ƒä»¬æŠŠé‡ç‚¹æ”¾åœ¨äº†ç»„å»ºç±»ä¼¼äººè„‘çš„ç¥ç»å…ƒç½‘ç»œ (Neural Network)ï¼Œæ‰€ä»¥èƒ½å®ç°ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ— æ³•åšåˆ°çš„éå¸¸å¤æ‚çš„åˆ¤æ–­ï¼Œä¾‹å¦‚åˆ¤æ–­å›¾ç‰‡ä¸­çš„ç‰©ä½“ç±»å‹ï¼Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚ä¸è¿‡ï¼Œå®ƒä»¬ç»„å»ºçš„ç¥ç»å…ƒç½‘ç»œå·¥ä½œæ–¹å¼æ˜¯ä¸æ˜¯çœŸçš„å’Œäººè„‘ç±»ä¼¼ä»ç„¶æœ‰å¾ˆå¤šäº‰è®®ï¼Œç›®å‰å·²ç»æœ‰äººå¼€å§‹ç€æ‰‹ç»„å»ºåŸç†ä¸Šæ›´æ¥è¿‘äººè„‘çš„ GNN (Graph Neural Network) ç½‘ç»œï¼Œä½†ä»æœªå®ç”¨åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™ä¸ªç³»åˆ—è¿˜æ˜¯ä¼šç€é‡è®²è§£å½“å‰å·²ç»å®ç”¨åŒ–å¹¶å¹¿æ³›åº”ç”¨åœ¨å„ä¸ªè¡Œä¸šçš„ç½‘ç»œæ¨¡å‹ã€‚

## å­¦ pytorch è¿˜æ˜¯å­¦ tensorflow å¥½ï¼Ÿ

å¯¹åˆå­¦è€…æ¥è¯´ä¸€ä¸ªå¾ˆå¸¸è§çš„é—®é¢˜æ˜¯ï¼Œå­¦ pytorch è¿˜æ˜¯å­¦ tensorflow å¥½ï¼ŸæŒ‰ç›®å‰çš„ç»Ÿè®¡æ•°æ®æ¥è¯´ï¼Œå…¬å¸æ›´å¤šä½¿ç”¨ tensorflowï¼Œè€Œç ”ç©¶äººå‘˜æ›´å¤šä½¿ç”¨ pytorchï¼Œpytorch çš„å¢é•¿é€Ÿåº¦éå¸¸å¿«ï¼Œæœ‰è¶…è¶Š tensorflow çš„è¶‹åŠ¿ã€‚æˆ‘çš„æ„è§æ˜¯å­¦å“ªä¸ªéƒ½æ— æ‰€è°“ï¼Œå¦‚æœä½ ç†Ÿæ‚‰ pytorchï¼Œå­¦ tensorflow ä¹Ÿå°±ä¸€ä¸¤å¤©çš„äº‹æƒ…ï¼Œåè¿‡æ¥ä¹Ÿä¸€æ ·ï¼Œå¹¶ä¸” pytorch å’Œ tensorflow çš„é¡¹ç›®å¯ä»¥äº’ç›¸ç§»æ¤ï¼Œé€‰ä¸€ä¸ªè§‰å¾—å¥½å­¦çš„å°±å¯ä»¥äº†ã€‚å› ä¸ºæˆ‘è§‰å¾— pytorch æ›´å¥½å­¦ (å°è£…éå¸¸ç›´è§‚ï¼Œä½¿ç”¨ Dynamic Graph ä½¿å¾—è°ƒè¯•éå¸¸å®¹æ˜“)ï¼Œæ‰€ä»¥è¿™ä¸ªç³»åˆ—ä¼šåŸºäº pytorch æ¥è®²ã€‚

### Dynamic Graph ä¸ Static Graph

æœºå™¨å­¦ä¹ æ¡†æ¶æŒ‰è¿ç®—çš„æµç¨‹æ˜¯å¦éœ€è¦é¢„å…ˆå›ºå®šå¯ä»¥åˆ†ä¸º Dynamic Graph å’Œ Static Graphï¼ŒDynamic Graph ä¸éœ€è¦é¢„å…ˆå›ºå®šè¿ç®—æµç¨‹ï¼Œè€Œ Static Graph éœ€è¦ã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå¯¹åŒä¸€ä¸ªå…¬å¼ `wx + b = y`ï¼ŒDynamic Graph å‹çš„æ¡†æ¶å¯ä»¥æŠŠ `wx`ï¼Œ`+b` åˆ†å¼€å†™å¹¶ä¸”é€æ­¥è®¡ç®—ï¼Œè®¡ç®—çš„è¿‡ç¨‹ä¸­éšæ—¶éƒ½å¯ä»¥ç”¨ `print` ç­‰æŒ‡ä»¤è¾“å‡ºé€”ä¸­çš„ç»“æœï¼Œæˆ–è€…æŠŠé€”ä¸­çš„ç»“æœå‘é€åˆ°å…¶ä»–åœ°æ–¹è®°å½•èµ·æ¥ï¼›è€Œ Static Graph å‹çš„æ¡†æ¶å¿…é¡»é¢„å…ˆå®šå¥½æ•´ä¸ªè®¡ç®—æµç¨‹ï¼Œä½ åªèƒ½ä¼ å…¥ `w`, `x`, `b` ç»™è®¡ç®—å™¨ï¼Œç„¶åè®©è®¡ç®—å™¨è¾“å‡º `y`ï¼Œä¸­é€”è®¡ç®—çš„ç»“æœåªèƒ½ä½¿ç”¨ä¸“é—¨çš„è°ƒè¯•å™¨æ¥æŸ¥çœ‹ã€‚

ä¸€èˆ¬çš„æ¥è¯´ Static Graph æ€§èƒ½ä¼šæ¯” Dynamic Graph å¥½ï¼ŒTensorflow (è€ç‰ˆæœ¬) ä½¿ç”¨çš„æ˜¯ Static Graphï¼Œè€Œ pytorch ä½¿ç”¨çš„æ˜¯ Dynamic Graphï¼Œä½†ä¸¤è€…å®é™…æ€§èƒ½ç›¸å·®å¾ˆå°ï¼Œå› ä¸ºæ¶ˆè€—èµ„æºçš„å¤§éƒ¨åˆ†éƒ½æ˜¯çŸ©é˜µè¿ç®—ï¼Œä½¿ç”¨æ‰¹æ¬¡è®­ç»ƒå¯ä»¥å¾ˆå¤§ç¨‹åº¦å‡å°‘å®ƒä»¬çš„å·®è·ã€‚é¡ºå¸¦ä¸€æï¼ŒTensorflow 1.7 å¼€å§‹æ”¯æŒäº† Dynamic Graphï¼Œå¹¶ä¸”åœ¨ 2.0 é»˜è®¤å¼€å¯ï¼Œä½†å¤§éƒ¨åˆ†äººåœ¨ä½¿ç”¨ Tensorflow çš„æ—¶å€™è¿˜æ˜¯ä¼šç”¨ Static Graphã€‚

``` python
# Dynamic Graph çš„å°è±¡ï¼Œè¿ç®—çš„æ¯ä¸€æ­¥éƒ½å¯ä»¥æ’å…¥è‡ªå®šä¹‰ä»£ç 
def forward(w, x, b):
    wx = w * x
    print(wx)
    y = wx + b
    print(y)
    return y
forward(w, x, b)

# Static Graph çš„å°è±¡ï¼Œéœ€è¦é¢„å…ˆç¼–è¯‘æ•´ä¸ªè®¡ç®—æµç¨‹
forward = compile("wx+b")
forward(w, x, b)
```

## å®‰è£… pytorch

å‡è®¾ä½ å·²ç»å®‰è£…äº† python3ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å®‰è£… pytorchï¼š

``` text
pip3 install pytorch
```

ä¹‹ååœ¨ python ä»£ç ä¸­ä½¿ç”¨ `import torch` å³å¯å¼•ç”¨ pytorch ç±»åº“ã€‚

## pytorch çš„åŸºæœ¬æ“ä½œ

æ¥ä¸‹æ¥æˆ‘ä»¬ç†Ÿæ‚‰ä¸€ä¸‹ pytorch é‡Œé¢æœ€åŸºæœ¬çš„æ“ä½œï¼Œpytorch ä¼šç”¨ `torch.Tensor` ç±»å‹æ¥ç»Ÿä¸€è¡¨ç°æ•°å€¼ï¼Œå‘é‡ (ä¸€ç»´æ•°ç»„) æˆ–çŸ©é˜µ (å¤šç»´æ•°ç»„)ï¼Œæ¨¡å‹çš„å‚æ•°ä¹Ÿä¼šä½¿ç”¨è¿™ä¸ªç±»å‹ã€‚(tensorflow ä¼šæ ¹æ®ç”¨é€”åˆ†ä¸ºå¥½å‡ ä¸ªç±»å‹ï¼Œè¿™ç‚¹ pytorch æ›´ç®€æ´æ˜äº†)

`torch.Tensor` ç±»å‹å¯ä»¥ä½¿ç”¨ `torch.tensor` å‡½æ•°æ„å»ºï¼Œä»¥ä¸‹æ˜¯ä¸€äº›ç®€å•çš„ä¾‹å­ï¼ˆè¿è¡Œåœ¨ python çš„ REPL ä¸­):

``` python
# å¼•ç”¨ pytorch
>>> import torch

# åˆ›å»ºä¸€ä¸ªæ•´æ•° tensor
>>> torch.tensor(1)
tensor(1)

# åˆ›å»ºä¸€ä¸ªå°æ•° tensor
>>> torch.tensor(1.0)
tensor(1.)

# å•å€¼ tensor ä¸­çš„å€¼å¯ä»¥ç”¨ item å‡½æ•°å–å‡º
>>> torch.tensor(1.0).item()
1.0

# ä½¿ç”¨ä¸€ç»´æ•°ç»„åˆ›å»ºä¸€ä¸ªå‘é‡ tensor
>>> torch.tensor([1.0, 2.0, 3.0])
tensor([1., 2., 3.])

# ä½¿ç”¨äºŒç»´æ•°ç»„åˆ›å»ºä¸€ä¸ªçŸ©é˜µ tensor
>>> torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0]])
tensor([[ 1.,  2.,  3.],
        [-1., -2., -3.]])
```

tensor å¯¹è±¡çš„æ•°å€¼ç±»å‹å¯ä»¥çœ‹å®ƒçš„ `dtype` æˆå‘˜ï¼š

``` python
>>> torch.tensor(1).dtype
torch.int64
>>> torch.tensor(1.0).dtype
torch.float32
>>> torch.tensor([1.0, 2.0, 3.0]).dtype
torch.float32
>>> torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0]]).dtype
torch.float32
```

pytorch æ”¯æŒæ•´æ•°ç±»å‹ `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64` ï¼Œæµ®ç‚¹æ•°ç±»å‹ `torch.float16`, `torch.float32`, `torch.float64`ï¼Œè¿˜æœ‰å¸ƒå°”å€¼ç±»å‹ `torch.bool`ã€‚ç±»å‹åçš„æ•°å­—ä»£è¡¨å®ƒçš„ä½æ•° (bit æ•°)ï¼Œè€Œ `uint8` å‰é¢çš„ `u` ä»£è¡¨å®ƒæ˜¯æ— ç¬¦å·æ•° (unsigned)ã€‚å®é™…ç»å¤§éƒ¨åˆ†åœºæ™¯éƒ½åªä¼šä½¿ç”¨ `torch.float32`ï¼Œè™½ç„¶ç²¾åº¦æ²¡æœ‰ `torch.float64` é«˜ä½†å®ƒå ç”¨å†…å­˜å°å¹¶ä¸”è¿ç®—é€Ÿåº¦å¿«ã€‚æ³¨æ„ä¸€ä¸ª tensor å¯¹è±¡é‡Œé¢åªèƒ½ä¿å­˜ä¸€ç§ç±»å‹çš„æ•°å€¼ï¼Œä¸èƒ½æ··åˆå­˜æ”¾ã€‚

åˆ›å»º tensor å¯¹è±¡æ—¶å¯ä»¥é€šè¿‡ `dtype` å‚æ•°å¼ºåˆ¶æŒ‡å®šç±»å‹ï¼š

``` python
>>> torch.tensor(1, dtype=torch.int32)
tensor(1, dtype=torch.int32)
>>> torch.tensor([1.1, 2.9, 3.5], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)

>>> torch.tensor(1, dtype=torch.int64)
tensor(1)

>>> torch.tensor(1, dtype=torch.float32)
tensor(1.)

>>> torch.tensor(1, dtype=torch.float64)
tensor(1., dtype=torch.float64)
>>> torch.tensor([1, 2, 3], dtype=torch.float64)
tensor([1., 2., 3.], dtype=torch.float64)

>>> torch.tensor([1, 2, 0], dtype=torch.bool)
tensor([ True,  True, False])
```

tensor å¯¹è±¡çš„å½¢çŠ¶å¯ä»¥çœ‹å®ƒçš„ `shape` æˆå‘˜ï¼š

``` python
# æ•´æ•° tensor çš„ shape ä¸ºç©º
>>> torch.tensor(1).shape
torch.Size([])
>>> torch.tensor(1.0).shape
torch.Size([])

# æ•°ç»„ tensor çš„ shape åªæœ‰ä¸€ä¸ªå€¼ï¼Œä»£è¡¨æ•°ç»„çš„é•¿åº¦
>>> torch.tensor([1.0]).shape
torch.Size([1])
>>> torch.tensor([1.0, 2.0, 3.0]).shape
torch.Size([3])

# çŸ©é˜µ tensor çš„ shape æ ¹æ®å®ƒçš„ç»´åº¦è€Œå®šï¼Œæ¯ä¸ªå€¼ä»£è¡¨å„ä¸ªç»´åº¦çš„å¤§å°ï¼Œè¿™ä¸ªä¾‹å­ä»£è¡¨çŸ©é˜µæœ‰ 2 è¡Œ 3 åˆ—
>>> torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0]]).shape
torch.Size([2, 3])
```

tensor å¯¹è±¡ä¸æ•°å€¼ï¼Œtensor å¯¹è±¡ä¸ tensor å¯¹è±¡ä¹‹é—´å¯ä»¥è¿›è¡Œè¿ç®—ï¼š

``` python
>>> torch.tensor(1.0) * 2
tensor(2.)
>>> torch.tensor(1.0) * torch.tensor(2.0)
tensor(2.)
>>> torch.tensor(3.0) * torch.tensor(2.0)
tensor(6.)
```

å‘é‡å’ŒçŸ©é˜µè¿˜å¯ä»¥æ‰¹é‡è¿›è¡Œè¿ç®—ï¼ˆå†…éƒ¨ä¼šå¹¶åˆ—åŒ–è¿ç®—ï¼‰ï¼š

``` python
# å‘é‡å’Œæ•°å€¼ä¹‹é—´çš„è¿ç®—
>>> torch.tensor([1.0, 2.0, 3.0])
tensor([1., 2., 3.])
>>> torch.tensor([1.0, 2.0, 3.0]) * 3
tensor([3., 6., 9.])
>>> torch.tensor([1.0, 2.0, 3.0]) * 3 - 1
tensor([2., 5., 8.])

# çŸ©é˜µå’Œå•å€¼ tensor å¯¹è±¡ä¹‹é—´çš„è¿ç®—
>>> torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0]])
tensor([[ 1.,  2.,  3.],
        [-1., -2., -3.]])
>>> torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0]]) / torch.tensor(2)
tensor([[ 0.5000,  1.0000,  1.5000],
        [-0.5000, -1.0000, -1.5000]])

# çŸ©é˜µå’Œä¸çŸ©é˜µæœ€åä¸€ä¸ªç»´åº¦ç›¸åŒé•¿åº¦å‘é‡ä¹‹é—´çš„è¿ç®—
>>> torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0]]) * torch.tensor([1.0, 1.5, 2.0])
tensor([[ 1.,  3.,  6.],
        [-1., -3., -6.]])
```

tensor å¯¹è±¡ä¹‹é—´çš„è¿ç®—ä¸€èˆ¬éƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ tensor å¯¹è±¡ï¼Œå¦‚æœä½ æƒ³é¿å…ç”Ÿæˆæ–°å¯¹è±¡ (æé«˜æ€§èƒ½)ï¼Œå¯ä»¥ä½¿ç”¨ `_` ç»“å°¾çš„å‡½æ•°ï¼Œå®ƒä»¬ä¼šä¿®æ”¹åŸæœ‰çš„å¯¹è±¡ï¼š

``` python
# ç”Ÿæˆæ–°å¯¹è±¡ï¼ŒåŸæœ‰å¯¹è±¡ä¸å˜ï¼Œadd å’Œ + æ„ä¹‰ç›¸åŒ
>>> a = torch.tensor([1,2,3])
>>> b = torch.tensor([7,8,9])
>>> a.add(b)
tensor([ 8, 10, 12])
>>> a
tensor([1, 2, 3])

# åœ¨åŸæœ‰å¯¹è±¡ä¸Šæ‰§è¡Œæ“ä½œï¼Œé¿å…ç”Ÿæˆæ–°å¯¹è±¡
>>> a.add_(b)
tensor([ 8, 10, 12])
>>> a
tensor([ 8, 10, 12])
```

pytorch è¿˜æä¾›äº†ä¸€ç³»åˆ—æ–¹ä¾¿çš„å‡½æ•°æ±‚æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼ï¼Œæ ‡å‡†å·®ç­‰:

``` python
>>> torch.tensor([1.0, 2.0, 3.0])
tensor([1., 2., 3.])
>>> torch.tensor([1.0, 2.0, 3.0]).min()
tensor(1.)
>>> torch.tensor([1.0, 2.0, 3.0]).max()
tensor(3.)
>>> torch.tensor([1.0, 2.0, 3.0]).mean()
tensor(2.)
>>> torch.tensor([1.0, 2.0, 3.0]).std()
tensor(1.)
```

pytorch è¿˜æ”¯æŒæ¯”è¾ƒ tensor å¯¹è±¡æ¥ç”Ÿæˆå¸ƒå°”å€¼ç±»å‹çš„ tensor:

``` python
# tensor å¯¹è±¡ä¸æ•°å€¼æ¯”è¾ƒ
>>> torch.tensor([1.0, 2.0, 3.0]) > 1.0
tensor([False,  True,  True])
>>> torch.tensor([1.0, 2.0, 3.0]) <= 2.0
tensor([ True,  True, False])

# tensor å¯¹è±¡ä¸ tensor å¯¹è±¡æ¯”è¾ƒ
>>> torch.tensor([1.0, 2.0, 3.0]) > torch.tensor([1.1, 1.9, 3.0])
tensor([False,  True, False])
>>> torch.tensor([1.0, 2.0, 3.0]) <= torch.tensor([1.1, 1.9, 3.0])
tensor([ True, False,  True])
```

pytorch è¿˜æ”¯æŒç”ŸæˆæŒ‡å®šå½¢çŠ¶çš„ tensor å¯¹è±¡ï¼š

``` python
# ç”Ÿæˆ 2 è¡Œ 3 åˆ—çš„çŸ©é˜µ tensorï¼Œå€¼å…¨éƒ¨ä¸º 0
>>> torch.zeros(2, 3)
tensor([[0., 0., 0.],
        [0., 0., 0.]])

# ç”Ÿæˆ 3 è¡Œ 2 åˆ—çš„çŸ©é˜µ tensorï¼Œå€¼å…¨éƒ¨ä¸º 1
torch.ones(3, 2)
>>> torch.ones(2, 3)
tensor([[1., 1., 1.],
        [1., 1., 1.]])

# ç”Ÿæˆ 3 è¡Œ 2 åˆ—çš„çŸ©é˜µ tensorï¼Œå€¼å…¨éƒ¨ä¸º 100
>>> torch.full((3, 2), 100)
tensor([[100., 100.],
        [100., 100.],
        [100., 100.]])

# ç”Ÿæˆ 3 è¡Œ 3 åˆ—çš„çŸ©é˜µ tensorï¼Œå€¼ä¸ºèŒƒå›´ [0, 1) çš„éšæœºæµ®ç‚¹æ•°
>>> torch.rand(3, 3)
tensor([[0.4012, 0.2412, 0.1532],
        [0.1178, 0.2319, 0.4056],
        [0.7879, 0.8318, 0.7452]])

# ç”Ÿæˆ 3 è¡Œ 3 åˆ—çš„çŸ©é˜µ tensorï¼Œå€¼ä¸ºèŒƒå›´ [1, 10] çš„éšæœºæ•´æ•°
>>> (torch.rand(3, 3) * 10 + 1).long()
tensor([[ 8,  1,  5],
        [ 8,  6,  5],
        [ 1,  6, 10]])

# å’Œä¸Šé¢çš„å†™æ³•æ•ˆæœä¸€æ ·
>>> torch.randint(1, 11, (3, 3))
tensor([[7, 1, 3],
        [7, 9, 8],
        [4, 7, 3]])
```

è¿™é‡Œæåˆ°çš„æ“ä½œåªæ˜¯å¸¸ç”¨çš„ä¸€éƒ¨åˆ†ï¼Œå¦‚æœä½ æƒ³äº†è§£æ›´å¤š tensor å¯¹è±¡æ”¯æŒçš„æ“ä½œï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡æ¡£ï¼š

- https://pytorch.org/docs/stable/tensors.html

## pytorch ä¿å­˜ tensor ä½¿ç”¨çš„æ•°æ®ç»“æ„

ä¸ºäº†å‡å°‘å†…å­˜å ç”¨ä¸æå‡è®¿é—®é€Ÿåº¦ï¼Œpytorch ä¼šä½¿ç”¨ä¸€å—è¿ç»­çš„å‚¨å­˜ç©ºé—´ (ä¸ç®¡æ˜¯åœ¨ç³»ç»Ÿå†…å­˜è¿˜æ˜¯åœ¨ GPU å†…å­˜ä¸­) ä¿å­˜ tensorï¼Œä¸ç®¡ tensor æ˜¯æ•°å€¼ï¼Œå‘é‡è¿˜æ˜¯çŸ©é˜µã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `storage` æŸ¥çœ‹ tensor å¯¹è±¡ä½¿ç”¨çš„å‚¨å­˜ç©ºé—´ï¼š

``` python
# æ•°å€¼çš„å‚¨å­˜ç©ºé—´é•¿åº¦æ˜¯ 1
>>> torch.tensor(1).storage()
 1
[torch.LongStorage of size 1]

# å‘é‡çš„å‚¨å­˜ç©ºé—´é•¿åº¦ç­‰äºå‘é‡çš„é•¿åº¦
>>> torch.tensor([1, 2, 3], dtype=torch.float32).storage()
 1.0
 2.0
 3.0
[torch.FloatStorage of size 3]

# çŸ©é˜µçš„å‚¨å­˜ç©ºé—´é•¿åº¦ç­‰äºæ‰€æœ‰ç»´åº¦ç›¸ä¹˜çš„ç»“æœï¼Œè¿™é‡Œæ˜¯ 2 è¡Œ 3 åˆ—æ€»å…± 6 ä¸ªå…ƒç´ 
>>> torch.tensor([[1, 2, 3], [-1, -2, -3]], dtype=torch.float64).storage()
 1.0
 2.0
 3.0
 -1.0
 -2.0
 -3.0
[torch.DoubleStorage of size 6]
```

pytorch ä¼šä½¿ç”¨ `stride` æ¥ç¡®å®šä¸€ä¸ª tensor å¯¹è±¡çš„ç»´åº¦ï¼š

``` python
# å‚¨å­˜ç©ºé—´æœ‰ 6 ä¸ªå…ƒç´ 
>>> torch.tensor([[1, 2, 3], [-1, -2, -3]]).storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]

# ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ 2ï¼Œç¬¬äºŒä¸ªç»´åº¦æ˜¯ 3 (2 è¡Œ 3 åˆ—)
>>> torch.tensor([[1, 2, 3], [-1, -2, -3]]).shape
torch.Size([2, 3])

# stride çš„æ„ä¹‰æ˜¯è¡¨ç¤ºæ¯ä¸ªç»´åº¦ä¹‹é—´å…ƒç´ çš„è·ç¦»
# ç¬¬ä¸€ä¸ªç»´åº¦ä¼šæŒ‰ 3 ä¸ªå…ƒç´ æ¥åˆ‡åˆ† (6 ä¸ªå…ƒç´ å¯ä»¥åˆ‡åˆ†æˆ 2 ç»„)ï¼Œç¬¬äºŒä¸ªç»´åº¦ä¼šæŒ‰ 1 ä¸ªå…ƒç´ æ¥åˆ‡åˆ† (3 ä¸ªå…ƒç´ )
>>> torch.tensor([[1, 2, 3], [-1, -2, -3]])
tensor([[ 1,  2,  3],
        [-1, -2, -3]])
>>> torch.tensor([[1, 2, 3], [-1, -2, -3]]).stride()
(3, 1)
```

pytorch çš„ä¸€ä¸ªå¾ˆå¼ºå¤§çš„åœ°æ–¹æ˜¯ï¼Œé€šè¿‡ `view` å‡½æ•°å¯ä»¥ä¿®æ”¹ tensor å¯¹è±¡çš„ç»´åº¦ (å†…éƒ¨æ”¹å˜äº† `stride`)ï¼Œä½†æ˜¯ä¸éœ€è¦åˆ›å»ºæ–°çš„å‚¨å­˜ç©ºé—´å¹¶å¤åˆ¶å…ƒç´ ï¼š

``` python
# åˆ›å»ºä¸€ä¸ª 2 è¡Œ 3 åˆ—çš„çŸ©é˜µ
>>> a = torch.tensor([[1, 2, 3], [-1, -2, -3]])
>>> a
tensor([[ 1,  2,  3],
        [-1, -2, -3]])
>>> a.shape
torch.Size([2, 3])
>>> a.stride()
(3, 1)

# æŠŠç»´åº¦æ”¹ä¸º 3 è¡Œ 2 åˆ—
>>> b = a.view(3, 2)
>>> b
tensor([[ 1,  2],
        [ 3, -1],
        [-2, -3]])
>>> b.shape
torch.Size([3, 2])
>>> b.stride()
(2, 1)

# è½¬æ¢ä¸ºå‘é‡
>>> c = b.view(6)
>>> c
tensor([ 1,  2,  3, -1, -2, -3])
>>> c.shape
torch.Size([6])
>>> c.stride()
(1,)

# å®ƒä»¬çš„å‚¨å­˜ç©ºé—´æ˜¯ä¸€æ ·çš„
>>> a.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]
>>> b.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]
>>> c.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]
```

ä½¿ç”¨ `stride` ç¡®å®šç»´åº¦çš„å¦ä¸€ä¸ªæ„ä¹‰æ˜¯å®ƒå¯ä»¥æ”¯æŒå…±ç”¨åŒä¸€ä¸ªç©ºé—´å®ç°è½¬ç½® (Transpose) æ“ä½œ:

``` python
# åˆ›å»ºä¸€ä¸ª 2 è¡Œ 3 åˆ—çš„çŸ©é˜µ
>>> a = torch.tensor([[1, 2, 3], [-1, -2, -3]])
>>> a
tensor([[ 1,  2,  3],
        [-1, -2, -3]])
>>> a.shape
torch.Size([2, 3])
>>> a.stride()
(3, 1)

# ä½¿ç”¨è½¬ç½®æ“ä½œäº¤æ¢ç»´åº¦ (è¡Œè½¬åˆ—)
>>> b = a.transpose(0, 1)
>>> b
tensor([[ 1, -1],
        [ 2, -2],
        [ 3, -3]])
>>> b.shape
torch.Size([3, 2])
>>> b.stride()
(1, 3)

# å®ƒä»¬çš„å‚¨å­˜ç©ºé—´æ˜¯ä¸€æ ·çš„
>>> a.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]
>>> b.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]
```

è½¬ç½®æ“ä½œå†…éƒ¨å°±æ˜¯äº¤æ¢äº†æŒ‡å®šç»´åº¦åœ¨ `stride` ä¸­å¯¹åº”çš„å€¼ï¼Œä½ å¯ä»¥æ ¹æ®å‰é¢çš„æè¿°æƒ³æƒ³å¯¹è±¡åœ¨è½¬ç½®åçš„çŸ©é˜µä¸­ä¼šå¦‚ä½•åˆ’åˆ†ã€‚

ç°åœ¨å†æƒ³æƒ³ï¼Œå¦‚æœæŠŠè½¬ç½®åçš„çŸ©é˜µç”¨ `view` å‡½æ•°ä¸“ä¸ºå‘é‡ä¼šå˜ä¸ºä»€ä¹ˆï¼Ÿä¼šå˜ä¸º `[1, -1, 2, -2, 3, -3]` å—ï¼Ÿ

å®é™…ä¸Šè¿™æ ·çš„æ“ä½œä¼šå¯¼è‡´å‡ºé”™ğŸ˜±ï¼š

``` python
>>> b
tensor([[ 1, -1],
        [ 2, -2],
        [ 3, -3]])
>>> b.view(6)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
```

è¿™æ˜¯å› ä¸ºè½¬ç½®åçŸ©é˜µå…ƒç´ çš„è‡ªç„¶é¡ºåºå’Œå‚¨å­˜ç©ºé—´ä¸­çš„é¡ºåºä¸ä¸€è‡´ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ `is_contiguous` å‡½æ•°æ¥æ£€æµ‹ï¼š

``` python
>>> a.is_contiguous()
True
>>> b.is_contiguous()
False
```

è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯é¦–å…ˆç”¨ `contiguous` å‡½æ•°æŠŠå‚¨å­˜ç©ºé—´å¦å¤–å¤åˆ¶ä¸€ä»½ä½¿å¾—é¡ºåºä¸€è‡´ï¼Œç„¶åå†ç”¨ `view` å‡½æ•°æ”¹å˜ç»´åº¦ï¼›æˆ–è€…ç”¨æ›´æ–¹ä¾¿çš„ `reshape` å‡½æ•°ï¼Œ`reshape` å‡½æ•°ä¼šæ£€æµ‹æ”¹å˜ç»´åº¦çš„æ—¶å€™æ˜¯å¦éœ€è¦å¤åˆ¶å‚¨å­˜ç©ºé—´ï¼Œå¦‚æœéœ€è¦åˆ™å¤åˆ¶ï¼Œä¸éœ€è¦åˆ™å’Œ `view` ä¸€æ ·åªä¿®æ”¹å†…éƒ¨çš„ `stride`ã€‚

``` python
>>> b.contiguous().view(6)
tensor([ 1, -1,  2, -2,  3, -3])
>>> b.reshape(6)
tensor([ 1, -1,  2, -2,  3, -3])
```

pytorch è¿˜æ”¯æŒæˆªå–å‚¨å­˜ç©ºé—´çš„ä¸€éƒ¨åˆ†æ¥ä½œä¸ºä¸€ä¸ªæ–°çš„ tensor å¯¹è±¡ï¼ŒåŸºäºå†…éƒ¨çš„ `storage_offset` ä¸ `size` å±æ€§ï¼ŒåŒæ ·ä¸éœ€è¦å¤åˆ¶ï¼š

``` python
# æˆªå–å‘é‡çš„ä¾‹å­
>>> a = torch.tensor([1, 2, 3, -1, -2, -3])
>>> b = a[1:3]
>>> b
tensor([2, 3])
>>> b.storage_offset()
1
>>> b.size()
torch.Size([2])
>>> b.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]

# æˆªå–çŸ©é˜µçš„ä¾‹å­
>>> a.view(3, 2)
tensor([[ 1,  2],
        [ 3, -1],
        [-2, -3]])
>>> c = a.view(3, 2)[1:] # ç¬¬ä¸€ç»´åº¦ (è¡Œ) æˆªå– 1~ç»“å°¾, ç¬¬äºŒç»´åº¦ä¸æˆªå–
>>> c
tensor([[ 3, -1],
        [-2, -3]])
>>> c.storage_offset()
2
>>> c.size()
torch.Size([2, 2])
>>> c.stride()
(2, 1)
>>> c.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]

# æˆªå–è½¬ç½®åçŸ©é˜µçš„ä¾‹å­ï¼Œæ›´å¤æ‚ä¸€äº›
>>> a.view(3, 2).transpose(0, 1)
tensor([[ 1,  3, -2],
        [ 2, -1, -3]])
>>> c = a.view(3, 2).transpose(0, 1)[:,1:] # ç¬¬ä¸€ç»´åº¦ (è¡Œ) ä¸æˆªå–ï¼Œç¬¬äºŒç»´åº¦ (åˆ—) æˆªå– 1~ç»“å°¾
>>> c
tensor([[ 3, -2],
        [-1, -3]])
>>> c.storage_offset()
2
>>> c.size()
torch.Size([2, 2])
>>> c.stride()
(1, 2)
>>> c.storage()
 1
 2
 3
 -1
 -2
 -3
[torch.LongStorage of size 6]
```

å¥½äº†ï¼Œçœ‹å®Œè¿™ä¸€èŠ‚ä½ åº”è¯¥å¯¹ pytorch å¦‚ä½•å‚¨å­˜ tensor å¯¹è±¡æœ‰ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„äº†è§£ã€‚ä¸ºäº†å®¹æ˜“ç†è§£æœ¬èŠ‚æœ€å¤šåªä½¿ç”¨äºŒç»´çŸ©é˜µåšä¾‹å­ï¼Œä½ å¯ä»¥è‡ªå·±è¯•è¯•æ›´å¤šç»´åº¦çš„çŸ©é˜µæ˜¯å¦å¯ä»¥ç”¨åŒæ ·çš„æ–¹å¼æ“ä½œã€‚

## çŸ©é˜µä¹˜æ³•ç®€ä»‹

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µä¹˜æ³• (Matrix Multiplication)ï¼Œè¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€æœ€æœ€é¢‘ç¹çš„æ“ä½œï¼Œé«˜ä¸­å­¦è¿‡å¹¶ä¸”è¿˜è®°å¾—çš„å°±å½“å¤ä¹ ä¸€ä¸‹å§ï¼Œ

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä¸€ä¸ª 2 è¡Œ 3 åˆ—çš„çŸ©é˜µä¹˜ä»¥ä¸€ä¸ª 3 è¡Œ 4 åˆ—çš„çŸ©é˜µå¯ä»¥å¾—å‡ºä¸€ä¸ª 2 è¡Œ 4 åˆ—çš„çŸ©é˜µï¼š

![01.png](./01.png)

çŸ©é˜µä¹˜æ³•ä¼šæŠŠç¬¬ä¸€ä¸ªçŸ©é˜µçš„æ¯ä¸€è¡Œä¸ç¬¬äºŒä¸ªçŸ©é˜µçš„æ¯ä¸€åˆ—ç›¸ä¹˜çš„å„ä¸ªåˆè®¡å€¼ä½œä¸ºç»“æœï¼Œå¯ä»¥å‚è€ƒä¸‹å›¾ç†è§£ï¼š

![02.png](./02.png)

æŒ‰è¿™ä¸ªè§„åˆ™æ¥ç®—ï¼Œä¸€ä¸ª n è¡Œ m åˆ—çš„çŸ©é˜µå’Œä¸€ä¸ª m è¡Œ p åˆ—çš„çŸ©é˜µç›¸ä¹˜ï¼Œä¼šå¾—å‡ºä¸€ä¸ª n è¡Œ p åˆ—çš„çŸ©é˜µ (ç¬¬ä¸€ä¸ªçŸ©é˜µçš„åˆ—æ•°ä¸ç¬¬äºŒä¸ªçŸ©é˜µçš„è¡Œæ•°å¿…é¡»ç›¸åŒ)ã€‚

é‚£çŸ©é˜µä¹˜æ³•æœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼ŸçŸ©é˜µä¹˜æ³•åœ¨æœºå™¨å­¦ä¹ ä¸­çš„æ„ä¹‰æ˜¯å¯ä»¥æŠŠå¯¹å¤šä¸ªè¾“å…¥è¾“å‡ºæˆ–è€…ä¸­é—´å€¼çš„è®¡ç®—åˆå¹¶åˆ°ä¸€ä¸ªæ“ä½œä¸­ (åœ¨æ•°å­¦ä¸Šä¹Ÿå¯ä»¥å¤§å¹…ç®€åŒ–å…¬å¼)ï¼Œæ¡†æ¶å¯ä»¥åœ¨å†…éƒ¨å¹¶åˆ—åŒ–è®¡ç®—ï¼Œå› ä¸ºé«˜ç«¯çš„ GPU æœ‰å‡ åƒä¸ªæ ¸å¿ƒï¼ŒæŠŠè®¡ç®—åˆ†å¸ƒåˆ°å‡ åƒä¸ªæ ¸å¿ƒä¸­å¯ä»¥å¤§å¹…æå‡è¿ç®—é€Ÿåº¦ã€‚åœ¨æ¥ä¸‹æ¥çš„ä¾‹å­ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°å¦‚ä½•ç”¨çŸ©é˜µä¹˜æ³•å®ç°æ‰¹æ¬¡è®­ç»ƒã€‚

## ä½¿ç”¨ pytorch è¿›è¡ŒçŸ©é˜µä¹˜æ³•è®¡ç®—

åœ¨ pytorch ä¸­çŸ©é˜µä¹˜æ³•å¯ä»¥è°ƒç”¨ `mm` å‡½æ•°ï¼š

``` python
>>> a = torch.tensor([[1,2,3],[4,5,6]])
>>> b = torch.tensor([[4,3,2,1],[8,7,6,5],[9,9,9,9]])
>>> a.mm(b)
tensor([[ 47,  44,  41,  38],
        [110, 101,  92,  83]])

# å¦‚æœå¤§å°ä¸åŒ¹é…ä¼šå‡ºé”™
>>> a = torch.tensor([[1,2,3],[4,5,6]])
>>> b = torch.tensor([[4,3,2,1],[8,7,6,5]])
>>> a.mm(b)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: size mismatch, m1: [2 x 3], m2: [2 x 4] at ../aten/src/TH/generic/THTensorMath.cpp:197

# mm å‡½æ•°ä¹Ÿå¯ä»¥ç”¨ @ æ“ä½œç¬¦ä»£æ›¿ï¼Œç»“æœæ˜¯ä¸€æ ·çš„
>>> a = torch.tensor([[1,2,3],[4,5,6]])
>>> b = torch.tensor([[4,3,2,1],[8,7,6,5],[9,9,9,9]])
>>> a @ b
tensor([[ 47,  44,  41,  38],
        [110, 101,  92,  83]])
```

é’ˆå¯¹æ›´å¤šç»´åº¦çš„çŸ©é˜µä¹˜æ³•ï¼Œpytorch æä¾›äº† `matmul` å‡½æ•°ï¼š

``` python
# n x m çš„çŸ©é˜µä¸ q x m x p çš„çŸ©é˜µç›¸ä¹˜ä¼šå¾—å‡º q x n x p çš„çŸ©é˜µ
>>> a = torch.ones(2,3)
>>> b = torch.ones(5,3,4)
>>> a.matmul(b)
tensor([[[3., 3., 3., 3.],
         [3., 3., 3., 3.]],

        [[3., 3., 3., 3.],
         [3., 3., 3., 3.]],

        [[3., 3., 3., 3.],
         [3., 3., 3., 3.]],

        [[3., 3., 3., 3.],
         [3., 3., 3., 3.]],

        [[3., 3., 3., 3.],
         [3., 3., 3., 3.]]])
>>> a.matmul(b).shape
torch.Size([5, 2, 4])
```

## pytorch çš„è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ (autograd)

pytorch æ”¯æŒè‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼ (å³å„ä¸ªå‚æ•°çš„æ¢¯åº¦)ï¼Œåˆ©ç”¨è¿™ä¸ªåŠŸèƒ½æˆ‘ä»¬ä¸å†éœ€è¦é€šè¿‡æ•°å­¦å…¬å¼æ±‚å„ä¸ªå‚æ•°çš„å¯¼å‡½æ•°å€¼ï¼Œä½¿å¾—æœºå™¨å­¦ä¹ çš„é—¨æ§›ä½äº†å¾ˆå¤šğŸ˜„ğŸ˜„ï¼Œä»¥ä¸‹æ˜¯è¿™ä¸ªåŠŸèƒ½çš„ä¾‹å­ï¼š

``` python
# å®šä¹‰å‚æ•°
# åˆ›å»º tensor å¯¹è±¡æ—¶è®¾ç½® requires_grad ä¸º True å³å¯å¼€å¯è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
>>> w = torch.tensor(1.0, requires_grad=True)
>>> b = torch.tensor(0.0, requires_grad=True)

# å®šä¹‰è¾“å…¥å’Œè¾“å‡ºçš„ tensor
>>> x = torch.tensor(2)
>>> y = torch.tensor(5)

# è®¡ç®—é¢„æµ‹è¾“å‡º
>>> p = x * w + b
>>> p
tensor(2., grad_fn=<AddBackward0>)

# è®¡ç®—æŸå¤±
# æ³¨æ„ pytorch çš„è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½è¦æ±‚æŸå¤±ä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå› ä¸º pytorch åªä¼šè€ƒè™‘å‡å°‘æŸå¤±è€Œä¸æ˜¯è®©æŸå¤±æ¥è¿‘ 0
# è¿™é‡Œç”¨ abs è®©æŸå¤±å˜ä¸ºç»å¯¹å€¼
>>> l = (p - y).abs()
>>> l
tensor(3., grad_fn=<AbsBackward>)

# ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
>>> l.backward()

# æŸ¥çœ‹å„ä¸ªå‚æ•°å¯¹åº”çš„å¯¼å‡½æ•°å€¼
# æ³¨æ„ pytorch ä¼šå‡è®¾è®©å‚æ•°å‡å» grad çš„å€¼æ‰èƒ½å‡å°‘æŸå¤±ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯è´Ÿæ•°ï¼ˆå‚æ•°ä¼šå˜å¤§ï¼‰
>>> w.grad
tensor(-2.)
>>> b.grad
tensor(-1.)

# å®šä¹‰å­¦ä¹ æ¯”ç‡ï¼Œå³æ¯æ¬¡æ ¹æ®å¯¼å‡½æ•°å€¼è°ƒæ•´å‚æ•°çš„æ¯”ç‡
>>> learning_rate = 0.01

# è°ƒæ•´å‚æ•°æ—¶éœ€è¦ç”¨ torch.no_grad æ¥ä¸´æ—¶ç¦æ­¢è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
>>> with torch.no_grad():
...     w -= w.grad * learning_rate
...     b -= b.grad * learning_rate
...

# æˆ‘ä»¬å¯ä»¥çœ‹åˆ° weight å’Œ bias åˆ†åˆ«å¢åŠ äº† 0.02 å’Œ 0.01
>>> w
tensor(1.0200, requires_grad=True)
>>> b
tensor(0.0100, requires_grad=True)

# æœ€åæˆ‘ä»¬éœ€è¦æ¸…ç©ºå‚æ•°çš„ grad å€¼ï¼Œè¿™ä¸ªå€¼ä¸ä¼šè‡ªåŠ¨æ¸…é›¶ï¼ˆå› ä¸ºæŸäº›æ¨¡å‹éœ€è¦å åŠ å¯¼å‡½æ•°å€¼ï¼‰
# ä½ å¯ä»¥è¯•è¯•å†è°ƒä¸€æ¬¡ backwardï¼Œä¼šå‘ç° grad æŠŠä¸¤æ¬¡çš„å€¼å åŠ èµ·æ¥
>>> w.grad.zero_()
>>> b.grad.zero_()
```

æˆ‘ä»¬å†æ¥è¯•è¯•å‰ä¸€èŠ‚æåˆ°çš„è®©æŸå¤±ç­‰äºç›¸å·®å€¼å¹³æ–¹çš„æ–¹æ³•ï¼š

``` python
# å®šä¹‰å‚æ•°
>>> w = torch.tensor(1.0, requires_grad=True)
>>> b = torch.tensor(0.0, requires_grad=True)

# å®šä¹‰è¾“å…¥å’Œè¾“å‡ºçš„ tensor
>>> x = torch.tensor(2)
>>> y = torch.tensor(5)

# è®¡ç®—é¢„æµ‹è¾“å‡º
>>> p = x * w + b
>>> p
tensor(2., grad_fn=<AddBackward0>)

# è®¡ç®—ç›¸å·®å€¼
>>> d = p - y
>>> d
tensor(-3., grad_fn=<SubBackward0>)

# è®¡ç®—æŸå¤± (ç›¸å·®å€¼çš„å¹³æ–¹, ä¸€å®šä¼šæ˜¯ 0 æˆ–è€…æ­£æ•°)
>>> l = d ** 2
>>> l
tensor(9., grad_fn=<PowBackward0>)

# ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
>>> l.backward()

# æŸ¥çœ‹å„ä¸ªå‚æ•°å¯¹åº”çš„å¯¼å‡½æ•°å€¼ï¼Œè·Ÿæˆ‘ä»¬ä¸Šä¸€ç¯‡ç”¨æ•°å­¦å…¬å¼æ±‚å‡ºæ¥çš„å€¼ä¸€æ ·å§
# w çš„å¯¼å‡½æ•°å€¼ = 2 * d * x = 2 * -3 * 2 = -12
# b çš„å¯¼å‡½æ•°å€¼ = 2 * d = 2 * -3 = -6
>>> w.grad
tensor(-12.)
>>> b.grad
tensor(-6.)

# ä¹‹åå’Œä¸Šä¸€ä¸ªä¾‹å­ä¸€æ ·è°ƒæ•´å‚æ•°å³å¯
```

è…»å®³å­ğŸ˜¼ï¼Œå†å¤æ‚çš„æ¨¡å‹åªè¦è°ƒç”¨ `backward` éƒ½å¯ä»¥è‡ªåŠ¨å¸®æˆ‘ä»¬è®¡ç®—å‡ºå¯¼å‡½æ•°å€¼ï¼Œä»ç°åœ¨å¼€å§‹æˆ‘ä»¬å¯ä»¥æŠŠæ•°å­¦è¯¾æœ¬ä¸¢æ‰äº† (è¿™æ˜¯å¼€ç©ç¬‘çš„ï¼Œä¸€äº›é—®é¢˜ä»ç„¶éœ€è¦ç”¨æ•°å­¦æ¥ç†è§£ï¼Œä½†å¤§éƒ¨åˆ†æƒ…å†µä¸‹åªæœ‰åŸºç¡€æ•°å­¦çŸ¥è¯†çš„äººä¹Ÿèƒ½ç©å¾—èµ·)ã€‚

## pytorch çš„æŸå¤±è®¡ç®—å™¨å°è£… (loss function)

pytorch æä¾›äº†å‡ ç§å¸¸è§çš„æŸå¤±è®¡ç®—å™¨çš„å°è£…ï¼Œæˆ‘ä»¬æœ€å¼€å§‹çœ‹åˆ°çš„ä¹Ÿç§° L1 æŸå¤± (L1 Loss)ï¼Œè¡¨ç¤ºæ‰€æœ‰é¢„æµ‹è¾“å‡ºä¸æ­£ç¡®è¾“å‡ºçš„ç›¸å·®çš„ç»å¯¹å€¼çš„å¹³å‡ (æœ‰çš„åœºæ™¯ä¼šæœ‰å¤šä¸ªè¾“å‡º)ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨ L1 æŸå¤±çš„ä¾‹å­ï¼š

``` python
# å®šä¹‰å‚æ•°
>>> w = torch.tensor(1.0, requires_grad=True)
>>> b = torch.tensor(0.0, requires_grad=True)

# å®šä¹‰è¾“å…¥å’Œè¾“å‡ºçš„ tensor
# æ³¨æ„ pytorch æä¾›çš„æŸå¤±è®¡ç®—å™¨è¦æ±‚é¢„æµ‹è¾“å‡ºå’Œæ­£ç¡®è¾“å‡ºå‡ä¸ºæµ®ç‚¹æ•°ï¼Œæ‰€ä»¥å®šä¹‰è¾“å…¥ä¸è¾“å‡ºçš„æ—¶å€™ä¹Ÿéœ€è¦ç”¨æµ®ç‚¹æ•°
>>> x = torch.tensor(2.0)
>>> y = torch.tensor(5.0)

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
>>> loss_function = torch.nn.L1Loss()

# è®¡ç®—é¢„æµ‹è¾“å‡º
>>> p = x * w + b
>>> p
tensor(2., grad_fn=<AddBackward0>)

# è®¡ç®—æŸå¤±
# ç­‰åŒäº (p - y).abs().mean()
>>> l = loss_function(p, y)
>>> l
tensor(3., grad_fn=<L1LossBackward>)
```

è€Œè®¡ç®—ç›¸å·®å€¼çš„å¹³æ–¹ä½œä¸ºæŸå¤±ç§°ä¸º MSE æŸå¤± (Mean Squared Error)ï¼Œæœ‰çš„åœ°æ–¹åˆç§° L2 æŸå¤±ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨ MSE æŸå¤±çš„ä¾‹å­ï¼š

``` python
# å®šä¹‰å‚æ•°
>>> w = torch.tensor(1.0, requires_grad=True)
>>> b = torch.tensor(0.0, requires_grad=True)

# å®šä¹‰è¾“å…¥å’Œè¾“å‡ºçš„ tensor
>>> x = torch.tensor(2.0)
>>> y = torch.tensor(5.0)

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
>>> loss_function = torch.nn.MSELoss()

# è®¡ç®—é¢„æµ‹è¾“å‡º
>>> p = x * w + b
>>> p
tensor(2., grad_fn=<AddBackward0>)

# è®¡ç®—æŸå¤±
# ç­‰åŒäº ((p - y) ** 2).mean()
>>> l = loss_function(p, y)
>>> l
tensor(9., grad_fn=<MseLossBackward>)
```

æ–¹ä¾¿å­ğŸ™‚ï¸ï¼Œå¦‚æœä½ æƒ³çœ‹æ›´å¤šçš„æŸå¤±è®¡ç®—å™¨å¯ä»¥å‚è€ƒä»¥ä¸‹åœ°å€ï¼š

- https://pytorch.org/docs/stable/nn.html#loss-functions

## pytorch çš„å‚æ•°è°ƒæ•´å™¨å°è£… (optimizer)

pytorch è¿˜æä¾›äº†æ ¹æ®å¯¼å‡½æ•°å€¼è°ƒæ•´å‚æ•°çš„è°ƒæ•´å™¨å°è£…ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸¤ç¯‡æ–‡ç« ä¸­çœ‹åˆ°çš„æ–¹æ³• (éšæœºåˆå§‹åŒ–å‚æ•°å€¼ï¼Œç„¶åæ ¹æ®å¯¼å‡½æ•°å€¼ * å­¦ä¹ æ¯”ç‡è°ƒæ•´å‚æ•°å‡å°‘æŸå¤±) åˆç§°éšæœºæ¢¯åº¦ä¸‹é™æ³• (Stochastic Gradient Descent)ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨å°è£…å¥½çš„è°ƒæ•´å™¨çš„ä¾‹å­ï¼š

``` python
# å®šä¹‰å‚æ•°
>>> w = torch.tensor(1.0, requires_grad=True)
>>> b = torch.tensor(0.0, requires_grad=True)

# å®šä¹‰è¾“å…¥å’Œè¾“å‡ºçš„ tensor
>>> x = torch.tensor(2.0)
>>> y = torch.tensor(5.0)

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
>>> loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
# éœ€è¦ä¼ å…¥å‚æ•°åˆ—è¡¨å’ŒæŒ‡å®šå­¦ä¹ æ¯”ç‡ï¼Œè¿™é‡Œçš„å­¦ä¹ æ¯”ç‡æ˜¯ 0.01
>>> optimizer = torch.optim.SGD([w, b], lr=0.01)

# è®¡ç®—é¢„æµ‹è¾“å‡º
>>> p = x * w + b
>>> p
tensor(2., grad_fn=<AddBackward0>)

# è®¡ç®—æŸå¤±
>>> l = loss_function(p, y)
>>> l
tensor(9., grad_fn=<MseLossBackward>)

# ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
>>> l.backward()

# ç¡®è®¤å‚æ•°çš„å¯¼å‡½æ•°å€¼
>>> w.grad
tensor(-12.)
>>> b.grad
tensor(-6.)

# ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
# ç­‰åŒäº:
# with torch.no_grad():
#     w -= w.grad * learning_rate
#     b -= b.grad * learning_rate
optimizer.step()

# æ¸…ç©ºå¯¼å‡½æ•°å€¼
# ç­‰åŒäº:
# w.grad.zero_()
# b.grad.zero_()
optimizer.zero_grad()

# ç¡®è®¤è°ƒæ•´åçš„å‚æ•°
>>> w
tensor(1.1200, requires_grad=True)
>>> b
tensor(0.0600, requires_grad=True)
>>> w.grad
tensor(0.)
>>> b.grad
tensor(0.)
```

SGD å‚æ•°è°ƒæ•´å™¨çš„å­¦ä¹ æ¯”ç‡æ˜¯å›ºå®šçš„ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´å­¦ä¹ æ¯”ç‡ï¼Œå¯ä»¥ä½¿ç”¨å…¶ä»–å‚æ•°è°ƒæ•´å™¨ï¼Œä¾‹å¦‚ Adam è°ƒæ•´å™¨ã€‚æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥å¼€å¯å†²é‡ (momentum) é€‰é¡¹æ”¹è¿›å­¦ä¹ é€Ÿåº¦ï¼Œè¯¥é€‰é¡¹å¼€å¯åå¯ä»¥åœ¨å‚æ•°è°ƒæ•´æ—¶å‚è€ƒå‰ä¸€æ¬¡è°ƒæ•´çš„æ–¹å‘ (æ­£è´Ÿ)ï¼Œå¦‚æœç›¸åŒåˆ™è°ƒæ•´æ›´å¤šï¼Œè€Œä¸åŒåˆ™è°ƒæ•´æ›´å°‘ã€‚

å¦‚æœä½ å¯¹ Adam è°ƒæ•´å™¨çš„å®ç°å’Œå†²é‡çš„å®ç°æœ‰å…´è¶£ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡ç«  (éœ€è¦ä¸€å®šçš„æ•°å­¦çŸ¥è¯†):

- https://mlfromscratch.com/optimizers-explained

å¦‚æœä½ æƒ³æŸ¥çœ‹ pytorch æä¾›çš„å…¶ä»–å‚æ•°è°ƒæ•´å™¨å¯ä»¥è®¿é—®ä»¥ä¸‹åœ°å€ï¼š

- https://pytorch.org/docs/stable/optim.html

## ä½¿ç”¨ pytorch å®ç°ä¸Šä¸€ç¯‡æ–‡ç« çš„ä¾‹å­

å¥½äº†ï¼Œå­¦åˆ°è¿™é‡Œæˆ‘ä»¬åº”è¯¥å¯¹ pytorch çš„åŸºæœ¬æ“ä½œæœ‰ä¸€å®šäº†è§£ï¼Œç°åœ¨æˆ‘ä»¬æ¥è¯•è¯•ç”¨ pytorch å®ç°ä¸Šä¸€ç¯‡æ–‡ç« æœ€åçš„ä¾‹å­ã€‚

ä¸Šä¸€ç¯‡æ–‡ç« æœ€åçš„ä¾‹å­ä»£ç å¦‚ä¸‹ï¼š

``` python
# å®šä¹‰å‚æ•°
weight = 1
bias = 0

# å®šä¹‰å­¦ä¹ æ¯”ç‡
learning_rate = 0.01

# å‡†å¤‡è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
training_set = [(2, 5), (5, 11), (6, 13), (7, 15), (8, 17)]
validating_set = [(12, 25), (1, 3)]
testing_set = [(9, 19), (13, 27)]

# è®°å½• weight ä¸ bias çš„å†å²å€¼
weight_history = [weight]
bias_history = [bias]

for epoch in range(1, 10000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    for x, y in training_set:
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = x * weight + bias
        # è®¡ç®—æŸå¤±
        diff = predicted - y
        loss = diff ** 2
        # æ‰“å°é™¤é”™ä¿¡æ¯
        print(f"training x: {x}, y: {y}, predicted: {predicted}, loss: {loss}, weight: {weight}, bias: {bias}")
        # è®¡ç®—å¯¼å‡½æ•°å€¼
        derivative_weight = 2 * diff * x
        derivative_bias = 2 * diff
        # ä¿®æ”¹ weight å’Œ bias ä»¥å‡å°‘ loss
        # diff ä¸ºæ­£æ—¶ä»£è¡¨é¢„æµ‹è¾“å‡º > æ­£ç¡®è¾“å‡ºï¼Œä¼šå‡å°‘ weight å’Œ bias
        # diff ä¸ºè´Ÿæ—¶ä»£è¡¨é¢„æµ‹è¾“å‡º < æ­£ç¡®è¾“å‡ºï¼Œä¼šå¢åŠ  weight å’Œ bias
        weight -= derivative_weight * learning_rate
        bias -= derivative_bias * learning_rate
        # è®°å½• weight å’Œ bias çš„å†å²å€¼
        weight_history.append(weight)
        bias_history.append(bias)

    # æ£€æŸ¥éªŒè¯é›†
    validating_accuracy = 0
    for x, y in validating_set:
        predicted = x * weight + bias
        validating_accuracy += 1 - abs(y - predicted) / y
        print(f"validating x: {x}, y: {y}, predicted: {predicted}")
    validating_accuracy /= len(validating_set)

    # å¦‚æœéªŒè¯é›†æ­£ç¡®ç‡å¤§äº 99 %ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    print(f"validating accuracy: {validating_accuracy}")
    if validating_accuracy > 0.99:
        break

# æ£€æŸ¥æµ‹è¯•é›†
testing_accuracy = 0
for x, y in testing_set:
    predicted = x * weight + bias
    testing_accuracy += 1 - abs(y - predicted) / y
    print(f"testing x: {x}, y: {y}, predicted: {predicted}")
testing_accuracy /= len(testing_set)
print(f"testing accuracy: {testing_accuracy}")

# æ˜¾ç¤º weight ä¸ bias çš„å˜åŒ–
from matplotlib import pyplot
pyplot.plot(weight_history, label="weight")
pyplot.plot(bias_history, label="bias")
pyplot.legend()
pyplot.show()
```

ä½¿ç”¨ pytorch å®ç°åä»£ç å¦‚ä¸‹:

``` python
# å¼•ç”¨ pytorch
import torch

# å®šä¹‰å‚æ•°
weight = torch.tensor(1.0, requires_grad=True)
bias = torch.tensor(0.0, requires_grad=True)

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD([weight, bias], lr=0.01)

# å‡†å¤‡è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
training_set = [
    (torch.tensor(2.0), torch.tensor(5.0)),
    (torch.tensor(5.0), torch.tensor(11.0)),
    (torch.tensor(6.0), torch.tensor(13.0)),
    (torch.tensor(7.0), torch.tensor(15.0)),
    (torch.tensor(8.0), torch.tensor(17.0))
]
validating_set = [
    (torch.tensor(12.0), torch.tensor(25.0)),
    (torch.tensor(1.0), torch.tensor(3.0))
]
testing_set = [
    (torch.tensor(9.0), torch.tensor(19.0)),
    (torch.tensor(13.0), torch.tensor(27.0))
]

# è®°å½• weight ä¸ bias çš„å†å²å€¼
weight_history = [weight.item()]
bias_history = [bias.item()]

for epoch in range(1, 10000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    for x, y in training_set:
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = x * weight + bias
        # è®¡ç®—æŸå¤±
        loss = loss_function(predicted, y)
        # æ‰“å°é™¤é”™ä¿¡æ¯
        print(f"training x: {x}, y: {y}, predicted: {predicted}, loss: {loss}, weight: {weight}, bias: {bias}")
        # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
        loss.backward()
        # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
        optimizer.step()
        # æ¸…ç©ºå¯¼å‡½æ•°å€¼
        optimizer.zero_grad()
        # è®°å½• weight å’Œ bias çš„å†å²å€¼
        weight_history.append(weight.item())
        bias_history.append(bias.item())

    # æ£€æŸ¥éªŒè¯é›†
    validating_accuracy = 0
    for x, y in validating_set:
        predicted = x * weight.item() + bias.item()
        validating_accuracy += 1 - abs(y - predicted) / y
        print(f"validating x: {x}, y: {y}, predicted: {predicted}")
    validating_accuracy /= len(validating_set)

    # å¦‚æœéªŒè¯é›†æ­£ç¡®ç‡å¤§äº 99 %ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    print(f"validating accuracy: {validating_accuracy}")
    if validating_accuracy > 0.99:
        break

# æ£€æŸ¥æµ‹è¯•é›†
testing_accuracy = 0
for x, y in testing_set:
    predicted = x * weight.item() + bias.item()
    testing_accuracy += 1 - abs(y - predicted) / y
    print(f"testing x: {x}, y: {y}, predicted: {predicted}")
testing_accuracy /= len(testing_set)
print(f"testing accuracy: {testing_accuracy}")

# æ˜¾ç¤º weight ä¸ bias çš„å˜åŒ–
from matplotlib import pyplot
pyplot.plot(weight_history, label="weight")
pyplot.plot(bias_history, label="bias")
pyplot.legend()
pyplot.show()
```

è¾“å‡ºå¦‚ä¸‹:

``` text
epoch: 1
training x: 2.0, y: 5.0, predicted: 2.0, loss: 9.0, weight: 1.0, bias: 0.0
training x: 5.0, y: 11.0, predicted: 5.659999847412109, loss: 28.515602111816406, weight: 1.1200000047683716, bias: 0.05999999865889549
training x: 6.0, y: 13.0, predicted: 10.090799331665039, loss: 8.463448524475098, weight: 1.6540000438690186, bias: 0.16679999232292175
training x: 7.0, y: 15.0, predicted: 14.246713638305664, loss: 0.5674403309822083, weight: 2.0031042098999023, bias: 0.22498400509357452
training x: 8.0, y: 17.0, predicted: 17.108564376831055, loss: 0.011786224320530891, weight: 2.1085643768310547, bias: 0.24004973471164703
validating x: 12.0, y: 25.0, predicted: 25.33220863342285
validating x: 1.0, y: 3.0, predicted: 2.3290724754333496
validating accuracy: 0.8815345764160156
epoch: 2
training x: 2.0, y: 5.0, predicted: 4.420266628265381, loss: 0.3360907733440399, weight: 2.0911941528320312, bias: 0.2378784418106079
training x: 5.0, y: 11.0, predicted: 10.821391105651855, loss: 0.03190113604068756, weight: 2.1143834590911865, bias: 0.24947310984134674
training x: 6.0, y: 13.0, predicted: 13.04651165008545, loss: 0.002163333585485816, weight: 2.132244348526001, bias: 0.25304529070854187
training x: 7.0, y: 15.0, predicted: 15.138755798339844, loss: 0.019253171980381012, weight: 2.1266629695892334, bias: 0.25211507081985474
training x: 8.0, y: 17.0, predicted: 17.107236862182617, loss: 0.011499744839966297, weight: 2.1072371006011963, bias: 0.24933995306491852
validating x: 12.0, y: 25.0, predicted: 25.32814598083496
validating x: 1.0, y: 3.0, predicted: 2.3372745513916016
validating accuracy: 0.8829828500747681
epoch: 3
training x: 2.0, y: 5.0, predicted: 4.427353858947754, loss: 0.32792359590530396, weight: 2.0900793075561523, bias: 0.24719521403312683
training x: 5.0, y: 11.0, predicted: 10.82357406616211, loss: 0.0311261098831892, weight: 2.112985134124756, bias: 0.2586481273174286
training x: 6.0, y: 13.0, predicted: 13.045942306518555, loss: 0.002110695466399193, weight: 2.1306276321411133, bias: 0.26217663288116455
training x: 7.0, y: 15.0, predicted: 15.137059211730957, loss: 0.018785227090120316, weight: 2.1251144409179688, bias: 0.2612577974796295
training x: 8.0, y: 17.0, predicted: 17.105924606323242, loss: 0.011220022104680538, weight: 2.105926036834717, bias: 0.2585166096687317
validating x: 12.0, y: 25.0, predicted: 25.324134826660156
validating x: 1.0, y: 3.0, predicted: 2.3453762531280518
validating accuracy: 0.8844133615493774

çœç•¥é€”ä¸­çš„è¾“å‡º

epoch: 202
training x: 2.0, y: 5.0, predicted: 4.950470924377441, loss: 0.0024531292729079723, weight: 2.0077908039093018, bias: 0.9348894953727722
training x: 5.0, y: 11.0, predicted: 10.984740257263184, loss: 0.00023285974748432636, weight: 2.0097720623016357, bias: 0.9358800649642944
training x: 6.0, y: 13.0, predicted: 13.003972053527832, loss: 1.5777208318468183e-05, weight: 2.0112979412078857, bias: 0.9361852407455444
training x: 7.0, y: 15.0, predicted: 15.011855125427246, loss: 0.00014054399798624218, weight: 2.0108213424682617, bias: 0.9361057877540588
training x: 8.0, y: 17.0, predicted: 17.00916290283203, loss: 8.39587883092463e-05, weight: 2.0091617107391357, bias: 0.9358686804771423
validating x: 12.0, y: 25.0, predicted: 25.028034210205078
validating x: 1.0, y: 3.0, predicted: 2.9433810710906982
validating accuracy: 0.9900028705596924
testing x: 9.0, y: 19.0, predicted: 19.004947662353516
testing x: 13.0, y: 27.0, predicted: 27.035730361938477
testing accuracy: 0.9992080926895142
```

åŒæ ·çš„è®­ç»ƒæˆåŠŸäº†ğŸ˜¼ã€‚ä½ å¯èƒ½ä¼šå‘ç°è¾“å‡ºçš„å€¼å’Œå‰ä¸€ç¯‡æ–‡ç« çš„å€¼æœ‰ä¸€äº›ä¸åŒï¼Œè¿™æ˜¯å› ä¸º pytorch é»˜è®¤ä½¿ç”¨ 32 ä½æµ®ç‚¹æ•° (float32) è¿›è¡Œè¿ç®—ï¼Œè€Œ python ä½¿ç”¨çš„æ˜¯ 64 ä½æµ®ç‚¹æ•° (float64), å¦‚æœä½ æŠŠå‚æ•°å®šä¹‰çš„éƒ¨åˆ†æ”¹æˆè¿™æ ·ï¼š

``` python
# å®šä¹‰å‚æ•°
weight = torch.tensor(1.0, dtype=torch.float64, requires_grad=True)
bias = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)
```

ç„¶åè®¡ç®—æŸå¤±çš„éƒ¨åˆ†æ”¹æˆè¿™æ ·ï¼Œåˆ™å¯ä»¥å¾—åˆ°å’Œå‰ä¸€ç¯‡æ–‡ç« ä¸€æ ·çš„è¾“å‡ºï¼š

``` python
# è®¡ç®—æŸå¤±
loss = loss_function(predicted, y.double())
```

## ä½¿ç”¨çŸ©é˜µä¹˜æ³•å®ç°æ‰¹æ¬¡è®­ç»ƒ

å‰é¢çš„ä¾‹å­è™½ç„¶ä½¿ç”¨ pytorch å®ç°äº†è®­ç»ƒï¼Œä½†è¿˜æ˜¯ä¸€ä¸ªä¸€ä¸ªå€¼çš„è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µä¹˜æ³•æ¥å®ç°æ‰¹æ¬¡è®­ç»ƒï¼Œä¸€æ¬¡è®¡ç®—å¤šä¸ªå€¼ï¼Œä»¥ä¸‹ä¿®æ”¹åçš„ä»£ç ï¼š

``` python
# å¼•ç”¨ pytorch
import torch

# å®šä¹‰å‚æ•°
weight = torch.tensor([[1.0]], requires_grad=True) # 1 è¡Œ 1 åˆ—
bias = torch.tensor(0.0, requires_grad=True)

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
optimizer = torch.optim.SGD([weight, bias], lr=0.01)

# å‡†å¤‡è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
training_set_x = torch.tensor([[2.0], [5.0], [6.0], [7.0], [8.0]]) # 5 è¡Œ 1 åˆ—ï¼Œä»£è¡¨æœ‰ 5 ç»„ï¼Œæ¯ç»„æœ‰ 1 ä¸ªè¾“å…¥
training_set_y = torch.tensor([[5.0], [11.0], [13.0], [15.0], [17.0]]) # 5 è¡Œ 1 åˆ—ï¼Œä»£è¡¨æœ‰ 5 ç»„ï¼Œæ¯ç»„æœ‰ 1 ä¸ªè¾“å‡º
validating_set_x = torch.tensor([[12.0], [1.0]]) # 2 è¡Œ 1 åˆ—ï¼Œä»£è¡¨æœ‰ 2 ç»„ï¼Œæ¯ç»„æœ‰ 1 ä¸ªè¾“å…¥
validating_set_y = torch.tensor([[25.0], [3.0]]) # 2 è¡Œ 1 åˆ—ï¼Œä»£è¡¨æœ‰ 2 ç»„ï¼Œæ¯ç»„æœ‰ 1 ä¸ªè¾“å‡º
testing_set_x = torch.tensor([[9.0], [13.0]]) # 2 è¡Œ 1 åˆ—ï¼Œä»£è¡¨æœ‰ 2 ç»„ï¼Œæ¯ç»„æœ‰ 1 ä¸ªè¾“å…¥
testing_set_y = torch.tensor([[19.0], [27.0]]) # 2 è¡Œ 1 åˆ—ï¼Œä»£è¡¨æœ‰ 2 ç»„ï¼Œæ¯ç»„æœ‰ 1 ä¸ªè¾“å‡º

# è®°å½• weight ä¸ bias çš„å†å²å€¼
weight_history = [weight[0][0].item()]
bias_history = [bias.item()]

for epoch in range(1, 10000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°

    # è®¡ç®—é¢„æµ‹å€¼
    # 5 è¡Œ 1 åˆ—çš„çŸ©é˜µä¹˜ä»¥ 1 è¡Œ 1 åˆ—çš„çŸ©é˜µï¼Œä¼šå¾—å‡º 5 è¡Œ 1 åˆ—çš„çŸ©é˜µ
    predicted = training_set_x.mm(weight) + bias
    # è®¡ç®—æŸå¤±
    loss = loss_function(predicted, training_set_y)
    # æ‰“å°é™¤é”™ä¿¡æ¯
    print(f"training x: {training_set_x}, y: {training_set_y}, predicted: {predicted}, loss: {loss}, weight: {weight}, bias: {bias}")
    # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
    loss.backward()
    # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
    optimizer.step()
    # æ¸…ç©ºå¯¼å‡½æ•°å€¼
    optimizer.zero_grad()
    # è®°å½• weight å’Œ bias çš„å†å²å€¼
    weight_history.append(weight[0][0].item())
    bias_history.append(bias.item())

    # æ£€æŸ¥éªŒè¯é›†
    with torch.no_grad(): # ç¦æ­¢è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
        predicted = validating_set_x.mm(weight) + bias
        validating_accuracy = 1 - ((validating_set_y - predicted).abs() / validating_set_y).mean()
    print(f"validating x: {validating_set_x}, y: {validating_set_y}, predicted: {predicted}")

    # å¦‚æœéªŒè¯é›†æ­£ç¡®ç‡å¤§äº 99 %ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    print(f"validating accuracy: {validating_accuracy}")
    if validating_accuracy > 0.99:
        break

# æ£€æŸ¥æµ‹è¯•é›†
with torch.no_grad(): # ç¦æ­¢è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
    predicted = testing_set_x.mm(weight) + bias
    testing_accuracy = 1 - ((testing_set_y - predicted).abs() / testing_set_y).mean()
print(f"testing x: {testing_set_x}, y: {testing_set_y}, predicted: {predicted}")
print(f"testing accuracy: {testing_accuracy}")

# æ˜¾ç¤º weight ä¸ bias çš„å˜åŒ–
from matplotlib import pyplot
pyplot.plot(weight_history, label="weight")
pyplot.plot(bias_history, label="bias")
pyplot.legend()
pyplot.show()
```

è¾“å‡ºå¦‚ä¸‹:

``` text
epoch: 1
training x: tensor([[2.],
        [5.],
        [6.],
        [7.],
        [8.]]), y: tensor([[ 5.],
        [11.],
        [13.],
        [15.],
        [17.]]), predicted: tensor([[2.],
        [5.],
        [6.],
        [7.],
        [8.]], grad_fn=<AddBackward0>), loss: 47.79999923706055, weight: tensor([[1.]], requires_grad=True), bias: 0.0
validating x: tensor([[12.],
        [ 1.]]), y: tensor([[25.],
        [ 3.]]), predicted: tensor([[22.0200],
        [ 1.9560]])
validating accuracy: 0.7663999795913696
epoch: 2
training x: tensor([[2.],
        [5.],
        [6.],
        [7.],
        [8.]]), y: tensor([[ 5.],
        [11.],
        [13.],
        [15.],
        [17.]]), predicted: tensor([[ 3.7800],
        [ 9.2520],
        [11.0760],
        [12.9000],
        [14.7240]], grad_fn=<AddBackward0>), loss: 3.567171573638916, weight: tensor([[1.8240]], requires_grad=True), bias: 0.13199999928474426
validating x: tensor([[12.],
        [ 1.]]), y: tensor([[25.],
        [ 3.]]), predicted: tensor([[24.7274],
        [ 2.2156]])
validating accuracy: 0.8638148307800293

çœç•¥é€”ä¸­çš„è¾“å‡º

epoch: 1103
training x: tensor([[2.],
        [5.],
        [6.],
        [7.],
        [8.]]), y: tensor([[ 5.],
        [11.],
        [13.],
        [15.],
        [17.]]), predicted: tensor([[ 4.9567],
        [10.9867],
        [12.9966],
        [15.0066],
        [17.0166]], grad_fn=<AddBackward0>), loss: 0.0004764374461956322, weight: tensor([[2.0100]], requires_grad=True), bias: 0.936755359172821
validating x: tensor([[12.],
        [ 1.]]), y: tensor([[25.],
        [ 3.]]), predicted: tensor([[25.0564],
        [ 2.9469]])
validating accuracy: 0.99001544713974
testing x: tensor([[ 9.],
        [13.]]), y: tensor([[19.],
        [27.]]), predicted: tensor([[19.0265],
        [27.0664]])
testing accuracy: 0.998073160648346
```

å—¯ï¼Ÿè¿™å›æ€ä¹ˆç”¨äº† 1103 æ¬¡æ‰è®­ç»ƒæˆåŠŸï¼Ÿè¿™æ˜¯å› ä¸º weight å’Œ bias è°ƒæ•´çš„æ–¹å‘å§‹ç»ˆéƒ½æ˜¯ä¸€è‡´çš„ï¼Œæ‰€ä»¥åªç”¨ä¸€ä¸ªæ‰¹æ¬¡è®­ç»ƒåè€Œä¼šæ›´æ…¢ã€‚åœ¨ä¹‹åçš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨æ›´å¤šçš„å‚æ•° (ç¥ç»å…ƒ) æ¥è®­ç»ƒï¼Œè€Œå®ƒä»¬å¯ä»¥æœ‰ä¸åŒçš„è°ƒæ•´æ–¹å‘ï¼Œæ‰€ä»¥ä¸ä¼šå‡ºç°è¿™ä¸ªä¾‹å­ä¸­çš„é—®é¢˜ã€‚å½“ç„¶ï¼Œä¸šåŠ¡ä¸Šæœ‰çš„æ—¶å€™ä¼šå‡ºç°å› ä¸ºå‚æ•°è°ƒæ•´æ–¹å‘å…¨éƒ¨ä¸€è‡´å¯¼è‡´è®­ç»ƒå¾ˆæ…¢ï¼Œæˆ–è€…æ ¹æœ¬æ— æ³•æ”¶æ•›çš„é—®é¢˜ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ¢æ¨¡å‹ï¼Œæˆ–è€…åˆ‡åˆ†å¤šä¸ªæ‰¹æ¬¡æ¥è§£å†³ã€‚

## åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„ä¾‹å­

ä¸Šé¢çš„ä¾‹å­å®šä¹‰è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ—¶å€™éƒ½æ˜¯ä¸€ä¸ªä¸ª tensor çš„å®šä¹‰ï¼Œæœ‰æ²¡æœ‰è§‰å¾—å¾ˆéº»çƒ¦ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡ pytorch æä¾›çš„ tensor æ“ä½œæ¥æ›´æ–¹ä¾¿çš„åˆ’åˆ†å®ƒä»¬ï¼š

``` python
# åŸå§‹æ•°æ®é›†
>>> dataset = [(1, 3), (2, 5), (5, 11), (6, 13), (7, 15), (8, 17), (9, 19), (12, 25), (13, 27)]

# è½¬æ¢åŸå§‹æ•°æ®é›†åˆ° tensorï¼Œå¹¶ä¸”æŒ‡å®šæ•°å€¼ç±»å‹ä¸ºæµ®ç‚¹æ•°
>>> dataset_tensor = torch.tensor(dataset, dtype=torch.float32)
>>> dataset_tensor
tensor([[ 1.,  3.],
        [ 2.,  5.],
        [ 5., 11.],
        [ 6., 13.],
        [ 7., 15.],
        [ 8., 17.],
        [ 9., 19.],
        [12., 25.],
        [13., 27.]])

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
# è¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
>>> torch.random.manual_seed(0)
<torch._C.Generator object at 0x10cc03070>

# ç”Ÿæˆéšæœºç´¢å¼•å€¼, ç”¨äºæ‰“ä¹±æ•°æ®é¡ºåºé˜²æ­¢åˆ†å¸ƒä¸å‡
>>> dataset_tensor.shape
torch.Size([9, 2])
>>> random_indices = torch.randperm(dataset_tensor.shape[0])
>>> random_indices
tensor([8, 0, 2, 3, 7, 1, 4, 5, 6])

# è®¡ç®—è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•å€¼åˆ—è¡¨
# 60 % çš„æ•°æ®åˆ’åˆ†åˆ°è®­ç»ƒé›†ï¼Œ20 % çš„æ•°æ®åˆ’åˆ†åˆ°éªŒè¯é›†ï¼Œ20 % çš„æ•°æ®åˆ’åˆ†åˆ°æµ‹è¯•é›†
>>> training_indices = random_indices[:int(len(random_indices)*0.6)]
>>> training_indices
tensor([8, 0, 2, 3, 7])
>>> validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
>>> validating_indices
tensor([1, 4])
>>> testing_indices = random_indices[int(len(random_indices)*0.8):]
>>> testing_indices
tensor([5, 6])

# åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
>>> training_set_x = dataset_tensor[training_indices][:,:1] # ç¬¬ä¸€ç»´åº¦ä¸æˆªå–ï¼Œç¬¬äºŒç»´åº¦æˆªå–ç´¢å¼•å€¼å°äº 1 çš„å…ƒç´ 
>>> training_set_y = dataset_tensor[training_indices][:,1:] # ç¬¬ä¸€ç»´åº¦ä¸æˆªå–ï¼Œç¬¬äºŒç»´åº¦æˆªå–ç´¢å¼•å€¼å¤§äºæˆ–ç­‰äº 1 çš„å…ƒç´ 
>>> training_set_x
tensor([[13.],
        [ 1.],
        [ 5.],
        [ 6.],
        [12.]])
>>> training_set_y
tensor([[27.],
        [ 3.],
        [11.],
        [13.],
        [25.]])
>>> validating_set_x = dataset_tensor[validating_indices][:,:1]
>>> validating_set_y = dataset_tensor[validating_indices][:,1:]
>>> validating_set_x
tensor([[2.],
        [7.]])
>>> validating_set_y
tensor([[ 5.],
        [15.]])
>>> testing_set_x = dataset_tensor[testing_indices][:,:1]
>>> testing_set_y = dataset_tensor[testing_indices][:,1:]
>>> testing_set_x
tensor([[8.],
        [9.]])
>>> testing_set_y
tensor([[17.],
        [19.]])
```

å†™æˆä»£ç å¦‚ä¸‹ï¼š

``` python
# åŸå§‹æ•°æ®é›†
dataset = [(1, 3), (2, 5), (5, 11), (6, 13), (7, 15), (8, 17), (9, 19), (12, 25), (13, 27)]

# è½¬æ¢åŸå§‹æ•°æ®é›†åˆ° tensor
dataset_tensor = torch.tensor(dataset, dtype=torch.float32)

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
torch.random.manual_seed(0)

# åˆ‡åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
random_indices = torch.randperm(dataset_tensor.shape[0])
training_indices = random_indices[:int(len(random_indices)*0.6)]
validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
testing_indices = random_indices[int(len(random_indices)*0.8):]
training_set_x = dataset_tensor[training_indices][:,:1]
training_set_y = dataset_tensor[training_indices][:,1:]
validating_set_x = dataset_tensor[validating_indices][:,:1]
validating_set_y = dataset_tensor[validating_indices][:,1:]
testing_set_x = dataset_tensor[testing_indices][:,:1]
testing_set_y = dataset_tensor[testing_indices][:,1:]
```

æ³¨æ„æ”¹å˜æ•°æ®åˆ†å¸ƒå¯ä»¥å½±å“è®­ç»ƒé€Ÿåº¦ï¼Œä½ å¯ä»¥è¯•è¯•ä¸Šé¢çš„ä»£ç ç»è¿‡å¤šå°‘æ¬¡è®­ç»ƒå¯ä»¥è®­ç»ƒæˆåŠŸ (è¾¾åˆ° 99 % çš„æ­£ç¡®ç‡)ã€‚ä¸è¿‡ï¼Œæ•°æ®è¶Šå¤šè¶Šå‡åŒ€ï¼Œåˆ†å¸ƒå¯¹è®­ç»ƒé€Ÿåº¦çš„å½±å“å°±è¶Šå°‘ã€‚

## å®šä¹‰æ¨¡å‹ç±» (torch.nn.Module)

å¦‚æœæˆ‘ä»¬æƒ³æŠŠè‡ªå·±å†™å¥½çš„æ¨¡å‹æä¾›ç»™åˆ«äººç”¨ï¼Œæˆ–è€…ç”¨åˆ«äººå†™å¥½çš„æ¨¡å‹ï¼Œåº”è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿpytorch æä¾›äº†å°è£…æ¨¡å‹çš„åŸºç¡€ç±» `torch.nn.Module`ï¼Œä¸Šé¢ä¾‹å­ä¸­çš„æ¨¡å‹å¯ä»¥æ”¹å†™å¦‚ä¸‹ï¼š

``` python
# å¼•ç”¨ pytorch å’Œæ˜¾ç¤ºå›¾è¡¨ä½¿ç”¨çš„ matplotlib
import torch
from matplotlib import pyplot

# å®šä¹‰æ¨¡å‹
# æ¨¡å‹éœ€è¦å®šä¹‰ forward å‡½æ•°æ¥æ”¶è¾“å…¥å¹¶è¿”å›é¢„æµ‹è¾“å‡º
# add_history å’Œ show_history æ˜¯è‡ªå®šä¹‰å‡½æ•°ï¼Œå®ƒä»¬ä»…ç”¨äºå¸®åŠ©æˆ‘ä»¬ç†è§£æœºå™¨å­¦ä¹ çš„åŸç†ï¼Œå®é™…ä¸éœ€è¦è¿™æ ·åš
class MyModle(torch.nn.Module):
    def __init__(self):
        # åˆå§‹åŒ–åŸºç±»
        super().__init__()
        # å®šä¹‰å‚æ•°
        # éœ€è¦ä½¿ç”¨ torch.nn.Parameter åŒ…è£…ï¼Œrequires_grad ä¸éœ€è¦è®¾ç½® (ä¼šç»Ÿä¸€å¸®æˆ‘ä»¬è®¾ç½®)
        self.weight = torch.nn.Parameter(torch.tensor([[1.0]]))
        self.bias = torch.nn.Parameter(torch.tensor(0.0))
        # è®°å½• weight ä¸ bias çš„å†å²å€¼
        self.weight_history = [self.weight[0][0].item()]
        self.bias_history = [self.bias.item()]

    def forward(self, x):
        # è®¡ç®—é¢„æµ‹å€¼
        predicted = x.mm(self.weight) + self.bias
        return predicted

    def add_history(self):
        # è®°å½• weight å’Œ bias çš„å†å²å€¼
        self.weight_history.append(self.weight[0][0].item())
        self.bias_history.append(self.bias.item())

    def show_history(self):
        # æ˜¾ç¤º weight ä¸ bias çš„å˜åŒ–
        pyplot.plot(self.weight_history, label="weight")
        pyplot.plot(self.bias_history, label="bias")
        pyplot.legend()
        pyplot.show()

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModle()

# åˆ›å»ºæŸå¤±è®¡ç®—å™¨
loss_function = torch.nn.MSELoss()

# åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
# è°ƒç”¨ parameters å‡½æ•°å¯ä»¥è‡ªåŠ¨é€’å½’è·å–æ¨¡å‹ä¸­çš„å‚æ•°åˆ—è¡¨ (æ³¨æ„æ˜¯é€’å½’è·å–ï¼ŒåµŒå¥—æ¨¡å‹ä¹Ÿèƒ½æ”¯æŒ)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# åŸå§‹æ•°æ®é›†
dataset = [(1, 3), (2, 5), (5, 11), (6, 13), (7, 15), (8, 17), (9, 19), (12, 25), (13, 27)]

# è½¬æ¢åŸå§‹æ•°æ®é›†åˆ° tensor
dataset_tensor = torch.tensor(dataset, dtype=torch.float32)

# ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
# è¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
torch.random.manual_seed(0)

# åˆ‡åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†
random_indices = torch.randperm(dataset_tensor.shape[0])
training_indices = random_indices[:int(len(random_indices)*0.6)]
validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
testing_indices = random_indices[int(len(random_indices)*0.8):]
training_set_x = dataset_tensor[training_indices][:,:1]
training_set_y = dataset_tensor[training_indices][:,1:]
validating_set_x = dataset_tensor[validating_indices][:,:1]
validating_set_y = dataset_tensor[validating_indices][:,1:]
testing_set_x = dataset_tensor[testing_indices][:,:1]
testing_set_y = dataset_tensor[testing_indices][:,1:]

# å¼€å§‹è®­ç»ƒè¿‡ç¨‹
for epoch in range(1, 10000):
    print(f"epoch: {epoch}")

    # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
    # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.train()

    # è®¡ç®—é¢„æµ‹å€¼
    predicted = model(training_set_x)
    # è®¡ç®—æŸå¤±
    loss = loss_function(predicted, training_set_y)
    # æ‰“å°é™¤é”™ä¿¡æ¯
    print(f"training x: {training_set_x}, y: {training_set_y}, predicted: {predicted}, loss: {loss}, weight: {model.weight}, bias: {model.bias}")
    # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
    loss.backward()
    # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
    optimizer.step()
    # æ¸…ç©ºå¯¼å‡½æ•°å€¼
    optimizer.zero_grad()
    # è®°å½• weight å’Œ bias çš„å†å²å€¼
    model.add_history()

    # æ£€æŸ¥éªŒè¯é›†
    # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
    model.eval()
    predicted = model(validating_set_x)
    validating_accuracy = 1 - ((validating_set_y - predicted).abs() / validating_set_y).mean()
    print(f"validating x: {validating_set_x}, y: {validating_set_y}, predicted: {predicted}")

    # å¦‚æœéªŒè¯é›†æ­£ç¡®ç‡å¤§äº 99 %ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    print(f"validating accuracy: {validating_accuracy}")
    if validating_accuracy > 0.99:
        break

# æ£€æŸ¥æµ‹è¯•é›†
predicted = model(testing_set_x)
testing_accuracy = 1 - ((testing_set_y - predicted).abs() / testing_set_y).mean()
print(f"testing x: {testing_set_x}, y: {testing_set_y}, predicted: {predicted}")
print(f"testing accuracy: {testing_accuracy}")

# æ˜¾ç¤º weight ä¸ bias çš„å˜åŒ–
model.show_history()
```

å®šä¹‰å’Œä½¿ç”¨æ¨¡å‹ç±»éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š

- å¿…é¡»åœ¨æ„é€ å‡½æ•° `__init__` ä¸­è°ƒç”¨ `super().__init__()` åˆå§‹åŒ–åŸºç±» (ä¸€èˆ¬ python ç»§æ‰¿ç±»ä¹Ÿéœ€è¦è¿™æ ·åš)
- å¿…é¡»å®šä¹‰ `forward` å‡½æ•°æ¥æ”¶è¾“å…¥å¹¶è¿”å›é¢„æµ‹è¾“å‡º
- æ¨¡å‹ä¸­å®šä¹‰å‚æ•°éœ€è¦ä½¿ç”¨ `torch.nn.Parameter` åŒ…è£…ï¼Œ`requires_grad` ä¸éœ€è¦è®¾ç½® (ä¼šç»Ÿä¸€å¸®æˆ‘ä»¬è®¾ç½®)
- è°ƒç”¨ `model.parameters()` å¯ä»¥é€’å½’è·å–å‚æ•°åˆ—è¡¨ (æ”¯æŒåµŒå¥—æ¨¡å‹)ï¼Œåˆ›å»ºå‚æ•°è°ƒæ•´å™¨æ—¶éœ€è¦è¿™ä¸ªå‚æ•°åˆ—è¡¨
- åœ¨è®­ç»ƒå‰è°ƒç”¨ `model.train()` å¼€å¯è‡ªåŠ¨å¾®åˆ†ç­‰åŠŸèƒ½
- åœ¨éªŒè¯æˆ–è€…ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å‰è°ƒç”¨ `model.eval` å…³é—­è‡ªåŠ¨å¾®åˆ†ç­‰åŠŸèƒ½

æˆ‘ä»¬åœ¨åé¢ç»§ç»­ä½¿ç”¨ pytorch è¿›è¡Œæœºå™¨å­¦ä¹ æ—¶ï¼Œä»£ç çš„ç»“æ„ä¼šåŸºæœ¬å’Œä¸Šé¢çš„ä¾‹å­ä¸€æ ·ï¼Œåªæ˜¯æ¨¡å‹å’Œæ£€æŸ¥éªŒè¯é›†æµ‹è¯•é›†çš„éƒ¨åˆ†ä¸åŒã€‚æ­¤å¤–ï¼Œæ‰¹æ¬¡æ­£è§„åŒ–ä¸ Dropout ç­‰åŠŸèƒ½ä¼šåœ¨åé¢çš„æ–‡ç« ä¸­ä»‹ç»ã€‚

æœ¬ç¯‡å°±åˆ°æ­¤ç»“æŸäº†ï¼Œç›¸ä¿¡çœ‹åˆ°è¿™é‡Œä½ å·²ç»æŒæ¡äº†ç”¨ pytorch è¿›è¡Œæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¨¡å¼ğŸ˜¼ã€‚

## å†™åœ¨æœ€å

æœ¬ç¯‡ä»‹ç»çš„ä¸œè¥¿ä¹Ÿå¾ˆåŸºç¡€ï¼Œä½†æ˜¯è¿™äº›åŸºç¡€å¯¹åº”ç”¨æœºå™¨å­¦ä¹ å¿…ä¸å¯å°‘ã€‚è¿™ä¸¤ç¯‡çš„å†…å®¹åœ¨å¾ˆå¤šæœºå™¨å­¦ä¹ æ•™ç¨‹ä¸­éƒ½æ²¡æœ‰æåˆ°ï¼Œå®ƒä»¬ç›´æ¥å°±ä»å¤šå±‚çº¿æ€§æ¨¡å‹å¼€å§‹è®²äº†ï¼Œæ‰€ä»¥å¾ˆå¤šäººä¼šæŠ±æ€¨å…¥é—¨å¥½éš¾ğŸ˜«ã€‚å¦‚æœä½ çœ‹è¿‡ pytorch å‡ºçš„å®˜æ–¹ä¹¦ç± ã€ŠDeep Learning with Pytorchã€‹å¯èƒ½ä¼šå‘ç°ï¼Œè¿™ä¸¤ç¯‡çš„ä»‹ç»é¡ºåºå’Œè¿™æœ¬ä¹¦çš„ä»‹ç»é¡ºåºå¾ˆæ¥è¿‘ï¼Œæ˜¯çš„ï¼Œå†™è¿™ä¸¤ç¯‡çš„æ—¶å€™æˆ‘å‚è€ƒäº†è¿™æœ¬ä¹¦ï¼ŒæŒ‰è¿™ä¸ªé¡ºåºæ¥ç†è§£æ˜¯æœ€å®¹æ˜“çš„ã€‚

ä¸‹ä¸€ç¯‡å¼€å§‹å°†ä¼šè®²è§£çº¿æ€§æ¨¡å‹ï¼Œæ¿€æ´»å‡½æ•°å’Œå¤šå±‚çº¿æ€§æ¨¡å‹ï¼Œå¹¶ä¸”ä¼šç»™å‡ºæ›´æ¥è¿‘å®é™…çš„ä¾‹å­ï¼Œä½†å¯èƒ½ä¼šéœ€è¦æ›´å¤šæ—¶é—´ï¼Œæƒ³çœ‹çš„è€å¿ƒç­‰ç­‰å­ğŸ™ï¸ã€‚
