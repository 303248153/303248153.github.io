# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (å) - å¯¹è±¡è¯†åˆ« Faster-RCNN - è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©

æ¯æ¬¡çœ‹åˆ°å¤§æ•°æ®äººè„¸è¯†åˆ«æŠ“é€ƒçŠ¯çš„æ–°é—»æˆ‘éƒ½ä¼šæ„Ÿå¹æŠ€æœ¯å‘å±•çš„å¤ªå¿«äº†ï¼Œå›½å®¶æ²»å®‰æ°´å¹³ä¹Ÿè¶Šæ¥è¶Šå¥½äº†ğŸ¤©ã€‚ä¸è¿‡é‚£ç§ç³»ç»Ÿä¸ªäººæ˜¯æ²¡åŠæ³•åšå‡ºæ¥çš„ï¼Œä»Šå¤©æˆ‘ä»¬åªè¯•ç€åšä¸ªç®€å•çš„ï¼Œæ€ä¹ˆæ ¹æ®å›¾ç‰‡æŠŠæ²¡æœ‰æˆ´å£ç½©çš„å®¶ä¼™æŠ“å‡ºæ¥ğŸ¤¬ã€‚è¿™ç¯‡ä¼šä»‹ç»å®ç”¨æ€§æ¯”è¾ƒå¼ºçš„å¯¹è±¡è¯†åˆ«æ¨¡å‹ Faster-RCNNï¼Œéœ€è¦çš„åŸºç¡€çŸ¥è¯†æ¯”è¾ƒå¤šï¼Œå¦‚æœå¯¹æœºå™¨å­¦ä¹ å’Œå¯¹è±¡è¯†åˆ«æ²¡æœ‰åŸºç¡€äº†è§£è¯·çœ‹[è¿™ä¸ªç³»åˆ—ä¹‹å‰çš„æ–‡ç« ](https://www.cnblogs.com/zkweb/category/1690853.html)ã€‚

![01](./01.png)

## RCNN, Fast-RCNN çš„å¼±ç‚¹

æˆ‘åœ¨[ä¸Šä¸€ç¯‡](https://www.cnblogs.com/zkweb/p/14048685.html)æ–‡ç« ä»‹ç»äº†å¯¹è±¡è¯†åˆ«ä½¿ç”¨çš„ RCNN, Fast-RCNN æ¨¡å‹ï¼Œåœ¨è¿™é‡Œæˆ‘ç®€å•æ€»ç»“ä¸€ä¸‹å®ƒä»¬çš„ç¼ºç‚¹ï¼ŒFaster-RCNN å°†ä¼šå…‹æœå®ƒä»¬ï¼š

- é€‰å–åŒºåŸŸä½¿ç”¨çš„ç®—æ³•æ˜¯å›ºå®šçš„ï¼Œä¸å‚ä¸å­¦ä¹ 
- é€‰å–åŒºåŸŸçš„ç®—æ³•æœ¬èº«æ¶ˆè€—æ¯”è¾ƒé«˜ (æœç´¢é€‰æ‹©æ³•)
- é€‰å–åŒºåŸŸçš„ç®—æ³•é€‰å‡ºæ¥çš„åŒºåŸŸå¤§éƒ¨åˆ†éƒ½æ˜¯é‡åˆçš„ï¼Œå¹¶ä¸”åªæœ‰å¾ˆå°ä¸€éƒ¨åˆ†åŒ…å«æˆ‘ä»¬æƒ³è¦è¯†åˆ«çš„å¯¹è±¡
- åŒºåŸŸèŒƒå›´çš„ç²¾åº¦æ¯”è¾ƒä½ (å³ä½¿ç»è¿‡è°ƒæ•´)
- åˆ¤æ–­åˆ†ç±»æœ‰æ—¶åªèƒ½ä½¿ç”¨éƒ¨åˆ†åŒ…å«å¯¹è±¡çš„åŒºåŸŸ (ä¾‹å¦‚é€‰å–åŒºåŸŸçš„ç®—æ³•ç»™å‡ºå·¦åŠå¼ è„¸æ‰€åœ¨çš„åŒºåŸŸï¼Œé‚£ä¹ˆå°±åªèƒ½ä½¿ç”¨å·¦åŠå¼ è„¸åˆ¤æ–­åˆ†ç±»)

## Faster-RCNN æ¦‚è§ˆ

Faster-RCNN æ˜¯ RCNN å’Œ Fast-RCNN çš„è¿›åŒ–ç‰ˆï¼Œæœ€å¤§çš„ç‰¹å¾æ˜¯å¼•å…¥äº†åŒºåŸŸç”Ÿæˆç½‘ç»œ (RPN - Region Proposal Network)ï¼ŒåŒºåŸŸç”Ÿæˆç½‘ç»œæ”¯æŒä½¿ç”¨æœºå™¨å­¦ä¹ ä»£æ›¿å›ºå®šçš„ç®—æ³•æ‰¾å‡ºå›¾ç‰‡ä¸­å¯èƒ½åŒ…å«å¯¹è±¡çš„åŒºåŸŸï¼Œç²¾åº¦æ¯”å›ºå®šçš„ç®—æ³•è¦é«˜å¾ˆå¤šï¼Œè€Œä¸”é€Ÿåº¦ä¹Ÿå˜å¿«äº†ã€‚

Faster-RCNN çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆ†æˆäº†ä¸¤å¤§éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯åŒºåŸŸç”Ÿæˆç½‘ç»œï¼Œé¦–å…ˆä¼šæŠŠå›¾ç‰‡åˆ’åˆ†ä¸ºå¤šä¸ªå°åŒºåŸŸ (å¤§å°ä¾èµ–äºå›¾ç‰‡å¤§å°å’Œ CNN ç½‘ç»œç»“æ„ï¼Œè¯¦ç»†ä¼šåœ¨åé¢è¯´æ˜)ï¼Œæ¯ä¸ªå°åŒºåŸŸéƒ½å¯¹åº”ä¸€ä¸ªé”šç‚¹ (Anchor)ï¼ŒåŒºåŸŸç”Ÿæˆç½‘ç»œä¼šåˆ¤æ–­é”šç‚¹æ‰€åœ¨çš„åŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œä¸åŒ…å«çš„å¯¹è±¡çš„å½¢çŠ¶ (ä¾‹å¦‚åªåŒ…å«é¼»å­ï¼Œå°±å¤§çº¦å¯ä»¥ä¼°è®¡å‘¨å›´çš„å‡ ä¸ªåŒºåŸŸæ˜¯è„¸)ï¼›ç¬¬äºŒéƒ¨åˆ†æ˜¯æ ‡ç­¾åˆ†ç±»ç½‘ç»œï¼Œä¸ä¸Šä¸€ç¯‡æ–‡ç« ä»‹ç»çš„ Fast-RCNN åŸºæœ¬ä¸Šç›¸åŒï¼Œä¼šæ ¹æ®åŒºåŸŸç”Ÿæˆç½‘ç»œçš„è¾“å‡ºæˆªå–ç‰¹å¾ï¼Œå¹¶æ ¹æ®ç‰¹å¾åˆ¤æ–­å±äºä»€ä¹ˆåˆ†ç±»ã€‚

![02](./02.png)

å› ä¸ºåŒºåŸŸç”Ÿæˆç½‘ç»œå¯ä»¥å‚ä¸å­¦ä¹ ï¼Œæˆ‘ä»¬å¯ä»¥å®šåˆ¶ä¸€ä¸ªåªè¯†åˆ«æŸå‡ ç§å¯¹è±¡çš„ç½‘ç»œï¼Œä¾‹å¦‚å›¾ç‰‡ä¸­æœ‰äººï¼Œç‹—ï¼Œè½¦ï¼Œæ ‘ï¼Œæˆ¿å­çš„æ—¶å€™ï¼Œå›ºå®šçš„ç®—æ³•å¯èƒ½ä¼šæŠŠä»–ä»¬å…¨éƒ¨æå–å‡ºæ¥ï¼Œä½†åŒºåŸŸç”Ÿæˆç½‘ç»œç»è¿‡è®­ç»ƒå¯ä»¥åªæå–äººæ‰€åœ¨çš„åŒºåŸŸï¼Œå…¶ä»–å¯¹è±¡æ‰€åœ¨çš„åŒºåŸŸéƒ½ä¼šå½“ä½œèƒŒæ™¯å¤„ç†ï¼Œè¿™æ ·åŒºåŸŸç”Ÿæˆç½‘ç»œè¾“å‡ºçš„åŒºåŸŸæ•°é‡å°†ä¼šå°‘å¾ˆå¤šï¼Œè€Œä¸”åŒ…å«å¯¹è±¡çš„å¯èƒ½æ€§ä¼šå¾ˆé«˜ã€‚

Faster-RCNN å¦ä¸€ä¸ªæ¯”è¾ƒå¼ºå¤§çš„ç‰¹å¾æ˜¯ä¼šåˆ†ä¸¤æ­¥æ¥è¯†åˆ«åŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ä¸è°ƒæ•´åŒºåŸŸèŒƒå›´ï¼Œç¬¬ä¸€æ­¥åœ¨åŒºåŸŸç”Ÿæˆç½‘ç»œï¼Œç¬¬äºŒæ­¥åœ¨æ ‡ç­¾åˆ†ç±»ç½‘ç»œã€‚ä¸¾ä¸€ä¸ªé€šä¿—çš„ä¾‹å­ï¼Œå¦‚æœåŒºåŸŸç”Ÿæˆç½‘ç»œé€‰å–äº†æŸä¸ªåŒ…å«äº†è„¸çš„å·¦åŠéƒ¨åˆ†çš„åŒºåŸŸï¼Œå®ƒä¼šåˆ¤æ–­è¿™ä¸ªåŒºåŸŸå¯èƒ½åŒ…å«å¯¹è±¡ï¼Œå¹¶ä¸”è¦æ±‚åŒºåŸŸèŒƒå›´å‘å³æ‰©å¤§ä¸€äº›ï¼Œ**æ¥ä¸‹æ¥æ ‡ç­¾åˆ†ç±»ç½‘ç»œä¼šæˆªå–èŒƒå›´æ‰©å¤§ä¹‹åçš„åŒºåŸŸï¼Œè¿™ä¸ªåŒºåŸŸä¼šåŒæ—¶åŒ…å«è„¸çš„å·¦åŠéƒ¨åˆ†å’Œå³åŠéƒ¨åˆ†**ï¼Œä¹Ÿå°±æ˜¯æˆªå–å‡ºæ¥çš„ç‰¹å¾ä¼šåŒ…å«æ›´å¤šçš„ä¿¡æ¯ï¼Œè¿™æ—¶æ ‡ç­¾åˆ†ç±»ç½‘ç»œå¯ä»¥ä½¿ç”¨ç‰¹å¾è¿›ä¸€æ­¥åˆ¤æ–­è¿™å¼ è„¸æ‰€å±çš„åˆ†ç±»ï¼Œå¦‚æœèŒƒå›´æ‰©å¤§ä»¥åå‘ç°è¿™ä¸æ˜¯ä¸€å¼ è„¸è€Œæ˜¯åˆ«çš„ä»€ä¹ˆä¸œè¥¿é‚£ä¹ˆåŒºåŸŸåˆ†ç±»ç½‘ç»œä¼šè¾“å‡º "éå¯¹è±¡" çš„åˆ†ç±»æ’é™¤è¿™ä¸ªåŒºåŸŸï¼Œå¦‚æœåˆ¤æ–­æ˜¯è„¸é‚£ä¹ˆæ ‡ç­¾åˆ†ç±»ç½‘ç»œä¼šè¿›ä¸€æ­¥çš„è°ƒæ•´åŒºåŸŸèŒƒå›´ï¼Œä½¿å¾—èŒƒå›´æ›´ç²¾å‡†ã€‚è€Œ Fast-RCNN é‡åˆ°åŒæ ·çš„æƒ…å†µåªèƒ½æ ¹æ®è„¸çš„å·¦åŠéƒ¨åˆ†å¯¹åº”çš„ç‰¹å¾åˆ¤æ–­åˆ†ç±»ï¼Œä¿¡æ¯é‡ä¸è¶³å¯èƒ½ä¼šå¯¼è‡´ç»“æœä¸å‡†ç¡®ã€‚è¿™ç§åšæ³•ä½¿å¾— Faster-RCNN çš„è¯†åˆ«ç²¾åº¦ç›¸å¯¹äºä¹‹å‰çš„æ¨¡å‹æå‡äº†å¾ˆå¤šã€‚

æ¥ä¸‹æ¥çœ‹çœ‹ Faster-RCNN çš„å®ç°ç»†èŠ‚å§ï¼Œéƒ¨åˆ†å†…å®¹æœ‰ä¸€å®šéš¾åº¦ğŸ¤•ï¼Œå¦‚æœè§‰å¾—éš¾ä»¥ç†è§£å¯ä»¥å…ˆè·³è¿‡å»åé¢å†å‚è€ƒä»£ç å®ç°ã€‚

Faster-RCNN çš„åŸå§‹è®ºæ–‡åœ¨[è¿™é‡Œ](https://arxiv.org/pdf/1506.01497.pdf)ï¼Œæœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹ğŸ˜ˆã€‚

## Faster-RCNN çš„å®ç°

è¿™ç¯‡ç»™å‡ºçš„ä»£ç ä¼šä½¿ç”¨ Pillow ç±»åº“å®ç°ï¼Œä»£æ›¿ä¹‹å‰çš„ opencvï¼Œæ‰€ä»¥éƒ¨åˆ†å¤„ç†ç›¸åŒçš„æ­¥éª¤ä¹Ÿä¼šç»™å‡ºæ–°çš„ä»£ç ä¾‹å­ã€‚

### ç¼©æ”¾æ¥æºå›¾ç‰‡

å’Œ Fast-RCNN ä¸€æ ·ï¼ŒFaster-RCNN ä¹Ÿä¼šä½¿ç”¨ CNN æ¨¡å‹é’ˆå¯¹æ•´å¼ å›¾ç‰‡ç”Ÿæˆå„ä¸ªåŒºåŸŸçš„ç‰¹å¾ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç¼©æ”¾åŸå›¾ç‰‡ã€‚(å°½ç®¡ CNN æ¨¡å‹æ”¯æŒéå›ºå®šå¤§å°çš„æ¥æºï¼Œä½†ç»Ÿä¸€å¤§å°å¯ä»¥è®©åç»­çš„å¤„ç†æ›´ç®€å•ï¼Œå¹¶ä¸”ä¹Ÿå¯ä»¥æ‰¹é‡å¤„ç†å¤§å°ä¸ä¸€æ ·çš„å›¾ç‰‡ã€‚)

![03](./03.png)

è¿™ç¯‡æ–‡ç« ä¼šä½¿ç”¨ Pillow ä»£æ›¿ opencvï¼Œç¼©æ”¾å›¾ç‰‡çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

``` python
# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (256, 192)

def calc_resize_parameters(sw, sh):
    """è®¡ç®—ç¼©æ”¾å›¾ç‰‡çš„å‚æ•°"""
    sw_new, sh_new = sw, sh
    dw, dh = IMAGE_SIZE
    pad_w, pad_h = 0, 0
    if sw / sh < dw / dh:
        sw_new = int(dw / dh * sh)
        pad_w = (sw_new - sw) // 2 # å¡«å……å·¦å³
    else:
        sh_new = int(dh / dw * sw)
        pad_h = (sh_new - sh) // 2 # å¡«å……ä¸Šä¸‹
    return sw_new, sh_new, pad_w, pad_h

def resize_image(img):
    """ç¼©æ”¾å›¾ç‰‡ï¼Œæ¯”ä¾‹ä¸ä¸€è‡´æ—¶å¡«å……"""
    sw, sh = img.size
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    img_new = Image.new("RGB", (sw_new, sh_new))
    img_new.paste(img, (pad_w, pad_h))
    img_new = img_new.resize(IMAGE_SIZE)
    return img_new
```

### è®¡ç®—åŒºåŸŸç‰¹å¾

ä¸ Fast-RCNN ä¸€æ ·ï¼ŒFaster-RCNN è®¡ç®—åŒºåŸŸç‰¹å¾çš„æ—¶å€™ä¹Ÿä¼šä½¿ç”¨é™¤å»å…¨è¿æ¥å±‚çš„ CNN æ¨¡å‹ï¼Œä¾‹å¦‚ Resnet-18 æ¨¡å‹åœ¨åŸå›¾å¤§å°ä¸º 3,256,256 çš„æ—¶å€™ (3 ä»£è¡¨ RGB ä¸‰é€šé“)ä¼šè¾“å‡º 512,32,32 çš„çŸ©é˜µï¼Œé€šé“æ•°é‡å˜å¤šï¼Œé•¿å®½å˜ä¸ºåŸæœ‰çš„ 1/8ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ª 8x8 çš„åŒºåŸŸç»è¿‡å¤„ç†ä»¥åéƒ½å¯¹åº” 512 ä¸ªç‰¹å¾ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![04](./04.png)

å¯¹ CNN æ¨¡å‹ä¸ç†Ÿæ‚‰çš„å¯ä»¥å¤ä¹ è¿™ä¸ªç³»åˆ—çš„[ç¬¬å…«ç¯‡æ–‡ç« ](https://www.cnblogs.com/zkweb/p/13354826.html)ï¼Œè¯¦ç»†ä»‹ç»äº† Resnet-18 çš„ç»“æ„ä¸è®¡ç®—æµç¨‹ã€‚

ä¸Šä¸€ç¯‡æ–‡ç« çš„ Fast-RCNN ä¾‹å­æ”¹åŠ¨äº† Resnet æ¨¡å‹ä½¿å¾—è¾“å‡ºçš„ç‰¹å¾çŸ©é˜µé•¿å®½ä¸åŸå›¾ç›¸åŒï¼Œä»¥æ–¹ä¾¿åé¢æå–ç‰¹å¾ (ROI Pooling) çš„å¤„ç†ï¼Œè¿™ç¯‡å°†ä¸éœ€è¦è¿™ä¹ˆåšï¼Œè¿™ç¯‡ä½¿ç”¨çš„æ¨¡å‹ä¼šè¾“å‡ºé•¿å®½ä¸ºåŸæœ‰çš„ 1/8 çš„ç‰¹å¾çŸ©é˜µï¼Œä½†ä¸ºäº†é€‚åº”æ˜¾å­˜æ¯”è¾ƒä½çš„æœºå™¨ä¼šå‡å°‘è¾“å‡ºçš„é€šé“æ•°é‡ï¼Œå…·ä½“è¯·å‚è€ƒåé¢çš„å®ç°ä»£ç ã€‚

### å®šä¹‰é”šç‚¹ (Anchor)

Faster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œä¼šåŸºäºé”šç‚¹ (Anchor) åˆ¤æ–­æŸä¸ªåŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œä¸å¯¹è±¡ç›¸å¯¹äºé”šç‚¹çš„å½¢çŠ¶ã€‚é”šç‚¹å¯¹åº”çš„åŒºåŸŸå¤§å°å…¶å®å°±æ˜¯ä¸Šé¢ç‰¹å¾çŸ©é˜µä¸­æ¯ä¸ªç‚¹å¯¹åº”çš„åŒºåŸŸå¤§å°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![05](./05.png)

ä¸Šé¢çš„ä¾‹å­ä¸­åº”è¯¥æœ‰ 32x32 ä¸ªé”šç‚¹ï¼Œæ¯ä¸ªé”šç‚¹å¯¹åº” 512,1,1 çš„å€¼ã€‚

ä¹‹åå„ä¸ªé”šç‚¹å¯¹åº”çš„å€¼ä¼šäº¤ç»™çº¿æ€§æ¨¡å‹ï¼Œåˆ¤æ–­é”šç‚¹æ‰€åœ¨çš„åŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º (ä¸ºäº†ç®€åŒ–è¿™å¼ å›¾ç”¨äº† 4x4 ä¸ªé”šç‚¹ï¼Œçº¢è‰²çš„é”šç‚¹ä»£è¡¨åŒ…å«å¯¹è±¡)ï¼š

![06](./06.png)

å½“ç„¶çš„ï¼Œé”šç‚¹æ‰€åœ¨çš„åŒºåŸŸä¸å¯¹è±¡å®é™…æ‰€åœ¨çš„åŒºåŸŸèŒƒå›´å¹¶ä¸ä¼šå®Œå…¨ä¸€æ ·ï¼Œé”šç‚¹æ‰€åœ¨çš„åŒºåŸŸå¯èƒ½åªåŒ…å«å¯¹è±¡çš„å·¦åŠéƒ¨åˆ†ï¼Œå³åŠéƒ¨åˆ†ï¼Œæˆ–è€…ä¸­å¿ƒéƒ¨åˆ†ï¼Œå¯¹è±¡å¯èƒ½æ¯”é”šç‚¹æ‰€åœ¨åŒºåŸŸå¤§å¾ˆå¤šï¼Œä¹Ÿå¯èƒ½æ¯”é”šç‚¹æ‰€åœ¨åŒºåŸŸå°ï¼Œåªåˆ¤æ–­é”šç‚¹æ‰€åœ¨çš„åŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡å¹¶ä¸å¤Ÿå‡†ç¡®ã€‚

![07](./07.png)

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒFaster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œä¸ºæ¯ä¸ªé”šç‚¹å®šä¹‰äº†å‡ ä¸ªå›ºå®šçš„å½¢çŠ¶ï¼Œå½¢çŠ¶æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªæ˜¯å¤§å°æ¯”ä¾‹ï¼Œä¸€ä¸ªæ˜¯é•¿å®½æ¯”ä¾‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¯¹æ¯”ä¸Šé¢çš„å®é™…åŒºåŸŸå¯ä»¥å‘ç°å½¢çŠ¶ 6 å’Œå½¢çŠ¶ 7 çš„é‡å ç‡ (IOU) æ˜¯æ¯”è¾ƒé«˜çš„ï¼š

![08](./08.png)

ä¹‹ååŒºåŸŸç”Ÿæˆç½‘ç»œçš„çº¿æ€§æ¨¡å‹å¯ä»¥åˆ†åˆ«åˆ¤æ–­å„ä¸ªå½¢çŠ¶æ˜¯å¦åŒ…å«å¯¹è±¡ï¼š

![09](./09.png)

å†è¾“å‡ºå„ä¸ªå½¢çŠ¶å¯¹åº”çš„èŒƒå›´è°ƒæ•´å€¼ï¼Œå³å¯ç»™å‡ºå¯èƒ½åŒ…å«å¯¹è±¡çš„åŒºåŸŸã€‚åœ¨ä¸Šè¿°çš„ä¾‹å­ä¸­ï¼Œå¦‚æœåŒºåŸŸç”Ÿæˆç½‘ç»œå­¦ä¹ å¾—å½“ï¼Œå½¢çŠ¶ 6 å’Œå½¢çŠ¶ 7 ç»è¿‡åŒºåŸŸèŒƒå›´è°ƒæ•´ä»¥ååº”è¯¥ä¼šè¾“å‡ºå¾ˆæ¥è¿‘çš„åŒºåŸŸã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶é”šç‚¹æ”¯æŒåˆ¤æ–­æ¯”è‡ªå·±å¯¹åº”çš„åŒºåŸŸæ›´å¤§çš„èŒƒå›´æ˜¯å¦åŒ…å«å¯¹è±¡ï¼Œä½†åˆ¤æ–­çš„ä¾æ®åªæ¥æºäºè‡ªå·±å¯¹åº”çš„åŒºåŸŸã€‚ä¸¾ä¾‹æ¥è¯´å¦‚æœé”šç‚¹å¯¹åº”çš„åŒºåŸŸåªåŒ…å«é¼»å­ï¼Œé‚£ä¹ˆå®ƒå¯ä»¥åˆ¤æ–­å½¢çŠ¶ 7 å¯èƒ½åŒ…å«å¯¹è±¡ï¼Œä¹‹åå†äº¤ç»™æ ‡ç­¾åˆ†ç±»ç½‘ç»œä½œè¿›ä¸€æ­¥åˆ¤æ–­ã€‚å¦‚æœæ‰©å¤§ä»¥åå‘ç°å…¶å®ä¸æ˜¯äººè„¸ï¼Œè€Œæ˜¯åˆ«çš„ä»€ä¹ˆä¸œè¥¿ï¼Œé‚£ä¹ˆæ ‡ç­¾åˆ†ç±»ç½‘ç»œå°†ä¼šè¾“å‡º "éå¯¹è±¡" æ ‡ç­¾æ¥æ’é™¤è¿™ä¸ªåŒºåŸŸï¼Œå¦‚å‰æ–‡ä»‹ç»çš„ä¸€æ ·ã€‚

ç”Ÿæˆé”šç‚¹çš„ä»£ç å¦‚ä¸‹ï¼Œæ¯ä¸ªé”šç‚¹ä¼šå¯¹åº” `7 * 3 = 21` ä¸ªå½¢çŠ¶ï¼Œspan ä»£è¡¨ `åŸå›¾ç‰‡é•¿å®½ / CNN æ¨¡å‹è¾“å‡ºé•¿å®½`ï¼š

``` python
# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (256, 192)
# é”šç‚¹å¯¹åº”åŒºåŸŸçš„ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨
AnchorScales = (0.5, 1, 2, 3, 4, 5, 6)
# é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
AnchorAspects = ((1, 2), (1, 1), (2, 1))

def generate_anchors(span):
    """æ ¹æ®é”šç‚¹å’Œå½¢çŠ¶ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨"""
    w, h = IMAGE_SIZE
    anchors = []
    for x in range(0, w, span):
        for y in range(0, h, span):
            xcenter, ycenter = x + span / 2, y + span / 2
            for scale in AnchorScales:
                for ratio in AnchorAspects:
                    ww = span * scale * ratio[0]
                    hh = span * scale * ratio[1]
                    xx = xcenter - ww / 2
                    yy = ycenter - hh / 2
                    xx = max(int(xx), 0)
                    yy = max(int(yy), 0)
                    ww = min(int(ww), w - xx)
                    hh = min(int(hh), h - yy)
                    anchors.append((xx, yy, ww, hh))
    return anchors

Anchors = generate_anchors(8)
```

### åŒºåŸŸç”Ÿæˆç½‘ç»œ (RPN)

çœ‹å®Œä¸Šä¸€æ®µå…³äºé”šç‚¹çš„å®šä¹‰ä½ åº”è¯¥å¯¹åŒºåŸŸç”Ÿæˆç½‘ç»œçš„å·¥ä½œæ–¹å¼æœ‰ä¸ªå¤§æ¦‚çš„å°è±¡ï¼Œè¿™é‡Œæˆ‘å†ç»™å‡ºåŒºåŸŸç”Ÿæˆç½‘ç»œçš„å…·ä½“å®ç°æ¶æ„ï¼Œè¿™ä¸ªæ¶æ„è·Ÿåé¢çš„ä»£ç ä¾‹å­ç›¸åŒã€‚

![10](./10.png)

åŒºåŸŸç”Ÿæˆç½‘ç»œçš„å¤„ç†æœ¬èº«åº”è¯¥ä¸éœ€è¦å¤šè§£é‡Šäº†ğŸ¤’ï¼Œå¦‚æœè§‰å¾—éš¾ä»¥ç†è§£è¯·é‡æ–°é˜…è¯»è¿™ä¸€ç¯‡å‰é¢çš„éƒ¨åˆ†å’Œ[ä¸Šä¸€ç¯‡æ–‡ç« ](https://www.cnblogs.com/zkweb/p/14048685.html)ï¼Œç‰¹åˆ«æ˜¯ä¸Šä¸€ç¯‡æ–‡ç« çš„ä»¥ä¸‹éƒ¨åˆ†ï¼š

- æŒ‰é‡å ç‡ (IOU) åˆ¤æ–­æ¯ä¸ªåŒºåŸŸæ˜¯å¦åŒ…å«å¯¹è±¡
- è°ƒæ•´åŒºåŸŸèŒƒå›´

è®¡ç®—åŒºåŸŸèŒƒå›´åç§»çš„æŸå¤±è¿™é‡Œä½¿ç”¨äº† Smooth L1 (ä¸Šä¸€ç¯‡æ˜¯ MSELoss)ï¼Œå…·ä½“çš„è®¡ç®—æ–¹æ³•ä¼šåœ¨åé¢è®¡ç®—æŸå¤±çš„éƒ¨åˆ†ä»‹ç»ã€‚

åŒºåŸŸç”Ÿæˆç½‘ç»œæœ€ç»ˆä¼šè¾“å‡ºä¸å®šæ•°é‡çš„å¯èƒ½åŒ…å«å¯¹è±¡çš„åŒºåŸŸï¼Œæ¥ä¸‹æ¥å°±æ˜¯æå–è¿™äº›åŒºåŸŸå¯¹åº”çš„ç‰¹å¾äº†ï¼Œ**æ³¨æ„åŒºåŸŸç”Ÿæˆç½‘ç»œä½¿ç”¨çš„ç‰¹å¾å’Œæ ‡ç­¾åˆ†ç±»ç½‘ç»œä½¿ç”¨çš„ç‰¹å¾éœ€è¦åˆ†å¼€**ï¼Œå¾ˆå¤šæ–‡ç« æˆ–è€…å®ç°ä»‹ç» Faster-RCNN çš„æ—¶å€™éƒ½è®©ä¸¤ä¸ªç½‘ç»œä½¿ç”¨ç›¸åŒçš„ç‰¹å¾ï¼Œä½†ç»è¿‡æˆ‘å®æµ‹ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾ä¼šåœ¨è°ƒæ•´å‚æ•°çš„æ—¶å€™å‘ç”Ÿå¹²æ‰°å¯¼è‡´æ— æ³•å­¦ä¹ ï¼Œä¸ä¸Šä¸€ç¯‡æ–‡ç« æ­£è´Ÿæ ·æœ¬çš„æŸå¤±éœ€è¦åˆ†å¼€è®¡ç®—çš„åŸå› ä¸€æ ·ã€‚éƒ¨åˆ†å®ç°çš„ç¡®ä½¿ç”¨äº†ç›¸åŒçš„ç‰¹å¾ï¼Œä½†è¿™äº›å®ç°è°ƒæ•´å‚æ•°ä½¿ç”¨çš„ `backward` æ˜¯è‡ªå·±æ‰‹å†™çš„ï¼Œå¯èƒ½è¿™é‡Œæœ‰ä»€ä¹ˆç§˜å¯†å§ğŸ¥ºã€‚

### ä»åŒºåŸŸæå–ç‰¹å¾ - ä»¿å°„å˜æ¢ (ROI Pooling - Affine Transformation)

ä¸Šä¸€ç¯‡ä»‹ç»çš„ Fast-RCNN åœ¨ç”Ÿæˆç‰¹å¾çš„æ—¶å€™è®©é•¿å®½ä¸åŸå›¾ç‰‡ç›¸åŒï¼Œæ‰€ä»¥ ROI å±‚æå–ç‰¹å¾åªéœ€è¦ä½¿ç”¨ `[]` æ“ä½œç¬¦ï¼Œä½†è¿™ä¸€ç¯‡ç”Ÿæˆç‰¹å¾çš„æ—¶å€™é•¿å®½å˜ä¸ºäº†åŸæ¥çš„ `1/8`ï¼Œé‚£ä¹ˆéœ€è¦æ€æ ·æå–ç‰¹å¾å‘¢ï¼Ÿ

æœ€ç®€å•çš„æ–¹æ³•æ˜¯æŠŠåæ ‡å’Œé•¿å®½é™¤ä»¥ `8` å†ä½¿ç”¨ `[]` æ“ä½œç¬¦æå–ï¼Œç„¶åä½¿ç”¨ `AdaptiveMaxPool` ç¼©æ”¾åˆ°å›ºå®šçš„å¤§å°ã€‚ä½†è¿™é‡Œæˆ‘è¦ä»‹ç»ä¸€ä¸ªæ›´é«˜çº§çš„æ–¹æ³•ï¼Œå³ä»¿å°„å˜æ¢ (Affine Transformation)ï¼Œä½¿ç”¨ä»¿å°„å˜æ¢å¯ä»¥éå¸¸é«˜æ•ˆçš„å¯¹å›¾ç‰‡è¿›è¡Œæ‰¹é‡æˆªå–ã€ç¼©æ”¾ä¸æ—‹è½¬ç­‰æ“ä½œã€‚

![11](./11.png)

ä»¿å°„å˜æ¢çš„åŸç†æ˜¯ç»™åŸå›¾ç‰‡å’Œè¾“å‡ºå›¾ç‰‡ä¹‹é—´çš„åƒç´ åæ ‡å»ºç«‹å¯¹åº”å…³ç³»ï¼Œä¸€å…±æœ‰ 6 ä¸ªå‚æ•°ï¼Œå…¶ä¸­ 4 ä¸ªå‚æ•°ç”¨äºç»™åæ ‡åšçŸ©é˜µä¹˜æ³• (æ”¯æŒç¼©æ”¾ä¸æ—‹è½¬ç­‰å˜å½¢æ“ä½œ)ï¼Œ2 ä¸ªå‚æ•°ç”¨äºåšå®ŒçŸ©é˜µä¹˜æ³•ä»¥åç›¸åŠ  (æ”¯æŒå¹³ç§»ç­‰æ“ä½œ)ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

![12](./12.png)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä»¿å°„å˜æ¢é‡Œé¢ä¸ä¼šç›´æ¥è®¡ç®—åæ ‡çš„ç»å¯¹å€¼ï¼Œè€Œæ˜¯æŠŠå›¾ç‰‡çš„å·¦ä¸Šè§’å½“ä½œ `(-1, -1)`ï¼Œå³ä¸‹è§’å½“ä½œ `(1, 1)` ç„¶åè½¬æ¢åæ ‡åˆ°è¿™ä¸ªå°ºåº¦é‡Œé¢ï¼Œå†è¿›è¡Œè®¡ç®—ã€‚

ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœæƒ³æŠŠåŸå›¾ç‰‡çš„ä¸­å¿ƒéƒ¨åˆ†æ”¾å¤§ä¸¤å€åˆ°è¾“å‡ºå›¾ç‰‡ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼š

``` text
0.5,   0, 0
  0, 0.5, 0
```

æ•ˆæœå¦‚ä¸‹ï¼Œå¦‚æœä½ æ‹¿è¾“å‡ºå›¾ç‰‡çš„å››ä¸ªè§’çš„åæ ‡ç»“åˆä¸Šé¢çš„å‚æ•°è®¡ç®—ï¼Œå¯ä»¥å¾—å‡ºåŸå›¾ä¸­å¿ƒéƒ¨åˆ†çš„èŒƒå›´ï¼š

![13](./13.png)

æ›´å¤šä¾‹å­å¯ä»¥å‚è€ƒ[è¿™ç¯‡æ–‡ç« ](https://www.jianshu.com/p/723af68beb2e)ï¼Œå¯¹ç†è§£ä»¿å°„å˜æ¢éå¸¸æœ‰å¸®åŠ©ã€‚

é‚£ä¹ˆä»åŒºåŸŸæå–ç‰¹å¾çš„æ—¶å€™ï¼Œåº”è¯¥ä½¿ç”¨æ€æ ·çš„å‚æ•°å‘¢ï¼Ÿè®¡ç®—å‚æ•°çš„å…¬å¼æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ğŸ˜«ï¼š

![14](./14.png)

ä½¿ç”¨ pytorch å®ç°å¦‚ä¸‹ï¼Œæ³¨æ„ pytorch çš„ä»¿å°„å˜æ¢è¦æ±‚æ•°æ®ç»´åº¦æ˜¯ `(C, H, W)`ï¼Œè€Œæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®ç»´åº¦æ˜¯ `(C, W, H)`ï¼Œæ‰€ä»¥éœ€è¦è°ƒæ¢å‚æ•°çš„ä½ç½®ï¼Œ`pooling_size` ä»£è¡¨è¾“å‡ºå›¾ç‰‡çš„å¤§å°ï¼Œè¿™æ ·ä»¿å°„å˜æ¢ä¸ä»…å¯ä»¥æˆªå–èŒƒå›´è¿˜èƒ½å¸®æˆ‘ä»¬ç¼©æ”¾åˆ°æŒ‡å®šçš„å¤§å°ï¼š

``` python
# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (256, 192)

def roi_crop(features, rois, pooling_size):
    """æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾ï¼Œæ¯æ¬¡åªèƒ½å¤„ç†å•å¼ å›¾ç‰‡"""
    width, height = IMAGE_SIZE
    theta = []
    results = []
    for roi in rois:
        x1, y1, w, h = roi
        x2, y2 = x1 + w, y1 + h
        theta = [[
            [
                (y2 - y1) / height,
                0,
                (y2 + y1) / height - 1
            ],
            [
                0,
                (x2 - x1) / width,
                (x2 + x1) / width - 1
            ]
        ]]
        theta_tensor = torch.tensor(theta)
        grid = nn.functional.affine_grid(
            theta_tensor,
            torch.Size((1, 1, pooling_size, pooling_size)),
            align_corners=False).to(device)
        result = nn.functional.grid_sample(
            features.unsqueeze(0), grid, align_corners=False)
        results.append(result)
    if not results:
        return None
    results = torch.cat(results, dim=0)
    return results
```

å¦‚æœ `pooling_size` ä¸º 7ï¼Œé‚£ä¹ˆ results çš„ç»´åº¦å°±æ˜¯ `èŒƒå›´çš„æ•°é‡, 7, 7`ã€‚

ä»¿å°„å˜æ¢æœ¬æ¥æ˜¯ç”¨åœ¨ STN ç½‘ç»œé‡Œçš„ï¼Œç”¨äºæŠŠæ—‹è½¬å˜å½¢ä»¥åçš„å›¾ç‰‡è¿˜åŸï¼Œå¦‚æœä½ æœ‰å…´è¶£å¯ä»¥å‚è€ƒ[è¿™é‡Œ](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html)ã€‚

### æ ¹æ®ç‰¹å¾è¯†åˆ«åˆ†ç±»

æ¥ä¸‹æ¥å°±æ˜¯æ ¹æ®ç‰¹å¾è¯†åˆ«åˆ†ç±»äº†ğŸ¥³ï¼Œå¤„ç†ä¸Šä¸ä¹‹å‰çš„ Fast-RCNN åŸºæœ¬ä¸Šç›¸åŒï¼Œé™¤äº† Faster-RCNN åœ¨ç”ŸæˆèŒƒå›´è°ƒæ•´å‚æ•°çš„æ—¶å€™ä¼šé’ˆå¯¹æ¯ä¸ªåˆ†ç±»åˆ†åˆ«ç”Ÿæˆï¼Œå¦‚æœæœ‰ 5 ä¸ªåˆ†ç±»ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰ `5 * 4 = 20` ä¸ªè¾“å‡ºï¼Œè¿™ä¼šè®©èŒƒå›´è°ƒæ•´å˜å¾—æ›´å‡†ç¡®ã€‚

æ ‡ç­¾åˆ†ç±»ç½‘ç»œçš„å…·ä½“å®ç°æ¶æ„å¦‚ä¸‹ï¼Œæœ€ç»ˆä¼šè¾“å‡ºåŒ…å«å¯¹è±¡çš„èŒƒå›´ä¸å„ä¸ªèŒƒå›´å¯¹åº”çš„åˆ†ç±»ï¼Œæ•´ä¸ª Faster-RCNN çš„å¤„ç†å°±åˆ°æ­¤ä¸ºæ­¢äº†ğŸ˜¤ã€‚

![15](./15.png)

æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ ‡ç­¾åˆ†ç±»ç½‘ç»œä½¿ç”¨çš„åˆ†ç±»éœ€è¦é¢å¤–åŒ…å«ä¸€ä¸ª "éå¯¹è±¡" åˆ†ç±»ï¼Œä¾‹å¦‚åŸæœ‰åˆ†ç±»åˆ—è¡¨ä¸º `[æˆ´å£ç½©äººè„¸ï¼Œä¸æˆ´å£ç½©äººè„¸]` æ—¶ï¼Œå®é™…åˆ¤æ–­åˆ†ç±»åˆ—è¡¨åº”è¯¥ä¸º `[éäººè„¸, æˆ´å£ç½©äººè„¸ï¼Œä¸æˆ´å£ç½©äººè„¸]`ã€‚è¿™æ˜¯å› ä¸ºæ ‡ç­¾åˆ†ç±»ç½‘ç»œçš„ç‰¹å¾æˆªå–èŒƒå›´æ¯”åŒºåŸŸç”Ÿæˆç½‘ç»œè¦å¤§ï¼ŒèŒƒå›´ä¹Ÿæ›´å‡†ç¡®ï¼Œæ ‡ç­¾èŒƒå›´ç½‘ç»œå¯ä»¥æ ¹æ®æ›´å‡†ç¡®çš„ç‰¹å¾æ¥æ’é™¤é‚£äº›åŒºåŸŸç”Ÿæˆç½‘ç»œä»¥ä¸ºæ˜¯å¯¹è±¡ä½†å®é™…ä¸æ˜¯å¯¹è±¡çš„èŒƒå›´ã€‚

### è®¡ç®—æŸå¤±

åˆ°æ­¤ä¸ºæ­¢æˆ‘ä»¬çœ‹åˆ°äº†ä»¥ä¸‹çš„æŸå¤±ï¼š

- åŒºåŸŸç”Ÿæˆç½‘ç»œåˆ¤æ–­æ˜¯å¦å¯¹è±¡çš„æŸå¤±
- åŒºåŸŸç”Ÿæˆç½‘ç»œçš„èŒƒå›´è°ƒæ•´å‚æ•°çš„æŸå¤± (ä»…é’ˆå¯¹æ˜¯å¯¹è±¡çš„èŒƒå›´è®¡ç®—)
- æ ‡ç­¾åˆ†ç±»ç½‘ç»œåˆ¤æ–­å¯¹è±¡æ‰€å±åˆ†ç±»çš„æŸå¤±
- æ ‡ç­¾åˆ†ç±»ç½‘ç»œçš„èŒƒå›´è°ƒæ•´å‚æ•°çš„æŸå¤± (ä»…é’ˆå¯¹æ˜¯å¯¹è±¡ï¼Œå¹¶ä¸”å¯èƒ½æ€§æœ€å¤§çš„åˆ†ç±»è®¡ç®—)

è¿™äº›æŸå¤±å¯ä»¥é€šè¿‡ `+` åˆå¹¶ï¼Œç„¶åå†é€šè¿‡ `backward` åé¦ˆåˆ°å„ä¸ªç½‘ç»œçš„ CNN æ¨¡å‹ä¸çº¿æ€§æ¨¡å‹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ‰¹é‡è®­ç»ƒçš„æ—¶å€™å› ä¸ºå„ä¸ªå›¾ç‰‡çš„è¾“å‡ºèŒƒå›´æ•°é‡ä¸ä¸€æ ·ï¼Œä¸Šé¢çš„æŸå¤±ä¼šå…ˆæ ¹æ®å„å¼ å›¾ç‰‡è®¡ç®—åå†å¹³å‡ã€‚ä½ å¯èƒ½è®°å¾—ä¸Šä¸€ç¯‡ Fast-RCNN è®¡ç®—æŸå¤±çš„æ—¶å€™éœ€è¦æ ¹æ®æ­£è´Ÿæ ·æœ¬åˆ†åˆ«è®¡ç®—ï¼Œè¿™ä¸€ç¯‡ä¸éœ€è¦ï¼ŒFaster-RCNN çš„åŒºåŸŸç”Ÿæˆç½‘ç»œè¾“å‡ºçš„èŒƒå›´æ¯”è¾ƒå‡†ç¡®ï¼Œå¾ˆå°‘ä¼šå‡ºç°æ¥æºç‰¹å¾ç›¸åŒä½†åŒæ—¶è¾“å‡º "æ˜¯å¯¹è±¡" å’Œ "éå¯¹è±¡" ç»“æœçš„æƒ…å†µã€‚æ­¤å¤–ï¼Œå¦‚å‰æ–‡æ‰€æåˆ°çš„ï¼ŒåŒºåŸŸç”Ÿæˆç½‘ç»œä¸æ ‡ç­¾åˆ†ç±»ç½‘ç»œåº”è¯¥ä½¿ç”¨ä¸åŒçš„ CNN æ¨¡å‹ç”Ÿæˆä¸åŒçš„ç‰¹å¾ï¼Œä»¥é¿å…é€šè¿‡æŸå¤±è°ƒæ•´æ¨¡å‹å‚æ•°æ—¶å‘ç”Ÿå¹²æ‰°ã€‚

è®¡ç®—èŒƒå›´è°ƒæ•´æŸå¤±çš„æ—¶å€™ç”¨çš„æ˜¯ `Smooth L1` å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æˆ‘ä»¬ä¹‹å‰æ²¡æœ‰çœ‹åˆ°è¿‡ï¼Œæ‰€ä»¥æˆ‘å†ç®€å•ä»‹ç»ä¸€ä¸‹å®ƒçš„è®¡ç®—æ–¹æ³•ï¼š

![16](./16.png)

ç®€å•çš„æ¥è¯´å°±æ˜¯å¦‚æœé¢„æµ‹è¾“å‡ºå’Œå®é™…è¾“å‡ºä¹‹é—´çš„å·®è·æ¯”è¾ƒå°çš„æ—¶å€™ï¼Œåè¿‡æ¥å¢åŠ æŸå¤±ä½¿å¾—è°ƒæ•´é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºåŒºåŸŸèŒƒå›´åç§»éœ€è¦è®©é¢„æµ‹è¾“å‡ºåœ¨æ•°å€¼ä¸Šæ›´æ¥è¿‘å®é™…è¾“å‡º (ä¸åƒæ ‡ç­¾åˆ†ç±»å¯ä»¥åªè°ƒæ•´æ–¹å‘ä¸ç®¡å…·ä½“å€¼)ï¼Œä½¿ç”¨ `Smooth L1` è°ƒæ•´èµ·æ¥æ•ˆæœä¼šæ›´å¥½ã€‚

### åˆå¹¶ç»“æœåŒºåŸŸ

Faster-RCNN å¯èƒ½ä¼šé’ˆå¯¹åŒä¸€ä¸ªå¯¹è±¡è¾“å‡ºå¤šä¸ªé‡åˆçš„èŒƒå›´ï¼Œä½†å› ä¸º Faster-RCNN çš„ç²¾ç¡®åº¦æ¯”è¾ƒé«˜ï¼Œè¿™äº›é‡åˆçš„èŒƒå›´çš„é‡å ç‡åº”è¯¥ä¹Ÿæ¯”è¾ƒé«˜ï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆè¿™äº›èŒƒå›´å¾—å‡ºç»“æœèŒƒå›´ï¼š

![17](./17.png)

å¥½äº†ï¼Œå¯¹ Faster-RCNN çš„ä»‹ç»å°±åˆ°æ­¤ä¸ºæ­¢äº†ğŸ¤—ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹ä»£ç å®ç°å§ã€‚

## ä½¿ç”¨ Faster-RCNN è¯†åˆ«äººè„¸ä½ç½®ä¸æ˜¯å¦æˆ´å£ç½©

è¿™æ¬¡çš„ä»»åŠ¡æ˜¯è¯†åˆ«å›¾ç‰‡ä¸­äººè„¸çš„ä½ç½®ï¼Œä¸åˆ¤æ–­æ˜¯å¦æœ‰æ­£ç¡®ä½©æˆ´å£ç½©ï¼Œä¸€å…±æœ‰ä»¥ä¸‹çš„åˆ†ç±»ï¼š

- éäººè„¸: other
- æˆ´å£ç½©: with_mask
- æ²¡æˆ´å£ç½©: without_mask
- æˆ´äº†å£ç½©ä½†å§¿åŠ¿ä¸æ­£ç¡®: mask_weared_incorrect

è®­ç»ƒä½¿ç”¨çš„æ•°æ®ä¹Ÿæ˜¯æ¥æºäº kaggleï¼Œä¸‹è½½éœ€è¦æ³¨å†Œå¸å·ä½†ä¸ç”¨ç»™é’±ï¼š

https://www.kaggle.com/andrewmvd/face-mask-detection

ä¾‹å¦‚ä¸‹é¢è¿™å¼ å›¾ç‰‡ï¼š

![maksssksksss0](./maksssksksss0.png)

å¯¹åº”ä»¥ä¸‹çš„æ ‡è®° (xml æ ¼å¼)ï¼š

``` xml
<annotation>
    <folder>images</folder>
    <filename>maksssksksss0.png</filename>
    <size>
        <width>512</width>
        <height>366</height>
        <depth>3</depth>
    </size>
    <segmented>0</segmented>
    <object>
        <name>without_mask</name>
        <pose>Unspecified</pose>
        <truncated>0</truncated>
        <occluded>0</occluded>
        <difficult>0</difficult>
        <bndbox>
            <xmin>79</xmin>
            <ymin>105</ymin>
            <xmax>109</xmax>
            <ymax>142</ymax>
        </bndbox>
    </object>
    <object>
        <name>with_mask</name>
        <pose>Unspecified</pose>
        <truncated>0</truncated>
        <occluded>0</occluded>
        <difficult>0</difficult>
        <bndbox>
            <xmin>185</xmin>
            <ymin>100</ymin>
            <xmax>226</xmax>
            <ymax>144</ymax>
        </bndbox>
    </object>
    <object>
        <name>without_mask</name>
        <pose>Unspecified</pose>
        <truncated>0</truncated>
        <occluded>0</occluded>
        <difficult>0</difficult>
        <bndbox>
            <xmin>325</xmin>
            <ymin>90</ymin>
            <xmax>360</xmax>
            <ymax>141</ymax>
        </bndbox>
    </object>
</annotation>
```

ä½¿ç”¨ Faster-RCNN è®­ç»ƒä¸è¯†åˆ«çš„ä»£ç å¦‚ä¸‹ğŸ˜ˆï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
import random
import numpy
import math
import pandas
import json
from PIL import Image
from PIL import ImageDraw
from torch import nn
from matplotlib import pyplot
from collections import defaultdict
import xml.etree.cElementTree as ET
from collections import Counter

# ç¼©æ”¾å›¾ç‰‡çš„å¤§å°
IMAGE_SIZE = (256, 192)
# åˆ†æç›®æ ‡çš„å›¾ç‰‡æ‰€åœ¨çš„æ–‡ä»¶å¤¹
IMAGE_DIR = "./archive/images"
# å®šä¹‰å„ä¸ªå›¾ç‰‡ä¸­äººè„¸åŒºåŸŸä¸åˆ†ç±»çš„ CSV æ–‡ä»¶
ANNOTATION_DIR = "./archive/annotations"
# åˆ†ç±»åˆ—è¡¨
CLASSES = [ "other", "with_mask", "without_mask", "mask_weared_incorrect" ]
CLASSES_MAPPING = { c: index for index, c in enumerate(CLASSES) }
# åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¯¹è±¡ä½¿ç”¨çš„åŒºåŸŸé‡å ç‡çš„é˜ˆå€¼
IOU_POSITIVE_THRESHOLD = 0.35
IOU_NEGATIVE_THRESHOLD = 0.10
# åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶é‡å åŒºåŸŸçš„é‡å ç‡é˜ˆå€¼
IOU_MERGE_THRESHOLD = 0.35

# ç”¨äºå¯ç”¨ GPU æ”¯æŒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BasicBlock(nn.Module):
    """ResNet ä½¿ç”¨çš„åŸºç¡€å—"""
    expansion = 1 # å®šä¹‰è¿™ä¸ªå—çš„å®é™…å‡ºé€šé“æ˜¯ channels_out çš„å‡ å€ï¼Œè¿™é‡Œçš„å®ç°å›ºå®šæ˜¯ä¸€å€
    def __init__(self, channels_in, channels_out, stride):
        super().__init__()
        # ç”Ÿæˆ 3x3 çš„å·ç§¯å±‚
        # å¤„ç†é—´éš” stride = 1 æ—¶ï¼Œè¾“å‡ºçš„é•¿å®½ä¼šç­‰äºè¾“å…¥çš„é•¿å®½ï¼Œä¾‹å¦‚ (32-3+2)//1+1 == 32
        # å¤„ç†é—´éš” stride = 2 æ—¶ï¼Œè¾“å‡ºçš„é•¿å®½ä¼šç­‰äºè¾“å…¥çš„é•¿å®½çš„ä¸€åŠï¼Œä¾‹å¦‚ (32-3+2)//2+1 == 16
        # æ­¤å¤– resnet çš„ 3x3 å·ç§¯å±‚ä¸ä½¿ç”¨åç§»å€¼ bias
        self.conv1 = nn.Sequential(
            nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(channels_out))
        # å†å®šä¹‰ä¸€ä¸ªè®©è¾“å‡ºå’Œè¾“å…¥ç»´åº¦ç›¸åŒçš„ 3x3 å·ç§¯å±‚
        self.conv2 = nn.Sequential(
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(channels_out))
        # è®©åŸå§‹è¾“å…¥å’Œè¾“å‡ºç›¸åŠ çš„æ—¶å€™ï¼Œéœ€è¦ç»´åº¦ä¸€è‡´ï¼Œå¦‚æœç»´åº¦ä¸ä¸€è‡´åˆ™éœ€è¦æ•´åˆ
        self.identity = nn.Sequential()
        if stride != 1 or channels_in != channels_out * self.expansion:
            self.identity = nn.Sequential(
                nn.Conv2d(channels_in, channels_out * self.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(channels_out * self.expansion))

    def forward(self, x):
        # x => conv1 => relu => conv2 => + => relu
        # |                              ^
        # |==============================|
        tmp = self.conv1(x)
        tmp = nn.functional.relu(tmp, inplace=True)
        tmp = self.conv2(tmp)
        tmp += self.identity(x)
        y = nn.functional.relu(tmp, inplace=True)
        return y

class MyModel(nn.Module):
    """Faster-RCNN (åŸºäº ResNet-18 çš„å˜ç§)"""
    Anchors = None # é”šç‚¹åˆ—è¡¨ï¼ŒåŒ…å« é”šç‚¹æ•°é‡ * å½¢çŠ¶æ•°é‡ çš„èŒƒå›´
    AnchorSpan = 8 # é”šç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œåº”è¯¥ç­‰äºåŸæœ‰é•¿å®½ / resnet è¾“å‡ºé•¿å®½
    AnchorScales = (0.5, 1, 2, 3, 4, 5, 6) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨
    AnchorAspects = ((1, 2), (1, 1), (2, 1)) # é”šç‚¹å¯¹åº”åŒºåŸŸçš„é•¿å®½æ¯”ä¾‹åˆ—è¡¨
    AnchorBoxes = len(AnchorScales) * len(AnchorAspects) # æ¯ä¸ªé”šç‚¹å¯¹åº”çš„å½¢çŠ¶æ•°é‡

    def __init__(self):
        super().__init__()
        # æŠ½å–å›¾ç‰‡å„ä¸ªåŒºåŸŸç‰¹å¾çš„ ResNet (é™¤å» AvgPool å’Œå…¨è¿æ¥å±‚)
        # å’Œ Fast-RCNN ä¾‹å­ä¸åŒçš„æ˜¯è¾“å‡ºçš„é•¿å®½ä¼šæ˜¯åŸæœ‰çš„ 1/8ï¼Œåé¢ä¼šæ ¹æ®é”šç‚¹ä¸ affine_grid æˆªå–åŒºåŸŸ
        # æ­¤å¤–ï¼Œä¸ºäº†å¯ä»¥è®©æ¨¡å‹è·‘åœ¨ 4GB æ˜¾å­˜ä¸Šï¼Œè¿™é‡Œå‡å°‘äº†æ¨¡å‹çš„é€šé“æ•°é‡
        # æ³¨æ„:
        # RPN ä½¿ç”¨çš„æ¨¡å‹å’Œæ ‡ç­¾åˆ†ç±»ä½¿ç”¨çš„æ¨¡å‹éœ€è¦åˆ†å¼€ï¼Œå¦åˆ™ä¼šå‡ºç°æ— æ³•å­¦ä¹  (RPN æ€»æ˜¯è¾“å‡ºè´Ÿ) çš„é—®é¢˜
        self.previous_channels_out = 4
        self.rpn_resnet = nn.Sequential(
            nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.previous_channels_out),
            nn.ReLU(inplace=True),
            self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=1),
            self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2))
        self.previous_channels_out = 4
        self.cls_resnet = nn.Sequential(
            nn.Conv2d(3, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.previous_channels_out),
            nn.ReLU(inplace=True),
            self._make_layer(BasicBlock, channels_out=16, num_blocks=2, stride=1),
            self._make_layer(BasicBlock, channels_out=32, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=64, num_blocks=2, stride=2),
            self._make_layer(BasicBlock, channels_out=128, num_blocks=2, stride=2))
        self.features_channels = 128
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„å¯¹è±¡å¯èƒ½æ€§çš„æ¨¡å‹
        self.rpn_labels_model = nn.Sequential(
            nn.Linear(self.features_channels, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, MyModel.AnchorBoxes*2))
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„åŒºåŸŸåç§»çš„æ¨¡å‹
        self.rpn_offsets_model = nn.Sequential(
            nn.Linear(self.features_channels, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, MyModel.AnchorBoxes*4))
        # é€‰å–å¯èƒ½å‡ºç°å¯¹è±¡çš„åŒºåŸŸéœ€è¦çš„æœ€å°å¯èƒ½æ€§
        self.rpn_score_threshold = 0.9
        # æ¯å¼ å›¾ç‰‡æœ€å¤šé€‰å–çš„åŒºåŸŸåˆ—è¡¨
        self.rpn_max_candidates = 32
        # æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾åç¼©æ”¾åˆ°çš„å¤§å°
        self.pooling_size = 7
        # æ ¹æ®åŒºåŸŸç‰¹å¾åˆ¤æ–­åˆ†ç±»çš„æ¨¡å‹
        self.cls_labels_model = nn.Sequential(
            nn.Linear(self.features_channels * (self.pooling_size ** 2), 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, len(CLASSES)))
        # æ ¹æ®åŒºåŸŸç‰¹å¾å†æ¬¡ç”ŸæˆåŒºåŸŸåç§»çš„æ¨¡å‹ï¼Œæ³¨æ„åŒºåŸŸåç§»ä¼šé’ˆå¯¹å„ä¸ªåˆ†ç±»åˆ†åˆ«ç”Ÿæˆ
        self.cls_offsets_model = nn.Sequential(
            nn.Linear(self.features_channels * (self.pooling_size ** 2), 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, len(CLASSES)*4))

    def _make_layer(self, block_type, channels_out, num_blocks, stride):
        """åˆ›å»º resnet ä½¿ç”¨çš„å±‚"""
        blocks = []
        # æ·»åŠ ç¬¬ä¸€ä¸ªå—
        blocks.append(block_type(self.previous_channels_out, channels_out, stride))
        self.previous_channels_out = channels_out * block_type.expansion
        # æ·»åŠ å‰©ä½™çš„å—ï¼Œå‰©ä½™çš„å—å›ºå®šå¤„ç†é—´éš”ä¸º 1ï¼Œä¸ä¼šæ”¹å˜é•¿å®½
        for _ in range(num_blocks-1):
            blocks.append(block_type(self.previous_channels_out, self.previous_channels_out, 1))
            self.previous_channels_out *= block_type.expansion
        return nn.Sequential(*blocks)

    @staticmethod
    def _generate_anchors(span):
        """æ ¹æ®é”šç‚¹å’Œå½¢çŠ¶ç”Ÿæˆé”šç‚¹èŒƒå›´åˆ—è¡¨"""
        w, h = IMAGE_SIZE
        anchors = []
        for x in range(0, w, span):
            for y in range(0, h, span):
                xcenter, ycenter = x + span / 2, y + span / 2
                for scale in MyModel.AnchorScales:
                    for ratio in MyModel.AnchorAspects:
                        ww = span * scale * ratio[0]
                        hh = span * scale * ratio[1]
                        xx = xcenter - ww / 2
                        yy = ycenter - hh / 2
                        xx = max(int(xx), 0)
                        yy = max(int(yy), 0)
                        ww = min(int(ww), w - xx)
                        hh = min(int(hh), h - yy)
                        anchors.append((xx, yy, ww, hh))
        return anchors

    @staticmethod
    def _roi_crop(features, rois, pooling_size):
        """æ ¹æ®åŒºåŸŸæˆªå–ç‰¹å¾ï¼Œæ¯æ¬¡åªèƒ½å¤„ç†å•å¼ å›¾ç‰‡"""
        width, height = IMAGE_SIZE
        theta = []
        results = []
        for roi in rois:
            x1, y1, w, h = roi
            x2, y2 = x1 + w, y1 + h
            theta = [[
                [
                    (y2 - y1) / height,
                    0,
                    (y2 + y1) / height - 1
                ],
                [
                    0,
                    (x2 - x1) / width,
                    (x2 + x1) / width - 1
                ]
            ]]
            theta_tensor = torch.tensor(theta)
            grid = nn.functional.affine_grid(
                theta_tensor,
                torch.Size((1, 1, pooling_size, pooling_size)),
                align_corners=False).to(device)
            result = nn.functional.grid_sample(
                features.unsqueeze(0), grid, align_corners=False)
            results.append(result)
        if not results:
            return None
        results = torch.cat(results, dim=0)
        return results

    def forward(self, x):
        # ***** æŠ½å–ç‰¹å¾éƒ¨åˆ† *****
        # åˆ†åˆ«æŠ½å– RPN å’Œæ ‡ç­¾åˆ†ç±»ä½¿ç”¨çš„ç‰¹å¾
        # ç»´åº¦æ˜¯ B,128,W/8,H/8
        rpn_features_original = self.rpn_resnet(x)
        # ç»´åº¦æ˜¯ B*W/8*H/8,128 (æŠŠé€šé“æ”¾åœ¨æœ€åï¼Œç”¨äºä¼ ç»™çº¿æ€§æ¨¡å‹)
        rpn_features = rpn_features_original.permute(0, 2, 3, 1).reshape(-1, self.features_channels)
        # ç»´åº¦æ˜¯ B,128,W/8,H/8
        cls_features = self.cls_resnet(x)

        # ***** é€‰å–åŒºåŸŸéƒ¨åˆ† *****
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„å¯¹è±¡å¯èƒ½æ€§
        # ç»´åº¦æ˜¯ B,W/8*H/8*AnchorBoxes,2
        rpn_labels = self.rpn_labels_model(rpn_features)
        rpn_labels = rpn_labels.reshape(
            rpn_features_original.shape[0],
            rpn_features_original.shape[2] * rpn_features_original.shape[3] * MyModel.AnchorBoxes,
            2)
        # æ ¹æ®åŒºåŸŸç‰¹å¾ç”Ÿæˆå„ä¸ªé”šç‚¹å¯¹åº”çš„åŒºåŸŸåç§»
        # ç»´åº¦æ˜¯ B,W/8*H/8*AnchorBoxes,4
        rpn_offsets = self.rpn_offsets_model(rpn_features)
        rpn_offsets = rpn_offsets.reshape(
            rpn_features_original.shape[0],
            rpn_features_original.shape[2] * rpn_features_original.shape[3] * MyModel.AnchorBoxes,
            4)
        # é€‰å–å¯èƒ½å‡ºç°å¯¹è±¡çš„åŒºåŸŸï¼Œå¹¶è°ƒæ•´åŒºåŸŸèŒƒå›´
        with torch.no_grad():
            rpn_scores = nn.functional.softmax(rpn_labels, dim=2)[:,:,1]
            # é€‰å–å¯èƒ½æ€§æœ€é«˜çš„éƒ¨åˆ†åŒºåŸŸ
            rpn_top_scores = torch.topk(rpn_scores, k=self.rpn_max_candidates, dim=1)
            rpn_candidates_batch = []
            for x in range(0, rpn_scores.shape[0]):
                rpn_candidates = []
                for score, index in zip(rpn_top_scores.values[x], rpn_top_scores.indices[x]):
                    # è¿‡æ»¤å¯èƒ½æ€§ä½äºæŒ‡å®šé˜ˆå€¼çš„åŒºåŸŸ
                    if score.item() < self.rpn_score_threshold:
                        continue
                    anchor_box = MyModel.Anchors[index.item()]
                    offset = rpn_offsets[x,index.item()].tolist()
                    # è°ƒæ•´åŒºåŸŸèŒƒå›´
                    candidate_box = adjust_box_by_offset(anchor_box, offset)
                    rpn_candidates.append(candidate_box)
                rpn_candidates_batch.append(rpn_candidates)

        # ***** åˆ¤æ–­åˆ†ç±»éƒ¨åˆ† *****
        cls_output = []
        cls_result = []
        for index in range(0, cls_features.shape[0]):
            pooled = MyModel._roi_crop(
                cls_features[index], rpn_candidates_batch[index], self.pooling_size)
            if pooled is None:
                # æ²¡æœ‰æ‰¾åˆ°å¯èƒ½åŒ…å«å¯¹è±¡çš„åŒºåŸŸ
                cls_output.append(None)
                cls_result.append(None)
                continue
            pooled = pooled.reshape(pooled.shape[0], -1)
            labels = self.cls_labels_model(pooled)
            offsets = self.cls_offsets_model(pooled)
            cls_output.append((labels, offsets))
            # ä½¿ç”¨ softmax åˆ¤æ–­å¯èƒ½æ€§æœ€å¤§çš„åˆ†ç±»
            classes = nn.functional.softmax(labels, dim=1).max(dim=1).indices
            # æ ¹æ®åˆ†ç±»å¯¹åº”çš„åç§»å†æ¬¡è°ƒæ•´åŒºåŸŸèŒƒå›´
            offsets_map = offsets.reshape(offsets.shape[0] * len(CLASSES), 4)
            result = []
            for box_index in range(0, classes.shape[0]):
                predicted_label = classes[box_index].item()
                if predicted_label == 0:
                    continue # 0 ä»£è¡¨ other, è¡¨ç¤ºéå¯¹è±¡
                candidate_box = rpn_candidates_batch[index][box_index]
                offset = offsets_map[box_index * len(CLASSES) + predicted_label].tolist()
                predicted_box = adjust_box_by_offset(candidate_box, offset)
                # æ·»åŠ åˆ†ç±»ä¸æœ€ç»ˆé¢„æµ‹åŒºåŸŸ
                result.append((predicted_label, predicted_box))
            cls_result.append(result)

        # å‰é¢çš„é¡¹ç›®ç”¨äºå­¦ä¹ ï¼Œæœ€åä¸€é¡¹æ˜¯æœ€ç»ˆè¾“å‡ºç»“æœ
        return rpn_labels, rpn_offsets, rpn_candidates_batch, cls_output, cls_result

    @staticmethod
    def loss_function(predicted, actual):
        """Faster-RCNN ä½¿ç”¨çš„å¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨"""
        rpn_labels, rpn_offsets, rpn_candidates_batch, cls_output, _ = predicted
        rpn_labels_losses = []
        rpn_offsets_losses = []
        cls_labels_losses = []
        cls_offsets_losses = []
        for batch_index in range(len(actual)):
            # è®¡ç®— RPN çš„æŸå¤±
            (true_boxes_labels,
                actual_rpn_labels, actual_rpn_labels_mask,
                actual_rpn_offsets, actual_rpn_offsets_mask) = actual[batch_index]
            rpn_labels_losses.append(nn.functional.cross_entropy(
                rpn_labels[batch_index][actual_rpn_labels_mask],
                actual_rpn_labels.to(device)))
            rpn_offsets_losses.append(nn.functional.smooth_l1_loss(
                rpn_offsets[batch_index][actual_rpn_offsets_mask],
                actual_rpn_offsets.to(device)))
            # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æŸå¤±
            if cls_output[batch_index] is None:
                continue
            cls_labels_mask = []
            cls_offsets_mask = []
            cls_actual_labels = []
            cls_actual_offsets = []
            cls_predicted_labels, cls_predicted_offsets = cls_output[batch_index]
            cls_predicted_offsets_map = cls_predicted_offsets.reshape(-1, 4)
            rpn_candidates = rpn_candidates_batch[batch_index]
            for box_index, candidate_box in enumerate(rpn_candidates):
                iou_list = [ calc_iou(candidate_box, true_box) for (_, true_box) in true_boxes_labels ]
                positive_index = next((index for index, iou in enumerate(iou_list) if iou > IOU_POSITIVE_THRESHOLD), None)
                is_negative = all(iou < IOU_NEGATIVE_THRESHOLD for iou in iou_list)
                if positive_index is not None:
                    true_label, true_box = true_boxes_labels[positive_index]
                    cls_actual_labels.append(true_label)
                    cls_labels_mask.append(box_index)
                    # å¦‚æœåŒºåŸŸæ­£ç¡®ï¼Œåˆ™å­¦ä¹ çœŸå®åˆ†ç±»å¯¹åº”çš„åŒºåŸŸåç§»
                    cls_actual_offsets.append(calc_box_offset(candidate_box, true_box))
                    cls_offsets_mask.append(box_index * len(CLASSES) + true_label)
                elif is_negative:
                    cls_actual_labels.append(0) # 0 ä»£è¡¨ other, è¡¨ç¤ºéå¯¹è±¡
                    cls_labels_mask.append(box_index)
                # å¦‚æœå€™é€‰åŒºåŸŸä¸çœŸå®åŒºåŸŸçš„é‡å ç‡ä»‹äºä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´ï¼Œåˆ™ä¸å‚ä¸å­¦ä¹ 
            if cls_labels_mask:
                cls_labels_losses.append(nn.functional.cross_entropy(
                    cls_predicted_labels[cls_labels_mask],
                    torch.tensor(cls_actual_labels).to(device)))
            if cls_offsets_mask:
                cls_offsets_losses.append(nn.functional.smooth_l1_loss(
                    cls_predicted_offsets_map[cls_offsets_mask],
                    torch.tensor(cls_actual_offsets).to(device)))
        # åˆå¹¶æŸå¤±å€¼
        # æ³¨æ„ loss ä¸å¯ä»¥ä½¿ç”¨ += åˆå¹¶
        loss = torch.tensor(.0, requires_grad=True)
        loss = loss + torch.mean(torch.stack(rpn_labels_losses))
        loss = loss + torch.mean(torch.stack(rpn_offsets_losses))
        if cls_labels_losses:
            loss = loss + torch.mean(torch.stack(cls_labels_losses))
        if cls_offsets_losses:
            loss = loss + torch.mean(torch.stack(cls_offsets_losses))
        return loss

    @staticmethod
    def calc_accuracy(actual, predicted):
        """Faster-RCNN ä½¿ç”¨çš„æ­£ç¡®ç‡è®¡ç®—å™¨ï¼Œè¿™é‡Œåªè®¡ç®— RPN ä¸æ ‡ç­¾åˆ†ç±»çš„æ­£ç¡®ç‡ï¼ŒåŒºåŸŸåç§»ä¸è®¡ç®—"""
        rpn_labels, rpn_offsets, rpn_candidates_batch, cls_output, cls_result = predicted
        rpn_acc = 0
        cls_acc = 0
        for batch_index in range(len(actual)):
            # è®¡ç®— RPN çš„æ­£ç¡®ç‡ï¼Œæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æ­£ç¡®ç‡åˆ†åˆ«è®¡ç®—å†å¹³å‡
            (true_boxes_labels,
                actual_rpn_labels, actual_rpn_labels_mask,
                actual_rpn_offsets, actual_rpn_offsets_mask) = actual[batch_index]
            a = actual_rpn_labels.to(device)
            p = torch.max(rpn_labels[batch_index][actual_rpn_labels_mask], 1).indices
            rpn_acc_positive = ((a == 0) & (p == 0)).sum().item() / ((a == 0).sum().item() + 0.00001)
            rpn_acc_negative = ((a == 1) & (p == 1)).sum().item() / ((a == 1).sum().item() + 0.00001)
            rpn_acc += (rpn_acc_positive + rpn_acc_negative) / 2
            # è®¡ç®—æ ‡ç­¾åˆ†ç±»çš„æ­£ç¡®ç‡
            # æ­£ç¡®ç‡ = æœ‰å¯¹åº”é¢„æµ‹åŒºåŸŸå¹¶ä¸”é¢„æµ‹åˆ†ç±»æ­£ç¡®çš„çœŸå®åŒºåŸŸæ•°é‡ / æ€»çœŸå®åŒºåŸŸæ•°é‡
            cls_correct = 0
            for true_label, true_box in true_boxes_labels:
                if cls_result[batch_index] is None:
                    continue
                for predicted_label, predicted_box in cls_result[batch_index]:
                    if calc_iou(predicted_box, true_box) > IOU_POSITIVE_THRESHOLD and predicted_label == true_label:
                        cls_correct += 1
                        break
            cls_acc += cls_correct / len(true_boxes_labels)
        rpn_acc /= len(actual)
        cls_acc /= len(actual)
        return rpn_acc, cls_acc

MyModel.Anchors = MyModel._generate_anchors(8)

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def calc_resize_parameters(sw, sh):
    """è®¡ç®—ç¼©æ”¾å›¾ç‰‡çš„å‚æ•°"""
    sw_new, sh_new = sw, sh
    dw, dh = IMAGE_SIZE
    pad_w, pad_h = 0, 0
    if sw / sh < dw / dh:
        sw_new = int(dw / dh * sh)
        pad_w = (sw_new - sw) // 2 # å¡«å……å·¦å³
    else:
        sh_new = int(dh / dw * sw)
        pad_h = (sh_new - sh) // 2 # å¡«å……ä¸Šä¸‹
    return sw_new, sh_new, pad_w, pad_h

def resize_image(img):
    """ç¼©æ”¾å›¾ç‰‡ï¼Œæ¯”ä¾‹ä¸ä¸€è‡´æ—¶å¡«å……"""
    sw, sh = img.size
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    img_new = Image.new("RGB", (sw_new, sh_new))
    img_new.paste(img, (pad_w, pad_h))
    img_new = img_new.resize(IMAGE_SIZE)
    return img_new

def image_to_tensor(img):
    """è½¬æ¢å›¾ç‰‡å¯¹è±¡åˆ° tensor å¯¹è±¡"""
    arr = numpy.asarray(img)
    t = torch.from_numpy(arr)
    t = t.transpose(0, 2) # è½¬æ¢ç»´åº¦ H,W,C åˆ° C,W,H
    t = t / 255.0 # æ­£è§„åŒ–æ•°å€¼ä½¿å¾—èŒƒå›´åœ¨ 0 ~ 1
    return t

def map_box_to_resized_image(box, sw, sh):
    """æŠŠåŸå§‹åŒºåŸŸè½¬æ¢åˆ°ç¼©æ”¾åçš„å›¾ç‰‡å¯¹åº”çš„åŒºåŸŸ"""
    x, y, w, h = box
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int((x + pad_w) * scale)
    y = int((y + pad_h) * scale)
    w = int(w * scale)
    h = int(h * scale)
    if x + w > IMAGE_SIZE[0] or y + h > IMAGE_SIZE[1] or w == 0 or h == 0:
        return 0, 0, 0, 0
    return x, y, w, h

def map_box_to_original_image(box, sw, sh):
    """æŠŠç¼©æ”¾åå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸè½¬æ¢åˆ°ç¼©æ”¾å‰çš„åŸå§‹åŒºåŸŸ"""
    x, y, w, h = box
    sw_new, sh_new, pad_w, pad_h = calc_resize_parameters(sw, sh)
    scale = IMAGE_SIZE[0] / sw_new
    x = int(x / scale - pad_w)
    y = int(y / scale - pad_h)
    w = int(w / scale)
    h = int(h / scale)
    if x + w > sw or y + h > sh or x < 0 or y < 0 or w == 0 or h == 0:
        return 0, 0, 0, 0
    return x, y, w, h

def calc_iou(rect1, rect2):
    """è®¡ç®—ä¸¤ä¸ªåŒºåŸŸé‡å éƒ¨åˆ† / åˆå¹¶éƒ¨åˆ†çš„æ¯”ç‡ (intersection over union)"""
    x1, y1, w1, h1 = rect1
    x2, y2, w2, h2 = rect2
    xi = max(x1, x2)
    yi = max(y1, y2)
    wi = min(x1+w1, x2+w2) - xi
    hi = min(y1+h1, y2+h2) - yi
    if wi > 0 and hi > 0: # æœ‰é‡å éƒ¨åˆ†
        area_overlap = wi*hi
        area_all = w1*h1 + w2*h2 - area_overlap
        iou = area_overlap / area_all
    else: # æ²¡æœ‰é‡å éƒ¨åˆ†
        iou = 0
    return iou

def calc_box_offset(candidate_box, true_box):
    """è®¡ç®—å€™é€‰åŒºåŸŸä¸å®é™…åŒºåŸŸçš„åç§»å€¼"""
    # è¿™é‡Œè®¡ç®—å‡ºæ¥çš„åç§»å€¼åŸºäºæ¯”ä¾‹ï¼Œè€Œä¸å—å…·ä½“ä½ç½®å’Œå¤§å°å½±å“
    # w h ä½¿ç”¨ log æ˜¯ä¸ºäº†å‡å°‘è¿‡å¤§çš„å€¼çš„å½±å“
    x1, y1, w1, h1 = candidate_box
    x2, y2, w2, h2 = true_box
    x_offset = (x2 - x1) / w1
    y_offset = (y2 - y1) / h1
    w_offset = math.log(w2 / w1)
    h_offset = math.log(h2 / h1)
    return (x_offset, y_offset, w_offset, h_offset)

def adjust_box_by_offset(candidate_box, offset):
    """æ ¹æ®åç§»å€¼è°ƒæ•´å€™é€‰åŒºåŸŸ"""
    # exp éœ€è¦é™åˆ¶å€¼å°äº log(16)ï¼Œå¦‚æœå€¼è¿‡å¤§å¯èƒ½ä¼šå¼•å‘ OverflowError
    x1, y1, w1, h1 = candidate_box
    x_offset, y_offset, w_offset, h_offset = offset
    x2 = min(IMAGE_SIZE[0]-1,  max(0, w1 * x_offset + x1))
    y2 = min(IMAGE_SIZE[1]-1,  max(0, h1 * y_offset + y1))
    w2 = min(IMAGE_SIZE[0]-x2, max(1, math.exp(min(w_offset, 2.78)) * w1))
    h2 = min(IMAGE_SIZE[1]-y2, max(1, math.exp(min(h_offset, 2.78)) * h1))
    return (x2, y2, w2, h2)

def merge_box(box_a, box_b):
    """åˆå¹¶ä¸¤ä¸ªåŒºåŸŸ"""
    x1, y1, w1, h1 = box_a
    x2, y2, w2, h2 = box_b
    x = min(x1, x2)
    y = min(y1, y2)
    w = max(x1 + w1, x2 + w2) - x
    h = max(y1 + h1, y2 + h2) - y
    return (x, y, w, h)

def prepare_save_batch(batch, image_tensors, image_boxes_labels):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æŒ‰ç´¢å¼•å€¼åˆ—è¡¨ç”Ÿæˆè¾“å…¥å’Œè¾“å‡º tensor å¯¹è±¡çš„å‡½æ•°
    def split_dataset(indices):
        image_in = []
        boxes_labels_out = {}
        for new_image_index, original_image_index in enumerate(indices.tolist()):
            image_in.append(image_tensors[original_image_index])
            boxes_labels_out[new_image_index] = image_boxes_labels[original_image_index]
        tensor_image_in = torch.stack(image_in) # ç»´åº¦: B,C,W,H
        return tensor_image_in, boxes_labels_out

    # åˆ‡åˆ†è®­ç»ƒé›† (80%)ï¼ŒéªŒè¯é›† (10%) å’Œæµ‹è¯•é›† (10%)
    random_indices = torch.randperm(len(image_tensors))
    training_indices = random_indices[:int(len(random_indices)*0.8)]
    validating_indices = random_indices[int(len(random_indices)*0.8):int(len(random_indices)*0.9):]
    testing_indices = random_indices[int(len(random_indices)*0.9):]
    training_set = split_dataset(training_indices)
    validating_set = split_dataset(validating_indices)
    testing_set = split_dataset(testing_indices)

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # åŠ è½½å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
    # { å›¾ç‰‡å: [ åŒºåŸŸä¸åˆ†ç±», åŒºåŸŸä¸åˆ†ç±», .. ] }
    box_map = defaultdict(lambda: [])
    for filename in os.listdir(IMAGE_DIR):
        xml_path = os.path.join(ANNOTATION_DIR, filename.split(".")[0] + ".xml")
        if not os.path.isfile(xml_path):
            continue
        tree = ET.ElementTree(file=xml_path)
        objects = tree.findall("object")
        for obj in objects:
            class_name = obj.find("name").text
            x1 = int(obj.find("bndbox/xmin").text)
            x2 = int(obj.find("bndbox/xmax").text)
            y1 = int(obj.find("bndbox/ymin").text)
            y2 = int(obj.find("bndbox/ymax").text)
            box_map[filename].append((x1, y1, x2-x1, y2-y1, CLASSES_MAPPING[class_name]))

    # ä¿å­˜å›¾ç‰‡å’Œå›¾ç‰‡å¯¹åº”çš„åˆ†ç±»ä¸åŒºåŸŸåˆ—è¡¨
    batch_size = 20
    batch = 0
    image_tensors = [] # å›¾ç‰‡åˆ—è¡¨
    image_boxes_labels = {} # å›¾ç‰‡å¯¹åº”çš„çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨ï¼Œå’Œå€™é€‰åŒºåŸŸä¸åŒºåŸŸåç§»
    for filename, original_boxes_labels in box_map.items():
        image_path = os.path.join(IMAGE_DIR, filename)
        with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
            sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
            img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
            image_index = len(image_tensors) # å›¾ç‰‡åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•å€¼
            image_tensors.append(image_to_tensor(img)) # æ·»åŠ å›¾ç‰‡åˆ°åˆ—è¡¨
            true_boxes_labels = [] # å›¾ç‰‡å¯¹åº”çš„çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
        # æ·»åŠ çœŸå®åŒºåŸŸä¸åˆ†ç±»åˆ—è¡¨
        for box_label in original_boxes_labels:
            x, y, w, h, label = box_label
            x, y, w, h = map_box_to_resized_image((x, y, w, h), sw, sh) # ç¼©æ”¾å®é™…åŒºåŸŸ
            if w < 10 or h < 10:
                continue # ç¼©æ”¾ååŒºåŸŸè¿‡å°
            # æ£€æŸ¥è®¡ç®—æ˜¯å¦æœ‰é—®é¢˜
            # child_img = img.copy().crop((x, y, x+w, y+h))
            # child_img.save(f"{filename}_{x}_{y}_{w}_{h}_{label}.png")
            true_boxes_labels.append((label, (x, y, w, h)))
        # å¦‚æœå›¾ç‰‡ä¸­çš„æ‰€æœ‰åŒºåŸŸéƒ½è¿‡å°åˆ™è·³è¿‡
        if not true_boxes_labels:
            image_tensors.pop()
            image_index = len(image_tensors)
            continue
        # æ ¹æ®é”šç‚¹åˆ—è¡¨å¯»æ‰¾å€™é€‰åŒºåŸŸï¼Œå¹¶è®¡ç®—åŒºåŸŸåç§»
        actual_rpn_labels = []
        actual_rpn_labels_mask = []
        actual_rpn_offsets = []
        actual_rpn_offsets_mask = []
        positive_index_set = set()
        for index, anchor_box in enumerate(MyModel.Anchors):
            # å¦‚æœå€™é€‰åŒºåŸŸå’Œä»»æ„ä¸€ä¸ªå®é™…åŒºåŸŸé‡å ç‡å¤§äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯æ­£æ ·æœ¬
            # å¦‚æœå€™é€‰åŒºåŸŸå’Œæ‰€æœ‰å®é™…åŒºåŸŸé‡å ç‡éƒ½å°äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯è´Ÿæ ·æœ¬
            # é‡å ç‡ä»‹äºä¸¤ä¸ªé˜ˆå€¼ä¹‹é—´çš„åŒºåŸŸä¸å‚ä¸å­¦ä¹ 
            iou_list = [ calc_iou(anchor_box, true_box) for (_, true_box) in true_boxes_labels ]
            positive_index = next((index for index, iou in enumerate(iou_list) if iou > IOU_POSITIVE_THRESHOLD), None)
            is_negative = all(iou < IOU_NEGATIVE_THRESHOLD for iou in iou_list)
            if positive_index is not None:
                positive_index_set.add(positive_index)
                actual_rpn_labels.append(1)
                actual_rpn_labels_mask.append(index)
                # åªæœ‰åŒ…å«å¯¹è±¡çš„åŒºåŸŸå‚éœ€è¦è°ƒæ•´åç§»
                true_box = true_boxes_labels[positive_index][1]
                actual_rpn_offsets.append(calc_box_offset(anchor_box, true_box))
                actual_rpn_offsets_mask.append(index)
            elif is_negative:
                actual_rpn_labels.append(0)
                actual_rpn_labels_mask.append(index)
        # è¾“å‡ºæ‰¾ä¸åˆ°å€™é€‰åŒºåŸŸçš„çœŸå®åŒºåŸŸï¼Œè°ƒæ•´é”šç‚¹ç”Ÿæˆå‚æ•°æ—¶ä½¿ç”¨
        # for index in range(len(true_boxes_labels)):
        #    if index not in positive_index_set:
        #        print(true_boxes_labels[index][1])
        # print("-----")
        # å¦‚æœä¸€ä¸ªå€™é€‰åŒºåŸŸéƒ½æ‰¾ä¸åˆ°åˆ™è·³è¿‡
        if not positive_index_set:
            image_tensors.pop()
            image_index = len(image_tensors)
            continue
        image_boxes_labels[image_index] = (
            true_boxes_labels,
            torch.tensor(actual_rpn_labels, dtype=torch.long),
            torch.tensor(actual_rpn_labels_mask, dtype=torch.long),
            torch.tensor(actual_rpn_offsets, dtype=torch.float),
            torch.tensor(actual_rpn_offsets_mask, dtype=torch.long))
        # ä¿å­˜æ‰¹æ¬¡
        if len(image_tensors) >= batch_size:
            prepare_save_batch(batch, image_tensors, image_boxes_labels)
            image_tensors.clear()
            image_boxes_labels.clear()
            batch += 1
    # ä¿å­˜å‰©ä½™çš„æ‰¹æ¬¡
    if len(image_tensors) > 10:
        prepare_save_batch(batch, image_tensors, image_boxes_labels)

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel().to(device)

    # åˆ›å»ºå¤šä»»åŠ¡æŸå¤±è®¡ç®—å™¨
    loss_function = MyModel.loss_function

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.Adam(model.parameters())

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_rpn_accuracy_history = []
    training_cls_accuracy_history = []
    validating_rpn_accuracy_history = []
    validating_cls_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_rpn_accuracy_highest = -1
    validating_rpn_accuracy_highest_epoch = 0
    validating_cls_accuracy_highest = -1
    validating_cls_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            x, y = load_tensor(path)
            yield x.to(device), y

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    calc_accuracy = MyModel.calc_accuracy

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_rpn_accuracy_list = []
        training_cls_accuracy_list = []
        for batch_index, batch in enumerate(read_batches("data/training_set")):
            # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
            batch_x, batch_y = batch
            # è®¡ç®—é¢„æµ‹å€¼
            predicted = model(batch_x)
            # è®¡ç®—æŸå¤±
            loss = loss_function(predicted, batch_y)
            # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
            loss.backward()
            # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
            optimizer.step()
            # æ¸…ç©ºå¯¼å‡½æ•°å€¼
            optimizer.zero_grad()
            # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
            with torch.no_grad():
                training_batch_rpn_accuracy, training_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
            # è¾“å‡ºæ‰¹æ¬¡æ­£ç¡®ç‡
            training_rpn_accuracy_list.append(training_batch_rpn_accuracy)
            training_cls_accuracy_list.append(training_batch_cls_accuracy)
            print(f"epoch: {epoch}, batch: {batch_index}: " +
                f"batch rpn accuracy: {training_batch_rpn_accuracy}, cls accuracy: {training_batch_cls_accuracy}")
        training_rpn_accuracy = sum(training_rpn_accuracy_list) / len(training_rpn_accuracy_list)
        training_cls_accuracy = sum(training_cls_accuracy_list) / len(training_cls_accuracy_list)
        training_rpn_accuracy_history.append(training_rpn_accuracy)
        training_cls_accuracy_history.append(training_cls_accuracy)
        print(f"training rpn accuracy: {training_rpn_accuracy}, cls accuracy: {training_cls_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_rpn_accuracy_list = []
        validating_cls_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_y = batch
            predicted = model(batch_x)
            validating_batch_rpn_accuracy, validating_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
            validating_rpn_accuracy_list.append(validating_batch_rpn_accuracy)
            validating_cls_accuracy_list.append(validating_batch_cls_accuracy)
        validating_rpn_accuracy = sum(validating_rpn_accuracy_list) / len(validating_rpn_accuracy_list)
        validating_cls_accuracy = sum(validating_cls_accuracy_list) / len(validating_cls_accuracy_list)
        validating_rpn_accuracy_history.append(validating_rpn_accuracy)
        validating_cls_accuracy_history.append(validating_cls_accuracy)
        print(f"validating rpn accuracy: {validating_rpn_accuracy}, cls accuracy: {validating_cls_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_rpn_accuracy > validating_rpn_accuracy_highest:
            validating_rpn_accuracy_highest = validating_rpn_accuracy
            validating_rpn_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest rpn validating accuracy updated")
        elif validating_cls_accuracy > validating_cls_accuracy_highest:
            validating_cls_accuracy_highest = validating_cls_accuracy
            validating_cls_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest cls validating accuracy updated")
        elif (epoch - validating_rpn_accuracy_highest_epoch > 20 and
            epoch - validating_cls_accuracy_highest_epoch > 20):
            # åœ¨ 20 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 20 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest rpn validating accuracy: {validating_rpn_accuracy_highest}",
        f"from epoch {validating_rpn_accuracy_highest_epoch}")
    print(f"highest cls validating accuracy: {validating_cls_accuracy_highest}",
        f"from epoch {validating_cls_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_rpn_accuracy_list = []
    testing_cls_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_y = batch
        predicted = model(batch_x)
        testing_batch_rpn_accuracy, testing_batch_cls_accuracy = calc_accuracy(batch_y, predicted)
        testing_rpn_accuracy_list.append(testing_batch_rpn_accuracy)
        testing_cls_accuracy_list.append(testing_batch_cls_accuracy)
    testing_rpn_accuracy = sum(testing_rpn_accuracy_list) / len(testing_rpn_accuracy_list)
    testing_cls_accuracy = sum(testing_cls_accuracy_list) / len(testing_cls_accuracy_list)
    print(f"testing rpn accuracy: {testing_rpn_accuracy}, cls accuracy: {testing_cls_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_rpn_accuracy_history, label="training_rpn_accuracy")
    pyplot.plot(training_cls_accuracy_history, label="training_cls_accuracy")
    pyplot.plot(validating_rpn_accuracy_history, label="validating_rpn_accuracy")
    pyplot.plot(validating_cls_accuracy_history, label="validating_cls_accuracy")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel().to(device)
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®å›¾ç‰‡è·¯å¾„ï¼Œå¹¶æ˜¾ç¤ºæ‰€æœ‰å¯èƒ½æ˜¯äººè„¸çš„åŒºåŸŸ
    while True:
        try:
            image_path = input("Image path: ")
            if not image_path:
                continue
            # æ„å»ºè¾“å…¥
            with Image.open(image_path) as img_original: # åŠ è½½åŸå§‹å›¾ç‰‡
                sw, sh = img_original.size # åŸå§‹å›¾ç‰‡å¤§å°
                img = resize_image(img_original) # ç¼©æ”¾å›¾ç‰‡
                img_output = img_original.copy() # å¤åˆ¶å›¾ç‰‡ï¼Œç”¨äºåé¢æ·»åŠ æ ‡è®°
                tensor_in = image_to_tensor(img)
            # é¢„æµ‹è¾“å‡º
            cls_result = model(tensor_in.unsqueeze(0).to(device))[-1][0]
            # åˆå¹¶é‡å çš„ç»“æœåŒºåŸŸ, ç»“æœæ˜¯ [ [æ ‡ç­¾åˆ—è¡¨, åˆå¹¶åçš„åŒºåŸŸ], ... ]
            final_result = []
            for label, box in cls_result:
                for index in range(len(final_result)):
                    exists_labels, exists_box = final_result[index]
                    if calc_iou(box, exists_box) > IOU_MERGE_THRESHOLD:
                        exists_labels.append(label)
                        final_result[index] = (exists_labels, merge_box(box, exists_box))
                        break
                else:
                    final_result.append(([label], box))
            # åˆå¹¶æ ‡ç­¾ (é‡å åŒºåŸŸçš„æ ‡ç­¾ä¸­æ•°é‡æœ€å¤šçš„åˆ†ç±»ä¸ºæœ€ç»ˆåˆ†ç±»)
            for index in range(len(final_result)):
                labels, box = final_result[index]
                final_label = Counter(labels).most_common(1)[0][0]
                final_result[index] = (final_label, box)
            # æ ‡è®°åœ¨å›¾ç‰‡ä¸Š
            draw = ImageDraw.Draw(img_output)
            for label, box in final_result:
                x, y, w, h = map_box_to_original_image(box, sw, sh)
                draw.rectangle((x, y, x+w, y+h), outline="#FF0000")
                draw.text((x, y-10), CLASSES[label], fill="#FF0000")
                print((x, y, w, h), CLASSES[label])
            img_output.save("img_output.png")
            print("saved to img_output.png")
            print()
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    random.seed(0)
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒï¼š

``` text
python3 example.py prepare
python3 example.py train
```

æœ€ç»ˆè¾“å‡ºå¦‚ä¸‹ï¼š

``` text
epoch: 101, batch: 30: batch rpn accuracy: 0.9999998976070061, cls accuracy: 0.9114583333333333
epoch: 101, batch: 31: batch rpn accuracy: 0.9834558104401839, cls accuracy: 0.8140625
epoch: 101, batch: 32: batch rpn accuracy: 0.9999098026259949, cls accuracy: 0.7739583333333333
epoch: 101, batch: 33: batch rpn accuracy: 0.9998011454364403, cls accuracy: 0.8216517857142858
epoch: 101, batch: 34: batch rpn accuracy: 0.9968102716843542, cls accuracy: 0.7961309523809523
epoch: 101, batch: 35: batch rpn accuracy: 0.9992402167888915, cls accuracy: 0.9169642857142857
epoch: 101, batch: 36: batch rpn accuracy: 0.9991754689754888, cls accuracy: 0.784375
epoch: 101, batch: 37: batch rpn accuracy: 0.9998954174868623, cls accuracy: 0.808531746031746
epoch: 101, batch: 38: batch rpn accuracy: 0.999810537169184, cls accuracy: 0.8928571428571429
epoch: 101, batch: 39: batch rpn accuracy: 0.9993760622446838, cls accuracy: 0.7447916666666667
epoch: 101, batch: 40: batch rpn accuracy: 0.9990286666127914, cls accuracy: 0.8565972222222223
epoch: 101, batch: 41: batch rpn accuracy: 0.9999998978468275, cls accuracy: 0.8012820512820512
training rpn accuracy: 0.9992436053003302, cls accuracy: 0.8312847933023624
validating rpn accuracy: 0.89010891321815, cls accuracy: 0.6757137703566275
stop training because highest validating accuracy not updated in 20 epoches
highest rpn validating accuracy: 0.951476186351423 from epoch 63
highest cls validating accuracy: 0.707979883872741 from epoch 80
testing rpn accuracy: 0.9250985286757772, cls accuracy: 0.7238060880918024
```

`cls accuracy` ä»£è¡¨å¯ä»¥è¯†åˆ«å‡ºå¤šå°‘åŒ…å«å¯¹è±¡çš„åŒºåŸŸå¹¶ä¸”æ­£ç¡®åˆ¤æ–­å®ƒçš„åˆ†ç±»ï¼Œè™½ç„¶åªæœ‰ 70% å·¦å³ä½†å®é™…æ•ˆæœè¿˜æ˜¯ä¸é”™çš„ï¼Œå¦‚æœæœ‰æ›´å¤šæ˜¾å­˜å¯ä»¥å¢å¼º CNN æ¨¡å‹ (ä¾‹å¦‚ä½¿ç”¨ Resnet-50) ä¸åŠ å¤§ `IMAGE_SIZE`ã€‚

è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–å¦‚ä¸‹ï¼š

![Figure_1](Figure_1.png)

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå†è¾“å…¥å›¾ç‰‡è·¯å¾„å¯ä»¥ä½¿ç”¨å­¦ä¹ å¥½çš„æ¨¡å‹è¯†åˆ«å›¾ç‰‡ï¼š

``` text
python3 example.py eval
```

ä»¥ä¸‹æ˜¯éƒ¨åˆ†è¯†åˆ«ç»“æœï¼š

![img_output_1](./img_output_1.png)

![img_output_2](./img_output_2.png)

![img_output_3](./img_output_3.png)

![img_output_4](./img_output_4.png)

![img_output_5](./img_output_5.png)

æ•ˆæœè¿˜è¡Œå§ğŸ¤—ï¼Œé¡ºé“ä¸€ææ¯å¼ å›¾ç‰‡çš„è¯†åˆ«æ—¶é—´å¤§çº¦åœ¨ 0.05 ~ 0.06 ç§’ä¹‹é—´ï¼Œç›¸å¯¹äº Fast-RCNN å¿«äº†æ¥è¿‘ 10 å€ï¼Œç”¨åœ¨è§†é¢‘ä¸Šå¤§çº¦å¯ä»¥æ”¯æŒ 20fps å·¦å³ (æˆ‘æœºå™¨é…ç½®æ¯”è¾ƒä½ï¼Œ4 æ ¸ CPU + GTX1650ï¼Œé«˜é…æœºå™¨å¯ä»¥æ›´å¿«ğŸ¤’)ã€‚

## å†™åœ¨æœ€å

è¿™ç¯‡ä»‹ç»çš„ Faster-RCNN æ•ˆæœæ˜æ˜¾æ¯”ä¹‹å‰ä»‹ç»çš„ RCNN ä¸ Fast-RCNN æ›´å¥½ï¼Œä½†è¿˜æ˜¯æœ‰ç¼ºç‚¹çš„ï¼Œå¦‚æœå¯¹è±¡ç›¸å¯¹äºå›¾ç‰‡å¾ˆå°æˆ–è€…å¾ˆå¤§ï¼Œé‚£ä¹ˆå¯¹è±¡ä¸é”šç‚¹çš„å„ä¸ªå½¢çŠ¶çš„é‡å ç‡éƒ½ä¼šæ¯”è¾ƒä½ï¼Œå¯¼è‡´æ— æ³•è¯†åˆ«å‡ºæ¥ã€‚ä¸‹ä¸€ç¯‡ä»‹ç»çš„ YOLO æ¨¡å‹ä¸€å®šç¨‹åº¦ä¸Šæ”¹å–„äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†æ¥ä¸‹æ¥ä¸€å¤´åŠä¸ªæœˆæˆ‘ä¼°è®¡éƒ½æ²¡æ—¶é—´å†™ï¼Œæƒ³çœ‹çš„è€å¿ƒç­‰å§ğŸ¤•ã€‚
