# å†™ç»™ç¨‹åºå‘˜çš„æœºå™¨å­¦ä¹ å…¥é—¨ (äº”) - é€’å½’æ¨¡å‹ RNNï¼ŒLSTM ä¸ GRU

## é€’å½’æ¨¡å‹çš„åº”ç”¨åœºæ™¯

åœ¨å‰é¢çš„æ–‡ç« ä¸­æˆ‘ä»¬çœ‹åˆ°çš„å¤šå±‚çº¿æ€§æ¨¡å‹èƒ½å¤„ç†çš„è¾“å…¥æ•°é‡æ˜¯å›ºå®šçš„ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹èƒ½æ¥æ”¶ä¸¤ä¸ªè¾“å…¥é‚£ä¹ˆä½ å°±ä¸èƒ½ç»™å®ƒä¼ ä¸€ä¸ªæˆ–è€…ä¸‰ä¸ªã€‚è€Œæœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦æ ¹æ®æ•°é‡ä¸ä¸€å®šçš„è¾“å…¥æ¥é¢„æµ‹è¾“å‡ºï¼Œä¾‹å¦‚æ–‡æœ¬å°±æ˜¯æ•°é‡ä¸ä¸€å®šçš„è¾“å…¥ï¼Œâ€œè¿™éƒ¨ç‰‡éå¸¸å¥½çœ‹â€ æœ‰ 7 ä¸ªå­—ï¼Œâ€œè¿™éƒ¨ç‰‡å¾ˆæ— èŠâ€ æœ‰ 6 ä¸ªå­—ï¼Œå¦‚æœæˆ‘ä»¬æƒ³æ ¹æ®æ–‡æœ¬åˆ¤æ–­æ˜¯æ­£é¢è¯„ä»·è¿˜æ˜¯è´Ÿé¢è¯„ä»·ï¼Œé‚£ä¹ˆå°±éœ€è¦ä½¿ç”¨æ”¯æŒä¸å®šé•¿åº¦ (å³å¯ä»¥æ¥æ”¶ 6 ä¸ªåˆå¯ä»¥æ¥æ”¶ 7 ä¸ª) è¾“å…¥çš„æ¨¡å‹ã€‚æ—¶åºæ€§çš„æ•°æ®æ•°é‡ä¹Ÿæ˜¯ä¸ä¸€å®šçš„ï¼Œä¾‹å¦‚ä¸€ä¸ªè¿åŠ¨ä¸­çš„çƒï¼Œä»æŸä¸ªæ—¶é—´ç‚¹å¼€å§‹çš„ç¬¬ 0 ç§’åœ¨ä½ç½® 1ï¼Œç¬¬ 1 ç§’åœ¨ä½ç½® 3ï¼Œç¬¬ 2 ç§’åœ¨ä½ç½® 5ï¼Œé‚£ä¹ˆæ­£ç¡®çš„æ¨¡å‹åº”è¯¥å¯ä»¥é¢„æµ‹å‡ºç¬¬ 3 ç§’åœ¨ä½ç½® 7ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å½“ç„¶ï¼Œæ—¶åºæ€§çš„æ•°æ®å¯ä»¥å›ºå®šä¸€ä¸ªçª—å£ï¼ˆä¾‹å¦‚æœ€è¿‘çš„ 5 æ¡æ•°æ®ï¼‰æ¥å¤„ç†ï¼Œè¿™æ ·è¾“å…¥æ•°é‡å°±æ˜¯ä¸€å®šçš„ï¼Œä½†çµæ´»æ€§å°±é™ä½äº†ï¼Œçª—å£è®¾ç½®è¿‡å°å¯èƒ½ä¼šå¯¼è‡´æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ç”¨äºé¢„æµ‹è¾“å‡ºï¼Œè¿‡å¤§åˆ™ä¼šå½±å“æ€§èƒ½ã€‚

![01](./01.png)

é€’å½’æ¨¡å‹ (Recursive Model) å¯ä»¥ç”¨äºå¤„ç†ä¸å®šé•¿åº¦çš„è¾“å…¥ï¼Œç”¨æ³•æ˜¯ä¸€æ¬¡åªä¼ å›ºå®šæ•°é‡çš„è¾“å…¥ç»™æ¨¡å‹ï¼Œå¯ä»¥åˆ†å¤šæ¬¡ä¼ ï¼Œä¼ çš„æ¬¡æ•°æ ¹æ®æ•°æ®è€Œå®šã€‚ä»¥ä¸Šè¿°ä¾‹å­æ¥è¯´ï¼Œâ€œè¿™éƒ¨ç‰‡éå¸¸å¥½çœ‹â€ æ¯æ¬¡ä¼ ä¸€ä¸ªå­—éœ€è¦ä¼  7 æ¬¡ï¼Œâ€œè¿™éƒ¨ç‰‡å¾ˆæ— èŠâ€ æ¯æ¬¡ä¼ ä¸€ä¸ªå­—éœ€è¦ä¼  6 æ¬¡ã€‚è€Œé€’å½’æ¨¡å‹æ¯æ”¶åˆ°ä¸€æ¬¡è¾“å…¥éƒ½ä¼šè¿”å›ä¸€æ¬¡è¾“å‡ºï¼Œæœ‰çš„åœºæ™¯åªä¼šä½¿ç”¨æœ€åä¸€æ¬¡è¾“å‡ºçš„ç»“æœ (ä¾‹å¦‚è¿™ä¸ªä¾‹å­)ï¼Œè€Œæœ‰çš„åœºæ™¯åˆ™ä¼šä½¿ç”¨æ¯ä¸€æ¬¡è¾“å‡ºçš„ç»“æœã€‚

![02](./02.png)

æ¢æˆä»£ç å¯ä»¥è¿™æ ·ç†è§£ï¼š

``` text
model = MyRecursiveModel()
model('è¿™')
model('éƒ¨')
model('ç‰‡')
model('é')
model('å¸¸')
model('å¥½')
last_output = model('çœ‹')
print(last_output)
```

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹å‡ ä¸ªç»å…¸çš„é€’å½’æ¨¡å‹æ˜¯æ€ä¹ˆå®ç°çš„ã€‚

## æœ€ç®€å•çš„é€’å½’æ¨¡å‹ - RNN (tanh)

RNN tanh (Recurrent Neural Network - tanh) æ˜¯æœ€ç®€å•çš„é€’å½’æ¨¡å‹ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼Œæ•°å­¦ä¸å¥½çš„ç¬¬ä¸€å°è±¡å¯èƒ½ä¼šè§‰å¾—å¦ˆå‘€ä¸€çœ‹æ•°å­¦å…¬å¼å°±å¤´æ˜è„‘èƒ€äº†ğŸ™€ï¼Œæˆ‘ä»¬å…ˆä¸€ä¸ªä¸ªå‚æ•°æ¥åˆ†æï¼Œ`H` æ˜¯é€’å½’æ¨¡å‹å†…éƒ¨è®°å½•çš„ä¸€ä¸ªéšè—å€¼çŸ©é˜µï¼Œ`Ht` ä»£è¡¨å½“å‰æ—¶åºçš„å€¼ï¼Œè€Œ `H(t-1)` ä»£è¡¨å‰ä¸€ä¸ªæ—¶åºçš„å€¼ï¼Œ`t` å¯ä»¥ç½®æ¢åˆ°å…·ä½“çš„æ•°å­—ï¼Œä¾‹å¦‚ `Ht0` ä»£è¡¨éšè—å€¼çŸ©é˜µæœ€å¼€å§‹çš„çŠ¶æ€ (ä¸€èˆ¬ä¼šä½¿ç”¨ 0 åˆå§‹åŒ–)ï¼Œ`Ht1` ä»£è¡¨éšè—å€¼çŸ©é˜µç¬¬ä¸€æ¬¡æ›´æ–°ä»¥åçš„çŠ¶æ€ï¼Œ`Ht2` ä»£ç¬”éšè—å€¼çŸ©é˜µç¬¬äºŒæ¬¡æ›´æ–°ä»¥åçš„çŠ¶æ€ï¼Œè®¡ç®— `Ht1` æ—¶ `H(t-1)` ä»£è¡¨ `Ht0`ï¼Œè®¡ç®— `Ht2` æ—¶ `H(t-1)` ä»£è¡¨ `Ht1`ï¼›`Wx` æ˜¯é’ˆå¯¹è¾“å…¥çš„æƒé‡å€¼ï¼Œ`Bx` æ˜¯é’ˆå¯¹è¾“å…¥çš„åç§»å€¼ï¼Œ`Wh` æ˜¯é’ˆå¯¹éšè—çš„æƒé‡å€¼ï¼Œ`Bh` æ˜¯é’ˆå¯¹éšè—çš„åç§»å€¼ï¼›tanh ç”¨äºæŠŠå®æ•°è½¬æ¢ä¸º `-1 ~ 1` ä¹‹é—´çš„èŒƒå›´ã€‚è¿™ä¸ªå…¬å¼å’Œä¹‹å‰æˆ‘ä»¬çœ‹åˆ°çš„äººå·¥ç¥ç»å…ƒå¾ˆç›¸ä¼¼ï¼Œåªæ˜¯æŠŠæ¯ä¸€æ¬¡çš„è¾“å…¥å’Œå½“å‰éšè—å€¼ç»è¿‡è®¡ç®—åçš„å€¼åŠ åœ¨ä¸€èµ·ï¼Œç„¶åä½¿ç”¨ tanh ä½œä¸ºæ¿€æ´»å‡½æ•°ç”Ÿæˆæ–°çš„éšè—å€¼ï¼Œéšè—å€¼ä¼šå½“ä½œæ¯ä¸€æ¬¡çš„è¾“å‡ºä½¿ç”¨ã€‚

![03](./03.png)

å¦‚æœä½ è§‰å¾—æ–‡æœ¬éš¾ä»¥ç†è§£ï¼Œå¯ä»¥çœ‹å±•å¼€ä»¥åçš„å…¬å¼ï¼š

![04](./04.png)

å¯ä»¥çœ‹åˆ°æ¯æ¬¡çš„è¾“å‡ºç»“æœéƒ½ä¼šæ ¹æ®ä¹‹å‰çš„è¾“å…¥è®¡ç®—ï¼Œtanh ç”¨äºéçº¿æ€§è¿‡æ»¤å’Œæ§åˆ¶å€¼èŒƒå›´åœ¨ `-1 ~ 1` ä¹‹é—´ã€‚

ä½ ä¹Ÿå¯ä»¥å‚è€ƒä¸‹é¢çš„è®¡ç®—å›¾æ¥ç†è§£ï¼Œå›¾ä¸­å±•ç¤ºäº† RNN tanh æ¨¡å‹å¦‚ä½•è®¡ç®—ä¸‰æ¬¡è¾“å…¥å’Œè¿”å›ä¸‰æ¬¡è¾“å‡º (æ³¨æ„æœ€ååŠ äº†ä¸€å±‚é¢å¤–çš„çº¿æ€§æ¨¡å‹ç”¨äºæŠŠ RNN è¿”å›çš„éšè—å€¼è½¬æ¢ä¸ºé¢„æµ‹è¾“å‡º)ï¼š

(çœ‹ä¸æ¸…è¯·åœ¨æ–°æ ‡ç­¾å•ç‹¬æ‰“å¼€å›¾ç‰‡ï¼Œæˆ–è€…å¦å­˜ä¸ºåˆ°æœ¬åœ°ä»¥åæŸ¥çœ‹)

![rnn](./rnn.png)

## åœ¨ pytorch ä¸­ä½¿ç”¨ RNN

å› ä¸ºé€’å½’æ¨¡å‹æ”¯æŒä¸å®šé•¿åº¦çš„æ•°æ®ï¼Œè€Œ pytorch å›´ç»• tensor æ¥è¿›è¡Œè¿ç®—ï¼Œtensor çš„ç»´åº¦åˆæ˜¯å›ºå®šçš„ï¼Œè¦åœ¨ pytorch ä¸­ä½¿ç”¨é€’å½’æ¨¡å‹éœ€è¦å¾ˆç¹ççš„å¤„ç†ï¼Œä¸‹å›¾è¯´æ˜äº†å¤„ç†çš„æ•´ä¸ªæµç¨‹ï¼š

![05](./05.png)

æˆ‘ä»¬å†æ¥çœ‹çœ‹æ€æ ·åœ¨ä»£ç é‡Œé¢å®ç°è¿™ä¸ªæµç¨‹ï¼š

``` python
# å¼•ç”¨ pytorch ç±»åº“
>>> import torch

# å‡†å¤‡æ•°æ®é›†
>>> data1 = torch.tensor([1, 2, 3], dtype=torch.float)
>>> data2 = torch.tensor([3, 5, 7, 9, 11], dtype=torch.float)
>>> datalist = [data1, data2]

# åˆå¹¶åˆ°ä¸€ä¸ª tensor å¹¶å¡«å……æ•°æ®é›†åˆ°ç›¸åŒé•¿åº¦
# è¿™é‡Œä½¿ç”¨ batch_first æŒ‡å®šç¬¬ä¸€ç»´åº¦ä¸ºæ‰¹æ¬¡æ•°é‡
>>> padded = torch.nn.utils.rnn.pad_sequence(datalist, batch_first=True)
>>> padded
tensor([[ 1.,  2.,  3.,  0.,  0.],
        [ 3.,  5.,  7.,  9., 11.]])

# å¦å»ºä¸€ä¸ª tensor æ¥ä¿å­˜å„ç»„æ•°æ®çš„å®é™…é•¿åº¦
>>> lengths = torch.tensor([len(x) for x in datalist])
>>> lengths
tensor([3, 5])

# å»ºç«‹ RNN æ¨¡å‹ï¼Œæ¯æ¬¡æ¥æ”¶ 1 ä¸ªè¾“å…¥ï¼Œå†…éƒ¨æ‹¥æœ‰ 8 ä¸ªéšè—å€¼ (æ¯æ¬¡éƒ½ä¼šè¿”å› 8 ä¸ªæœ€æ–°çš„éšè—å€¼)
# æŒ‡å®š num_layers å¯ä»¥åœ¨å†…éƒ¨å åŠ  RNN æ¨¡å‹ï¼Œè¿™é‡Œä¸å åŠ æ‰€ä»¥åªæŒ‡å®š 1
>>> rnn_model = torch.nn.RNN(input_size = 1, hidden_size = 8, num_layers = 1, batch_first = True)

# å»ºç«‹ Linear æ¨¡å‹ï¼Œæ¯æ¬¡æ¥æ”¶ 8 ä¸ªè¾“å…¥ (RNN æ¨¡å‹è¿”å›çš„éšè—å€¼)ï¼Œè¿”å› 1 ä¸ªè¾“å‡º
>>> linear_model = torch.nn.Linear(in_features = 8, out_features = 1)

# æ”¹å˜ tensor ç»´åº¦åˆ° batch_size, input_size, step_size
# å³ æ‰¹æ¬¡æ•°é‡, è¾“å…¥æ¬¡æ•°, æ¯æ¬¡ä¼ ç»™æ¨¡å‹çš„è¾“å…¥æ•°é‡
>>> x = padded.reshape(padded.shape[0], padded.shape[1], 1)
>>> x
tensor([[[ 1.],
         [ 2.],
         [ 3.],
         [ 0.],
         [ 0.]],

        [[ 3.],
         [ 5.],
         [ 7.],
         [ 9.],
         [11.]]])

# æŠŠæ•°æ® tensor å’Œé•¿åº¦ tensor åˆå¹¶åˆ°ä¸€ä¸ªç»“æ„ä½“
# è¿™æ ·åšçš„æ„ä¹‰æ˜¯å¯ä»¥é¿å… RNN è®¡ç®—å¡«å……çš„é‚£äº› 0
# enforce_sorted è¡¨ç¤ºæ•°æ®äº‹å…ˆæ²¡æœ‰æ’åºè¿‡ï¼Œå¦‚æœä¸æŒ‡å®šè¿™ä¸ªé€‰é¡¹ä¼šå‡ºé”™
>>> packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
>>> packed
PackedSequence(data=tensor([[ 3.],
        [ 1.],
        [ 5.],
        [ 2.],
        [ 7.],
        [ 3.],
        [ 9.],
        [11.]]), batch_sizes=tensor([2, 2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))

# æŠŠç»“æ„ä½“ä¼ ç»™æ¨¡å‹
# æ¨¡å‹ä¼šè¿”å›å„ä¸ªè¾“å…¥å¯¹åº”çš„è¾“å‡ºï¼Œç»´åº¦æ˜¯ å®é™…å¤„ç†çš„è¾“å…¥æ•°é‡,  hidden_size
# æ¨¡å‹è¿˜ä¼šè¿”å›æœ€æ–°çš„éšè—å€¼
>>> output, hidden = rnn_model(packed)
>>> output
PackedSequence(data=tensor([[-0.3055,  0.2916,  0.2736, -0.0502, -0.4033, -0.1438, -0.6981,  0.6284],
        [-0.2343,  0.2279,  0.0595,  0.1867, -0.2527, -0.0518, -0.1913,  0.5276],
        [-0.0556,  0.2870,  0.3035, -0.3519, -0.4015,  0.1584, -0.9157,  0.6472],
        [-0.1488,  0.2706,  0.1115, -0.0131, -0.2350,  0.1252, -0.4981,  0.5706],
        [-0.0179,  0.1201,  0.4751, -0.5256, -0.3701,  0.1289, -0.9834,  0.7087],
        [-0.1094,  0.1283,  0.1698, -0.1136, -0.1999,  0.1847, -0.7394,  0.5756],
        [ 0.0426,  0.1866,  0.5581, -0.6716, -0.4857,  0.0039, -0.9964,  0.7603],
        [ 0.0931,  0.2418,  0.6602, -0.7674, -0.6003, -0.0989, -0.9991,  0.8172]],
       grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))
>>> hidden
tensor([[[-0.1094,  0.1283,  0.1698, -0.1136, -0.1999,  0.1847, -0.7394,
           0.5756],
         [ 0.0931,  0.2418,  0.6602, -0.7674, -0.6003, -0.0989, -0.9991,
           0.8172]]], grad_fn=<IndexSelectBackward>)

# æŠŠè¿åœ¨ä¸€èµ·çš„è¾“å‡ºè§£å‹åˆ°ä¸€ä¸ª tensor
# è§£å‹åçš„ç»´åº¦æ˜¯ batch_size, input_size, hidden_size
# æ³¨æ„ç¬¬äºŒä¸ªè¿”å›å€¼æ˜¯å„ç»„è¾“å‡ºçš„å®é™…é•¿åº¦ï¼Œç­‰äºä¹‹å‰çš„ lengthsï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦
>>> unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)
>>> unpacked
tensor([[[-0.2343,  0.2279,  0.0595,  0.1867, -0.2527, -0.0518, -0.1913,
           0.5276],
         [-0.1488,  0.2706,  0.1115, -0.0131, -0.2350,  0.1252, -0.4981,
           0.5706],
         [-0.1094,  0.1283,  0.1698, -0.1136, -0.1999,  0.1847, -0.7394,
           0.5756],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000]],

        [[-0.3055,  0.2916,  0.2736, -0.0502, -0.4033, -0.1438, -0.6981,
           0.6284],
         [-0.0556,  0.2870,  0.3035, -0.3519, -0.4015,  0.1584, -0.9157,
           0.6472],
         [-0.0179,  0.1201,  0.4751, -0.5256, -0.3701,  0.1289, -0.9834,
           0.7087],
         [ 0.0426,  0.1866,  0.5581, -0.6716, -0.4857,  0.0039, -0.9964,
           0.7603],
         [ 0.0931,  0.2418,  0.6602, -0.7674, -0.6003, -0.0989, -0.9991,
           0.8172]]], grad_fn=<IndexSelectBackward>)

# æå–æœ€åä¸€ä¸ªè¾“å‡º (æ ¹æ®ä¸šåŠ¡è€Œå®š)
# æå–åçš„ç»´åº¦æ˜¯ batch_size, hidden_size
# å¯ä»¥çœ‹åˆ°ä½¿ç”¨ RNN tanh æ—¶æœ€åä¸€ä¸ªè¾“å‡ºçš„å€¼ç­‰äºä¹‹å‰è¿”å›çš„ hidden
>>> last_hidden = unpacked.gather(1, (lengths - 1).reshape(-1, 1, 1).repeat(1, 1, unpacked.shape[2]))
>>> last_hidden
tensor([[[-0.1094,  0.1283,  0.1698, -0.1136, -0.1999,  0.1847, -0.7394,
           0.5756]],

        [[ 0.0931,  0.2418,  0.6602, -0.7674, -0.6003, -0.0989, -0.9991,
           0.8172]]], grad_fn=<GatherBackward>)

# æŠŠ RNN æ¨¡å‹è¿”å›çš„éšè—å€¼ä¼ ç»™ Linear æ¨¡å‹ï¼Œå¾—å‡ºé¢„æµ‹è¾“å‡º
>>> predicted = linear_model(last_hidden)
>>> predicted
tensor([[[0.1553]],

        [[0.1431]]], grad_fn=<AddBackward0>)
```

ä¹‹åæ ¹æ®å®é™…è¾“å‡ºè®¡ç®—è¯¯å·®ï¼Œç„¶åæ ¹æ®è‡ªåŠ¨å¾®åˆ†è°ƒæ•´å‚æ•°å³å¯è¿›è¡Œè®­ç»ƒã€‚

## RNN çš„ç®€å•ä¾‹å­

ç°åœ¨æˆ‘ä»¬æ¥é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­å®è·µ RNN æ¨¡å‹ï¼Œå‡è®¾æœ‰ä¸€ä¸ªçƒï¼Œçƒåªå¯ä»¥åŒæ–¹å‘åŒ€é€Ÿç§»åŠ¨ï¼ŒæŠŠæŸä¸€ç‚¹å®šåšä½ç½® 0ï¼Œç‚¹çš„å³è¾¹æŒ‰ä¸€å®šé—´éš”å®šåšä½ç½® 1, 2, 3 ...ï¼Œç‚¹çš„å·¦è¾¹æŒ‰ä¸€å®šé—´éš”å®šåšä½ç½® -1, -2, -3, ç°åœ¨æœ‰çƒçš„ç§»åŠ¨ä½ç½®æ•°æ®ï¼Œä¾‹å¦‚ï¼š

``` text
1,3,5,7,9
```

è¡¨ç¤ºè®°å½•äº† 5 æ¬¡çƒçš„ç§»åŠ¨ä½ç½®ï¼Œæ¯æ¬¡çƒéƒ½ç§»åŠ¨äº† 2 ä¸ªä½ç½®çš„è·ç¦»ï¼Œå¦‚æœæˆ‘ä»¬è¦å»ºç«‹ä¸€ä¸ª RNN æ¨¡å‹æ ¹æ®çƒçš„å†å²ä½ç½®é¢„æµ‹å°†æ¥ä½ç½®ï¼Œé‚£ä¹ˆä¼ å…¥ `1,3,5,7` æ¨¡å‹åº”è¯¥å¯ä»¥è¿”å› `9`ï¼Œå¦‚æœçƒå‘åæ–¹å‘è¿åŠ¨ï¼Œä¼ å…¥ `9,7,5,3` æ¨¡å‹åº”è¯¥å¯ä»¥è¿”å› `1`ã€‚

æˆ‘å‡†å¤‡äº† 10000 æ¡è¿™æ ·çš„æ•°æ®ï¼Œå¯ä»¥ä» https://github.com/303248153/BlogArchive/tree/master/ml-05/ball.csv ä¸‹è½½ã€‚

ä»¥ä¸‹æ˜¯è®­ç»ƒå’Œä½¿ç”¨æ¨¡å‹çš„ä»£ç ï¼Œè·Ÿä¸Šä¸€ç¯‡æ–‡ç« ä»‹ç»çš„ä»£ç ç»“æ„åŸºæœ¬ä¸Šç›¸åŒï¼Œæ³¨æ„ä»£ç ä¼šåˆ‡åˆ†æ•°æ®åˆ°è¾“å…¥ (é™¤äº†æœ€åä¸€ä¸ªä½ç½®) å’Œè¾“å‡º (æœ€åä¸€ä¸ªä½ç½®)ï¼š

``` python
import os
import sys
import torch
import gzip
import itertools
from torch import nn
from matplotlib import pyplot

class MyModel(nn.Module):
    """æ ¹æ®çƒçš„å†å²ä½ç½®é¢„æµ‹å°†æ¥ä½ç½®çš„æ¨¡å‹ï¼Œå‡è®¾çƒåªèƒ½åŒæ–¹å‘åŒ€é€Ÿç§»åŠ¨"""
    def __init__(self):
        super().__init__()
        self.rnn = nn.RNN(
            input_size = 1,
            hidden_size = 8,
            num_layers = 1,
            batch_first = True
        )
        self.linear = nn.Linear(in_features=8, out_features=1)

    def forward(self, x, lengths):
        # é™„åŠ é•¿åº¦ä¿¡æ¯ï¼Œé¿å… RNN è®¡ç®—å¡«å……çš„æ•°æ®
        packed = nn.utils.rnn.pack_padded_sequence(
            x, lengths, batch_first=True, enforce_sorted=False)
        # ä½¿ç”¨é€’å½’æ¨¡å‹è®¡ç®—ï¼Œè¿”å›å„ä¸ªè¾“å…¥å¯¹åº”çš„è¾“å‡ºå’Œæœ€ç»ˆçš„éšè—çŠ¶æ€
        # å› ä¸º RNN tanh ç›´æ¥æŠŠéšè—å€¼ä½œä¸ºè¾“å‡ºï¼Œæ‰€ä»¥ output çš„æœ€åä¸€ä¸ªå€¼æå–å‡ºæ¥ä¼šç­‰äº hidden
        # å¹³æ—¶ä¸ºäº†æ–¹ä¾¿å¯ä»¥ç›´æ¥ä½¿ç”¨ hiddenï¼Œä½†è¿™é‡Œä¸ºäº†æ¼”ç¤ºæ€ä¹ˆæå–ä¼šä½¿ç”¨ output
        output, hidden = self.rnn(packed)
        # output å†…éƒ¨ä¼šè¿æ¥æ‰€æœ‰éšè—çŠ¶æ€ï¼Œshape = å®é™…è¾“å…¥æ•°é‡åˆè®¡, hidden_size
        # ä¸ºäº†æ¥ä¸‹æ¥çš„å¤„ç†ï¼Œéœ€è¦å…ˆæ•´ç† shape = batch_size, æ¯ç»„çš„æœ€å¤§è¾“å…¥æ•°é‡, hidden_size
        # ç¬¬äºŒä¸ªè¿”å›å€¼æ˜¯å„ä¸ª tensor çš„å®é™…é•¿åº¦ï¼Œå†…å®¹å’Œ lengths ç›¸åŒï¼Œæ‰€ä»¥å¯ä»¥çœç•¥æ‰
        unpacked, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)
        # æå–æœ€åä¸€ä¸ªè¾“å…¥å¯¹åº”çš„è¾“å‡º, shape = batch_size, 1, hidden_size
        # å¯¹äºå¤§éƒ¨åˆ†é€’å½’æ¨¡å‹, last_hidden çš„å€¼å®é™…ä¸Šä¼šç­‰äº hidden
        last_hidden = unpacked.gather(1, (lengths - 1).reshape(-1, 1, 1).repeat(1, 1, unpacked.shape[2]))
        # ä¼ ç»™çº¿æ€§æ¨¡å‹æŠŠéšè—å€¼è½¬æ¢åˆ°é¢„æµ‹è¾“å‡º
        y = self.linear(last_hidden.reshape(last_hidden.shape[0], last_hidden.shape[2]))
        return y

def save_tensor(tensor, path):
    """ä¿å­˜ tensor å¯¹è±¡åˆ°æ–‡ä»¶"""
    torch.save(tensor, gzip.GzipFile(path, "wb"))

def load_tensor(path):
    """ä»æ–‡ä»¶è¯»å– tensor å¯¹è±¡"""
    return torch.load(gzip.GzipFile(path, "rb"))

def prepare_save_batch(batch, pending_tensors):
    """å‡†å¤‡è®­ç»ƒ - ä¿å­˜å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # æ•´åˆé•¿åº¦ä¸ç­‰çš„ tensor åˆ—è¡¨åˆ°ä¸€ä¸ª tensorï¼Œä¸è¶³çš„é•¿åº¦ä¼šå¡«å…… 0
    dataset_tensor = nn.utils.rnn.pad_sequence(pending_tensors, batch_first=True)

    # æ­£è§„åŒ–æ•°æ®ï¼Œå› ä¸ºå¤§éƒ¨åˆ†æ•°æ®éƒ½è½åœ¨ -50 ~ 50 çš„åŒºé—´ä¸­ï¼Œè¿™é‡Œå¯ä»¥å…¨éƒ¨é™¤ä»¥ 50
    dataset_tensor /= 50

    # å¦å¤–ä¿å­˜å„ä¸ª tensor çš„é•¿åº¦
    lengths_tensor = torch.tensor([t.shape[0] for t in pending_tensors])

    # åˆ‡åˆ†è®­ç»ƒé›† (60%)ï¼ŒéªŒè¯é›† (20%) å’Œæµ‹è¯•é›† (20%)
    random_indices = torch.randperm(dataset_tensor.shape[0])
    training_indices = random_indices[:int(len(random_indices)*0.6)]
    validating_indices = random_indices[int(len(random_indices)*0.6):int(len(random_indices)*0.8):]
    testing_indices = random_indices[int(len(random_indices)*0.8):]
    training_set = (dataset_tensor[training_indices], lengths_tensor[training_indices])
    validating_set = (dataset_tensor[validating_indices], lengths_tensor[validating_indices])
    testing_set = (dataset_tensor[testing_indices], lengths_tensor[testing_indices])

    # ä¿å­˜åˆ°ç¡¬ç›˜
    save_tensor(training_set, f"data/training_set.{batch}.pt")
    save_tensor(validating_set, f"data/validating_set.{batch}.pt")
    save_tensor(testing_set, f"data/testing_set.{batch}.pt")
    print(f"batch {batch} saved")

def prepare():
    """å‡†å¤‡è®­ç»ƒ"""
    # æ•°æ®é›†è½¬æ¢åˆ° tensor ä»¥åä¼šä¿å­˜åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    if not os.path.isdir("data"):
        os.makedirs("data")

    # ä» csv è¯»å–åŸå§‹æ•°æ®é›†ï¼Œåˆ†æ‰¹æ¯æ¬¡å¤„ç† 2000 è¡Œ
    # å› ä¸º pandas ä¸æ”¯æŒè¯»å–åŠ¨æ€é•¿åº¦çš„ csv æ–‡ä»¶ï¼Œè¿™é‡Œéœ€è¦ä½¿ç”¨åŸå§‹æ–¹æ³•è¯»å–
    batch = 0
    pending_tensors = []
    for line in open('ball.csv', 'r'):
        t = torch.tensor([int(x) for x in line.split(',')], dtype=torch.float)
        pending_tensors.append(t)
        if len(pending_tensors) >= 2000:
            prepare_save_batch(batch, pending_tensors)
            batch += 1
            pending_tensors.clear()
    if pending_tensors:
        prepare_save_batch(batch, pending_tensors)
        batch += 1
        pending_tensors.clear()

def train():
    """å¼€å§‹è®­ç»ƒ"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MyModel()

    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨
    loss_function = torch.nn.MSELoss()

    # åˆ›å»ºå‚æ•°è°ƒæ•´å™¨
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    # è®°å½•è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    training_accuracy_history = []
    validating_accuracy_history = []

    # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡
    validating_accuracy_highest = 0
    validating_accuracy_highest_epoch = 0

    # è¯»å–æ‰¹æ¬¡çš„å·¥å…·å‡½æ•°
    def read_batches(base_path):
        for batch in itertools.count():
            path = f"{base_path}.{batch}.pt"
            if not os.path.isfile(path):
                break
            yield load_tensor(path)

    # è®¡ç®—æ­£ç¡®ç‡çš„å·¥å…·å‡½æ•°
    def calc_accuracy(actual, predicted):
        return 1 - ((actual - predicted).abs() / (actual.abs() + 0.0001)).mean().item()

    # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡ºçš„å·¥å…·å‡½æ•°ï¼Œè¾“å‡ºä¸ºæœ€åä¸€ä¸ªä½ç½®ï¼Œè¾“å…¥ä¸ºä¹‹å‰çš„ä½ç½®
    def split_batch_xy(batch, begin=None, end=None):
        # shape = batch_size, input_size
        batch_x = batch[0][begin:end]
        # shape = batch_size, 1
        batch_x_lengths = batch[1][begin:end] - 1
        # shape = batch_size, 1
        batch_y = batch_x.gather(1, batch_x_lengths.reshape(-1, 1))
        # shape = batch_size, input_size, step_size = batch_size, input_size, 1
        batch_x = batch_x.reshape(batch_x.shape[0], batch_x.shape[1], 1)
        return batch_x, batch_x_lengths, batch_y

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    for epoch in range(1, 10000):
        print(f"epoch: {epoch}")

        # æ ¹æ®è®­ç»ƒé›†è®­ç»ƒå¹¶ä¿®æ”¹å‚æ•°
        # åˆ‡æ¢æ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼ï¼Œå°†ä¼šå¯ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.train()
        training_accuracy_list = []
        for batch in read_batches("data/training_set"):
            # åˆ‡åˆ†å°æ‰¹æ¬¡ï¼Œæœ‰åŠ©äºæ³›åŒ–æ¨¡å‹
             for index in range(0, batch[0].shape[0], 100):
                # åˆ’åˆ†è¾“å…¥å’Œè¾“å‡º
                batch_x, batch_x_lengths, batch_y = split_batch_xy(batch, index, index+100)
                # è®¡ç®—é¢„æµ‹å€¼
                predicted = model(batch_x, batch_x_lengths)
                # è®¡ç®—æŸå¤±
                loss = loss_function(predicted, batch_y)
                # ä»æŸå¤±è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼å‡½æ•°å€¼
                loss.backward()
                # ä½¿ç”¨å‚æ•°è°ƒæ•´å™¨è°ƒæ•´å‚æ•°
                optimizer.step()
                # æ¸…ç©ºå¯¼å‡½æ•°å€¼
                optimizer.zero_grad()
                # è®°å½•è¿™ä¸€ä¸ªæ‰¹æ¬¡çš„æ­£ç¡®ç‡ï¼Œtorch.no_grad ä»£è¡¨ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
                with torch.no_grad():
                    training_accuracy_list.append(calc_accuracy(batch_y, predicted))
        training_accuracy = sum(training_accuracy_list) / len(training_accuracy_list)
        training_accuracy_history.append(training_accuracy)
        print(f"training accuracy: {training_accuracy}")

        # æ£€æŸ¥éªŒè¯é›†
        # åˆ‡æ¢æ¨¡å‹åˆ°éªŒè¯æ¨¡å¼ï¼Œå°†ä¼šç¦ç”¨è‡ªåŠ¨å¾®åˆ†ï¼Œæ‰¹æ¬¡æ­£è§„åŒ– (BatchNorm) ä¸ Dropout
        model.eval()
        validating_accuracy_list = []
        for batch in read_batches("data/validating_set"):
            batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
            predicted = model(batch_x, batch_x_lengths)
            validating_accuracy_list.append(calc_accuracy(batch_y, predicted))
        validating_accuracy = sum(validating_accuracy_list) / len(validating_accuracy_list)
        validating_accuracy_history.append(validating_accuracy)
        print(f"validating accuracy: {validating_accuracy}")

        # è®°å½•æœ€é«˜çš„éªŒè¯é›†æ­£ç¡®ç‡ä¸å½“æ—¶çš„æ¨¡å‹çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•
        if validating_accuracy > validating_accuracy_highest:
            validating_accuracy_highest = validating_accuracy
            validating_accuracy_highest_epoch = epoch
            save_tensor(model.state_dict(), "model.pt")
            print("highest validating accuracy updated")
        elif epoch - validating_accuracy_highest_epoch > 100:
            # åœ¨ 100 æ¬¡è®­ç»ƒåä»ç„¶æ²¡æœ‰åˆ·æ–°è®°å½•ï¼Œç»“æŸè®­ç»ƒ
            print("stop training because highest validating accuracy not updated in 100 epoches")
            break

    # ä½¿ç”¨è¾¾åˆ°æœ€é«˜æ­£ç¡®ç‡æ—¶çš„æ¨¡å‹çŠ¶æ€
    print(f"highest validating accuracy: {validating_accuracy_highest}",
        f"from epoch {validating_accuracy_highest_epoch}")
    model.load_state_dict(load_tensor("model.pt"))

    # æ£€æŸ¥æµ‹è¯•é›†
    testing_accuracy_list = []
    for batch in read_batches("data/testing_set"):
        batch_x, batch_x_lengths, batch_y = split_batch_xy(batch)
        predicted = model(batch_x, batch_x_lengths)
        testing_accuracy_list.append(calc_accuracy(batch_y, predicted))
    testing_accuracy = sum(testing_accuracy_list) / len(testing_accuracy_list)
    print(f"testing accuracy: {testing_accuracy}")

    # æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ­£ç¡®ç‡å˜åŒ–
    pyplot.plot(training_accuracy_history, label="training")
    pyplot.plot(validating_accuracy_history, label="validing")
    pyplot.ylim(0, 1)
    pyplot.legend()
    pyplot.show()

def eval_model():
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„çŠ¶æ€ï¼Œç„¶ååˆ‡æ¢åˆ°éªŒè¯æ¨¡å¼
    model = MyModel()
    model.load_state_dict(load_tensor("model.pt"))
    model.eval()

    # è¯¢é—®è¾“å…¥å¹¶é¢„æµ‹è¾“å‡º
    while True:
        try:
            x = torch.tensor([int(x) for x in input("History ball locations: ").split(",")], dtype=torch.float)
            # æ­£è§„åŒ–è¾“å…¥
            x /= 50
            # è½¬æ¢çŸ©é˜µå¤§å°ï¼Œshape = batch_size, input_size, step_size
            x = x.reshape(1, x.shape[0], 1)
            lengths = torch.tensor([x.shape[1]])
            # é¢„æµ‹è¾“å‡º
            y = model(x, lengths)
            # åæ­£è§„åŒ–è¾“å‡º
            y *= 50
            print("Next ball location:", y[0, 0].item(), "\n")
        except Exception as e:
            print("error:", e)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print(f"Please run: {sys.argv[0]} prepare|train|eval")
        exit()

    # ç»™éšæœºæ•°ç”Ÿæˆå™¨åˆ†é…ä¸€ä¸ªåˆå§‹å€¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œéƒ½å¯ä»¥ç”Ÿæˆç›¸åŒçš„éšæœºæ•°
    # è¿™æ˜¯ä¸ºäº†è®©è¿‡ç¨‹å¯é‡ç°ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¿™æ ·åš
    torch.random.manual_seed(0)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ“ä½œ
    operation = sys.argv[1]
    if operation == "prepare":
        prepare()
    elif operation == "train":
        train()
    elif operation == "eval":
        eval_model()
    else:
        raise ValueError(f"Unsupported operation: {operation}")

if __name__ == "__main__":
    main()
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å‡†å¤‡æ•°æ®é›†å¹¶è®­ç»ƒæ¨¡å‹ï¼š

``` text
python3 example.py prepare
python3 example.py train
```

æœ€ç»ˆè¾“å‡ºå¦‚ä¸‹ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†æ­£ç¡®åº¦éƒ½è¾¾åˆ°äº† 92% (è¿™ä¸ªä¾‹å­ä»…ç”¨äºæ¼”ç¤ºæ€æ ·ä½¿ç”¨ RNNï¼Œå¦‚æœä½ æœ‰å…´è¶£å¯ä»¥è¯•è¯•æ€æ ·æé«˜æ­£ç¡®åº¦)ï¼š

``` text
epoch: 2351
training accuracy: 0.9427350702928379
validating accuracy: 0.9283818986266852
stop training because highest validating accuracy not updated in 100 epoches
highest validating accuracy: 0.9284620460122823 from epoch 2250
testing accuracy: 0.9274853881448507
```

æ¥ä¸‹æ¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯ä½¿ç”¨æ¨¡å‹ï¼š

``` text
python3 example.py eval
```

è¯•è¯•æ ¹æ®å†å²ä½ç½®é¢„æµ‹å°†æ¥ä½ç½®ï¼Œå¯ä»¥çœ‹åˆ°é¢„æµ‹å€¼æ¯”è¾ƒæ¥è¿‘æˆ‘ä»¬é¢„æœŸçš„å®é™…å€¼ğŸ™‚ï¸ï¼š

``` text
History ball locations: 1,2,3
Next ball location: 3.8339991569519043

History ball locations: 3,5,7
Next ball location: 9.035120964050293

History ball locations: 9,7,5,3
Next ball location: 0.9755149483680725

History ball locations: 2,0,-2
Next ball location: -3.913722276687622

History ball locations: 0,-3,-6
Next ball location: -9.093448638916016
```

## é•¿çŸ­è®°å¿†æ¨¡å‹ - LSTM

åœ¨çœ‹ RNN tanh è®¡ç®—å…¬å¼çš„æ—¶å€™ï¼Œä½ å¯èƒ½ä¼šæƒ³èµ·ç¬¬ä¸‰ç¯‡æ–‡ç« åœ¨ä»‹ç»æ¿€æ´»å‡½æ•°æ—¶æåˆ°çš„æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient) é—®é¢˜ï¼Œå¦‚æœç»™æ¨¡å‹ä¼ äº† 10 æ¬¡è¾“å…¥ï¼Œé‚£ä¹ˆè¾“å‡ºå°±ä¼šå åŠ  10 æ¬¡ tanhï¼Œè¿™å°±å¯¼è‡´å‰é¢çš„è¾“å…¥å¯¹æœ€ç»ˆè¾“å‡ºçš„å½±å“éå¸¸éå¸¸çš„å°ï¼Œä¾‹å¦‚ `æˆ‘å¯¹è¿™éƒ¨ç”µè§†æœºéå¸¸æ»¡æ„ï¼Œä¾¿å®œåˆæ¸…æ™°ï¼Œæˆ‘ä¹‹å‰ä¹°çš„ LG ç”µè§†æœºç»å¸¸å¤±çµé€å»ç»´ä¿®å¥½å‡ æ¬¡éƒ½æ²¡æœ‰è§£å†³å®åœ¨æ˜¯å¤ªåƒåœ¾äº†`ï¼Œåˆ¤æ–­è¿™å¥è¯æ˜¯æ­£é¢è¯„ä»·è¿˜æ˜¯è´Ÿé¢è¯„ä»·éœ€è¦çœ‹å‰åŠéƒ¨åˆ†ï¼Œä½† RNN tanh çš„æœ€ç»ˆè¾“å‡ºå—ååŠéƒ¨åˆ†çš„å½±å“æ›´å¤šï¼Œå‰åŠéƒ¨åˆ†åŸºæœ¬ä¸Šä¼šè¢«å¿½ç•¥ï¼Œæ‰€ä»¥æ— æ³•ä½œå‡ºæ­£ç¡®çš„åˆ¤æ–­ã€‚æŠŠ tanh å‡½æ•°æ¢æˆ relu å‡½æ•°ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†è¾“å…¥çš„ä¼ é€’æ¬¡æ•°éå¸¸å¤šçš„æ—¶å€™è¿˜æ˜¯æ— æ³•é¿å…å‡ºç°æ¢¯åº¦æ¶ˆå¤±ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å‘æ˜å‡ºæ¥çš„å°±æ˜¯é•¿çŸ­è®°å¿†æ¨¡å‹ï¼Œç•¥ç§° LSTM (Long Short-Term Memory)ã€‚é•¿çŸ­è®°å¿†æ¨¡å‹çš„ç‰¹å¾æ˜¯é™¤äº†éšè—çŠ¶æ€ (Hidden State) ä»¥å¤–è¿˜æœ‰ä¸€ä¸ªç»†èƒçŠ¶æ€ (Cell State)ï¼Œå¹¶ä¸”æœ‰è¾“å…¥é—¨ (Input Gate)ï¼Œç»†èƒé—¨ (Cell Gate)ï¼Œå¿˜è®°é—¨ (Forget Gate) è´Ÿè´£æ›´æ–°ç»†èƒçŠ¶æ€ï¼Œå’Œè¾“å‡ºé—¨ (Output Gate) è´Ÿè´£ä»ç»†èƒçŠ¶æ€æå–éšè—çŠ¶æ€ï¼Œå…·ä½“çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

![06](./06.png)

å¦‚æœä½ è§‰å¾—å…¬å¼éš¾ä»¥ç†è§£å¯ä»¥å‚è€ƒä¸‹é¢çš„è®¡ç®—å›¾ï¼Œå›¾ä¸­å±•ç¤ºäº† LSTM æ¨¡å‹å¦‚ä½•è®¡ç®—ä¸‰æ¬¡è¾“å…¥å’Œè¿”å›ä¸‰æ¬¡è¾“å‡º (æ³¨æ„æœ€ååŠ äº†çº¿æ€§æ¨¡å‹ç”¨äºè½¬æ¢éšè—å€¼åˆ°é¢„æµ‹è¾“å‡º)ï¼š

(çœ‹ä¸æ¸…è¯·åœ¨æ–°æ ‡ç­¾å•ç‹¬æ‰“å¼€å›¾ç‰‡ï¼Œæˆ–è€…å¦å­˜ä¸ºåˆ°æœ¬åœ°ä»¥åæŸ¥çœ‹)

![lstm](./lstm.png)

LSTM æ¨¡å‹çš„ç»†èƒçŠ¶æ€åœ¨å åŠ çš„è¿‡ç¨‹ä¸­åªä¼šä½¿ç”¨ä¹˜æ³•å’ŒåŠ æ³•ï¼Œæ‰€ä»¥å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶ä¸”è¾“å…¥é—¨å¯ä»¥å†³å®šè¾“å…¥æ˜¯å¦é‡è¦ï¼Œå¦‚æœä¸é‡è¦åˆ™å‡å°‘å¯¹ç»†èƒçŠ¶æ€çš„å½±å“ (è¿”å›æ¥è¿‘ 0 çš„å€¼)ï¼Œå¿˜è®°é—¨å¯ä»¥åœ¨é‡åˆ°æŸäº›è¾“å…¥çš„æ—¶å€™å¿˜è®°ç»†èƒçŠ¶æ€ä¸­è®°å½•çš„éƒ¨åˆ†å€¼ï¼Œè¾“å‡ºé—¨å¯ä»¥æ ¹æ®è¾“å…¥å†³å®šå¦‚ä½•æå–ç»†èƒçŠ¶æ€åˆ°éšè—å€¼ (è¾“å‡º)ï¼Œè¿™äº›é—¨è®© LSTM å¯ä»¥å­¦ä¹ æ›´é•¿çš„æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥å‘æŒ¥æ›´å¼ºçš„åˆ¤æ–­èƒ½åŠ›ï¼Œä½†åŒæ—¶ä¹Ÿè®© LSTM çš„è®¡ç®—é‡å˜å¤§ï¼Œéœ€è¦ä½¿ç”¨æ›´é•¿çš„æ—¶é—´æ‰èƒ½è®­ç»ƒæˆåŠŸã€‚

## ç®€åŒ–ç‰ˆé•¿çŸ­è®°å¿†æ¨¡å‹ - GRU

GRU (Gated Recurrent Unit) æ˜¯ç®€åŒ–ç‰ˆçš„é•¿çŸ­è®°å¿†æ¨¡å‹ï¼Œå»æ‰äº†ç»†èƒçŠ¶æ€å¹¶ä¸”åªä½¿ç”¨ä¸‰ä¸ªé—¨ï¼Œæ–°å¢é—¨è´Ÿè´£è®¡ç®—æ›´æ–°éšè—çŠ¶æ€çš„å€¼ï¼Œé‡ç½®é—¨è´Ÿè´£è¿‡æ»¤éƒ¨åˆ†æ›´æ–°éšè—çŠ¶æ€çš„å€¼ (é’ˆå¯¹æ ¹æ®ä¹‹å‰éšè—å€¼è®¡ç®—çš„éƒ¨åˆ†)ï¼Œæ›´æ–°é—¨è´Ÿè´£è®¡ç®—æ›´æ–°æ¯”ä¾‹ï¼Œå…·ä½“çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

![07](./07.png)

å¦‚æœä½ è§‰å¾—å…¬å¼éš¾ä»¥ç†è§£å¯ä»¥å‚è€ƒä¸‹é¢çš„è®¡ç®—å›¾ï¼Œå›¾ä¸­å±•ç¤ºäº† GRU æ¨¡å‹å¦‚ä½•è®¡ç®—ä¸‰æ¬¡è¾“å…¥å’Œè¿”å›ä¸‰æ¬¡è¾“å‡º (æ³¨æ„æœ€ååŠ äº†çº¿æ€§æ¨¡å‹ç”¨äºè½¬æ¢éšè—å€¼åˆ°é¢„æµ‹è¾“å‡º)ï¼š

(çœ‹ä¸æ¸…è¯·åœ¨æ–°æ ‡ç­¾å•ç‹¬æ‰“å¼€å›¾ç‰‡ï¼Œæˆ–è€…å¦å­˜ä¸ºåˆ°æœ¬åœ°ä»¥åæŸ¥çœ‹)

![gru](./gru.png)

å¯ä»¥çœ‹åˆ° GRU æ¨¡å‹çš„éšè—çŠ¶æ€åœ¨å åŠ çš„è¿‡ç¨‹ä¸­ä¹Ÿæ˜¯åªä½¿ç”¨äº†ä¹˜æ³•å’ŒåŠ æ³•ï¼Œæ‰€ä»¥å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚GRU æ¨¡å‹æ¯” LSTM æ¨¡å‹ç¨å¼±ï¼Œä½†æ˜¯è®¡ç®—é‡ä¹Ÿç›¸åº”å‡å°‘äº†ï¼Œç»™æˆ‘ä»¬å¤šäº†ä¸€ä¸ªä¸é”™çš„ä¸­é—´é€‰æ‹©ğŸ˜¼ã€‚

## åœ¨ pytorch ä¸­ä½¿ç”¨ LSTM å’Œ GRU

pytorch å¸®æˆ‘ä»¬å°è£…äº† LSTM å’Œ GRU æ¨¡å‹ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬åªéœ€è¦æ›¿æ¢ä»£ç ä¸­çš„ `RNN` åˆ° `LSTM` æˆ– `GRU` å³å¯ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

``` text
>>> rnn_model = torch.nn.RNN(input_size = 1, hidden_size = 8, num_layers = 1, batch_first = True)

>>> rnn_model = torch.nn.LSTM(input_size = 1, hidden_size = 8, num_layers = 1, batch_first = True)

>>> rnn_model = torch.nn.GRU(input_size = 1, hidden_size = 8, num_layers = 1, batch_first = True)
```

## å†™åœ¨æœ€å

è¿™ä¸€ç¯‡ç€é‡ä»‹ç»äº† RNN tanhï¼ŒLSTM ä¸ GRU çš„è®¡ç®—æ–¹å¼ï¼Œå¹¶ä¸”ç»™å‡ºäº†ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­è¯´æ˜å¦‚ä½•åœ¨ pytorch ä¸­åº”ç”¨ RNNã€‚ä¸‹ä¸€ç¯‡å°†ä¼šä»‹ç»æ›´å¤šå®ç”¨çš„ä¾‹å­ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¶‹åŠ¿é¢„æµ‹ç­‰ï¼ŒåŒæ—¶ä¹Ÿä¼šè®²è§£å¦‚ä½•ä½¿ç”¨åŒå‘é€’å½’æ¨¡å‹ã€‚

æœ€åå–Šä¸€å¥å£å·ğŸ˜¡ï¼šä¸­å›½åŠ æ²¹ï¼Œä¸­å›½å¿…èƒœï¼
